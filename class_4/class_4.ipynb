{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# State-of-the-Art Automatic Speech Recognition (ASR) with Whisper\n",
    "\n",
    "In previous classes, we built speech recognition from scratch - recording digits, training neural networks, and performing inference. That was incredible for understanding the fundamentals, but modern ASR systems can do *so much more*.\n",
    "\n",
    "**Automatic Speech Recognition (ASR)** is the task of converting spoken language into text. Today's state-of-the-art systems can:\n",
    "\n",
    "- Transcribe speech in **99+ languages**\n",
    "- **Translate** speech from one language to English\n",
    "- Provide **word-level timestamps**\n",
    "- Handle accents, background noise, and multiple speakers\n",
    "- Run on your **local machine** - no internet required!\n",
    "\n",
    "### What you'll learn:\n",
    "\n",
    "1. **How modern ASR works** - from audio waves to text (high-level)\n",
    "2. **OpenAI's Whisper** - a powerful, free, open-source ASR model\n",
    "3. **Transcription** - converting your speech to text with code\n",
    "4. **Whisper's features** - language detection, translation, timestamps\n",
    "5. **Real-time recognition** - live speech-to-text from your microphone\n",
    "6. **Triggering actions** - using speech to control other things (OSC, Serial, etc.)\n",
    "7. **Fine-tuning** (stretch goal) - customizing Whisper for specific tasks\n",
    "8. **Other ASR systems** - what else is out there?\n",
    "\n",
    "### Background Reading\n",
    "\n",
    "- [Whisper Paper: Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) - OpenAI (2022)\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The Transformer paper (2017)\n",
    "- [A Brief History of ASR](https://www.assemblyai.com/blog/the-history-of-speech-recognition-to-2022/) - AssemblyAI Blog\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Part 1: How Did We Get Here?\n",
    "\n",
    "### A Brief History of Speech Recognition\n",
    "\n",
    "| Year | System | What it could do |\n",
    "|------|--------|------------------|\n",
    "| 1952 | **AUDREY** (Bell Labs) | Recognized digits 0-9 from a single speaker |\n",
    "| 1962 | **IBM Shoebox** | Recognized 16 words |\n",
    "| 1970s | **HARPY** (CMU) | 1,011 word vocabulary using Hidden Markov Models |\n",
    "| 1990s | **Dragon NaturallySpeaking** | First commercial dictation software |\n",
    "| 2011 | **Siri** (Apple) | Voice assistant using deep learning |\n",
    "| 2014 | **Deep Speech** (Baidu) | End-to-end deep learning for ASR |\n",
    "| 2020 | **Wav2Vec 2.0** (Meta) | Self-supervised learning for speech |\n",
    "| 2022 | **Whisper** (OpenAI) | Trained on 680,000 hours of multilingual audio |\n",
    "| 2024+ | **Canary, Voxtral, etc.** | Even more accurate, multilingual, real-time |\n",
    "\n",
    "In Class 2, we built our own \"Audrey\" - a digit recognizer trained on our own voice. Now we'll use **Whisper**, which was trained on 680,000 hours of audio from the internet and can understand almost any spoken language!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## How Modern ASR Works (High Level)\n",
    "\n",
    "Modern ASR systems like Whisper use a **Transformer** architecture - the same type of neural network behind ChatGPT, but adapted for audio.\n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "```\n",
    "Your Voice -> Microphone -> Audio Waveform -> Mel Spectrogram -> Encoder -> Decoder -> Text\n",
    "```\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "1. **Audio Waveform**: Sound captured as numbers (like we learned in Class 1!)\n",
    "2. **Mel Spectrogram**: Converts the waveform into a visual representation of frequencies over time (also from Class 1!). This is the input to the neural network.\n",
    "3. **Encoder**: A neural network that reads the spectrogram and creates a rich \"understanding\" of the audio. Think of it like someone listening carefully and forming a mental picture of the sounds.\n",
    "4. **Decoder**: Another neural network that generates text one word (or \"token\") at a time, guided by the encoder's understanding. Like translating that mental picture into written words.\n",
    "\n",
    "### What's Special About Whisper?\n",
    "\n",
    "| Feature | Details |\n",
    "|---------|---------|\n",
    "| **Training Data** | 680,000 hours of audio from the internet |\n",
    "| **Languages** | 99+ languages supported |\n",
    "| **Multitask** | Transcription, translation, language detection, timestamps |\n",
    "| **Open Source** | Free to download, run, and modify! |\n",
    "| **Runs Locally** | No internet connection required after downloading the model |\n",
    "| **Model Sizes** | From tiny (39M params) to large (1.5B params) - pick what fits your computer |\n",
    "\n",
    "### How Whisper Was Trained\n",
    "\n",
    "Unlike our Audrey model (trained on 10 recordings!), Whisper was trained on an enormous amount of data scraped from the internet. The key insight: OpenAI collected audio that *already had* subtitles or transcriptions (like YouTube videos with captions), so they didn't need humans to manually label everything. This is called **weak supervision** - the labels aren't perfect, but there are *so many* of them that the model still learns incredibly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 2: Setup and Installation\n",
    "\n",
    "Before we can use Whisper, we need to install it and a system dependency called **ffmpeg**.\n",
    "\n",
    "### Step 1: Install ffmpeg (audio processing tool)\n",
    "\n",
    "Whisper uses ffmpeg under the hood to read and process audio files. Install it for your operating system:\n",
    "\n",
    "**Mac** (using Homebrew):\n",
    "```bash\n",
    "brew install ffmpeg\n",
    "```\n",
    "\n",
    "**Windows** (using Chocolatey):\n",
    "```bash\n",
    "choco install ffmpeg\n",
    "```\n",
    "\n",
    "**Windows** (manual): Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to your system PATH.\n",
    "\n",
    "**Linux**:\n",
    "```bash\n",
    "sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "### Step 2: Install the openai-whisper Python package\n",
    "\n",
    "```bash\n",
    "uv pip install openai-whisper\n",
    "```\n",
    "\n",
    "That's it! Let's verify everything is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ffmpeg is installed (required by Whisper for audio processing)\n",
    "# If this shows a warning, follow the installation instructions above!\n",
    "#\n",
    "# NOTE: Jupyter notebooks sometimes can't find programs that your terminal can.\n",
    "# This is because the notebook's PATH (where it looks for programs) is different\n",
    "# from your terminal's PATH. The code below checks common locations and fixes this.\n",
    "\n",
    "import shutil\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Common locations where ffmpeg might be installed\n",
    "# Jupyter notebooks often don't inherit your full shell PATH, so we check manually\n",
    "common_paths = [\n",
    "    \"/opt/homebrew/bin\",      # macOS Apple Silicon (M1/M2/M3) Homebrew\n",
    "    \"/usr/local/bin\",         # macOS Intel Homebrew / Linux\n",
    "    \"/usr/bin\",               # Linux system install\n",
    "    os.path.expanduser(\"~/bin\"),  # User local bin\n",
    "]\n",
    "\n",
    "# Add common paths to PATH if they're not already there\n",
    "for p in common_paths:\n",
    "    if os.path.isdir(p) and p not in os.environ.get(\"PATH\", \"\"):\n",
    "        os.environ[\"PATH\"] = p + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Now check for ffmpeg\n",
    "ffmpeg_path = shutil.which('ffmpeg')\n",
    "if ffmpeg_path:\n",
    "    print(f\"ffmpeg found at: {ffmpeg_path}\")\n",
    "    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)\n",
    "    # Print just the first line (version info)\n",
    "    print(result.stdout.split('\\n')[0])\n",
    "else:\n",
    "    print(\"WARNING: ffmpeg not found!\")\n",
    "    print(\"Please install ffmpeg before continuing:\")\n",
    "    if platform.system() == \"Darwin\":\n",
    "        print(\"  Mac: brew install ffmpeg\")\n",
    "    elif platform.system() == \"Windows\":\n",
    "        print(\"  Windows: choco install ffmpeg\")\n",
    "        print(\"  Or download from https://ffmpeg.org/download.html\")\n",
    "    else:\n",
    "        print(\"  Linux: sudo apt install ffmpeg\")\n",
    "    print()\n",
    "    print(\"If ffmpeg IS installed but still not found, find its location in\")\n",
    "    print(\"your terminal with 'which ffmpeg' (Mac/Linux) or 'where ffmpeg' (Windows)\")\n",
    "    print(\"and add the directory to PATH below:\")\n",
    "    print(\"  os.environ['PATH'] = '/your/ffmpeg/directory' + os.pathsep + os.environ['PATH']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries we'll need\n",
    "# If any of these fail, make sure you've run: uv pip install -r requirements.txt\n",
    "# And also: uv pip install openai-whisper\n",
    "\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Whisper version: {whisper.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Available Whisper models: {whisper.available_models()}\")\n",
    "\n",
    "# Check for GPU / Apple Silicon acceleration\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Whisper will use your GPU for faster transcription!\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"\\nApple Silicon (MPS) detected!\")\n",
    "    print(\"Note: Whisper will primarily use CPU, but PyTorch has MPS support.\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected - using CPU\")\n",
    "    print(\"This is totally fine! The 'tiny' and 'base' models run great on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Choosing a Whisper Model\n",
    "\n",
    "Whisper comes in several sizes. Smaller models are faster but less accurate. For working locally on your laptop, **tiny** or **base** are great starting points!\n",
    "\n",
    "| Model | Parameters | Download Size | Relative Speed | Best For |\n",
    "|-------|-----------|---------------|----------------|----------|\n",
    "| `tiny` | 39M | ~75 MB | ~10x | Quick experiments, real-time on CPU |\n",
    "| `base` | 74M | ~140 MB | ~7x | Good balance for local use |\n",
    "| `small` | 244M | ~460 MB | ~4x | Better accuracy, still comfortable on M1 |\n",
    "| `medium` | 769M | ~1.5 GB | ~2x | High accuracy, needs decent hardware |\n",
    "| `large` | 1550M | ~3 GB | 1x | Best accuracy, GPU recommended |\n",
    "\n",
    "**For this class**: Start with `tiny` to get things working fast, then try `base` or `small` if you want better results.\n",
    "\n",
    "**Pro tip**: On Apple Silicon Macs (M1/M2/M3), the `small` model runs very comfortably!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Whisper model\n",
    "# The first time you run this, it will DOWNLOAD the model (~75MB for tiny)\n",
    "# After that, it's cached locally on your machine - no internet needed!\n",
    "\n",
    "model_name = \"tiny\"  # Start with tiny - change to \"base\" or \"small\" later!\n",
    "\n",
    "print(f\"Loading Whisper '{model_name}' model...\")\n",
    "print(\"(First time will download - this only happens once!)\\n\")\n",
    "\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "print(f\"Model '{model_name}' loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"\\nFor comparison, our Audrey CNN had ~{128 * 16 * 45 + 512 * 10:,} parameters\")\n",
    "print(f\"Whisper '{model_name}' is about {sum(p.numel() for p in model.parameters()) // 100000}x bigger!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Part 3: Your First Transcription\n",
    "\n",
    "Let's record some audio and transcribe it with Whisper! This is the moment where months of AI research become a single function call.\n",
    "\n",
    "**Important**: Whisper works best with audio at **16,000 Hz** (16kHz). This is lower than the 44,100 Hz we used in previous classes, but speech doesn't need those high frequencies - human speech is mostly below 8,000 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio from your microphone\n",
    "# Say something clearly - a sentence or two works great!\n",
    "\n",
    "SAMPLE_RATE = 16000  # 16kHz - what Whisper expects\n",
    "DURATION = 5         # Record for 5 seconds\n",
    "RECORDING_FILE = \"my_recording.wav\"\n",
    "\n",
    "print(f\"Recording for {DURATION} seconds at {SAMPLE_RATE} Hz...\")\n",
    "print(\"Speak now!\\n\")\n",
    "\n",
    "recording = sd.rec(\n",
    "    int(DURATION * SAMPLE_RATE),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=1,\n",
    "    dtype='float32'\n",
    ")\n",
    "sd.wait()\n",
    "\n",
    "# Save the recording\n",
    "sf.write(RECORDING_FILE, recording, SAMPLE_RATE)\n",
    "print(f\"Recording saved to '{RECORDING_FILE}'\")\n",
    "\n",
    "# Play it back in the notebook\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(RECORDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the recording with Whisper!\n",
    "# This is where the magic happens - one line of code!\n",
    "\n",
    "result = model.transcribe(RECORDING_FILE)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION RESULT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nYou said: \\\"{result['text'].strip()}\\\"\")\n",
    "print(f\"\\nDetected language: {result['language']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Understanding the Result Object\n",
    "\n",
    "When Whisper transcribes audio, it returns a Python dictionary with several useful pieces of information. Let's explore what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a Python dictionary\n",
    "# Let's look at everything it contains\n",
    "\n",
    "print(\"Keys in the result dictionary:\")\n",
    "for key in result.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DETAILED BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. The full transcribed text\n",
    "print(f\"\\n1. TEXT (the transcription):\")\n",
    "print(f\"   '{result['text']}'\")\n",
    "\n",
    "# 2. The detected language\n",
    "print(f\"\\n2. LANGUAGE: '{result['language']}'\")\n",
    "\n",
    "# 3. Segments - Whisper breaks the transcription into time-stamped chunks\n",
    "print(f\"\\n3. SEGMENTS ({len(result['segments'])} segments):\")\n",
    "for seg in result['segments']:\n",
    "    start = seg['start']\n",
    "    end = seg['end']\n",
    "    text = seg['text']\n",
    "    print(f\"   [{start:5.1f}s -> {end:5.1f}s] {text}\")\n",
    "\n",
    "print(\"\\nEach segment also contains:\")\n",
    "if result['segments']:\n",
    "    seg = result['segments'][0]\n",
    "    for key in seg.keys():\n",
    "        print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "In one function call, Whisper:\n",
    "\n",
    "1. Loaded your audio file\n",
    "2. Converted it to a mel spectrogram (like we did manually in Class 1!)\n",
    "3. Passed the spectrogram through its encoder neural network\n",
    "4. Used its decoder to generate text, token by token\n",
    "5. Detected the language automatically\n",
    "6. Split the text into timestamped segments\n",
    "\n",
    "Compare that to our Audrey model which could only recognize single digits! This is the power of training on 680,000 hours of data with a much larger neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Part 4: Exploring Whisper's Features\n",
    "\n",
    "Whisper isn't just a simple speech-to-text system. It has several powerful features built in. Let's explore them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "\n",
    "Whisper can automatically detect what language is being spoken. This works by analyzing the first 30 seconds of audio and comparing it against all 99+ languages it knows.\n",
    "\n",
    "Let's see how confident Whisper is about the language of our recording!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection - what language does Whisper think you're speaking?\n",
    "\n",
    "# Step 1: Load and prepare audio in Whisper's expected format\n",
    "audio = whisper.load_audio(RECORDING_FILE)\n",
    "audio = whisper.pad_or_trim(audio)  # Whisper expects exactly 30 seconds of audio\n",
    "\n",
    "# Step 2: Create a mel spectrogram (same concept from Class 1!)\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# Step 3: Detect the language\n",
    "_, probs = model.detect_language(mel)\n",
    "\n",
    "# Show the top 10 detected languages with a visual bar\n",
    "print(\"Language Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "for lang, prob in sorted_probs[:10]:\n",
    "    bar = \"*\" * int(prob * 50)\n",
    "    print(f\"  {lang}: {prob:6.1%} {bar}\")\n",
    "\n",
    "print(f\"\\nMost likely language: {sorted_probs[0][0]} ({sorted_probs[0][1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "One of Whisper's coolest features: it can **translate** speech from any of its 99+ supported languages directly into English!\n",
    "\n",
    "If you speak another language, try recording yourself and see how Whisper translates it. If not, try finding a short audio clip in another language online (YouTube, a language learning app, etc.) and save it as a `.wav` or `.mp3` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation demo - Whisper can translate speech to English!\n",
    "# Try speaking in another language, or use any non-English audio file\n",
    "\n",
    "TRANSLATION_FILE = \"translation_test.wav\"\n",
    "\n",
    "print(f\"Recording for {DURATION} seconds...\")\n",
    "print(\"Try speaking in another language! (or English is fine too)\\n\")\n",
    "\n",
    "recording = sd.rec(\n",
    "    int(DURATION * SAMPLE_RATE),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=1,\n",
    "    dtype='float32'\n",
    ")\n",
    "sd.wait()\n",
    "sf.write(TRANSLATION_FILE, recording, SAMPLE_RATE)\n",
    "\n",
    "# Transcribe in the original language\n",
    "result_original = model.transcribe(TRANSLATION_FILE)\n",
    "print(f\"Transcription (original): {result_original['text']}\")\n",
    "print(f\"Detected language: {result_original['language']}\")\n",
    "\n",
    "# Translate to English (using task='translate')\n",
    "result_translated = model.transcribe(TRANSLATION_FILE, task=\"translate\")\n",
    "print(f\"\\nTranslation (English): {result_translated['text']}\")\n",
    "\n",
    "display(Audio(TRANSLATION_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Word-Level Timestamps\n",
    "\n",
    "Whisper can provide timestamps for each individual word in the transcription. This is incredibly useful for:\n",
    "\n",
    "- **Subtitling** videos\n",
    "- **Karaoke** applications\n",
    "- **Aligning** text with audio\n",
    "- **Creative coding** projects (trigger visuals at the exact moment specific words are spoken!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word-level timestamps\n",
    "# word_timestamps=True gives us the exact timing for each word\n",
    "\n",
    "result = model.transcribe(\n",
    "    RECORDING_FILE,\n",
    "    word_timestamps=True\n",
    ")\n",
    "\n",
    "print(\"Word-Level Timestamps:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Each segment contains a list of words with their timestamps\n",
    "for segment in result['segments']:\n",
    "    if 'words' in segment:\n",
    "        for word_info in segment['words']:\n",
    "            word = word_info['word']\n",
    "            start = word_info['start']\n",
    "            end = word_info['end']\n",
    "            print(f\"  [{start:6.2f}s -> {end:6.2f}s]  {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the word timestamps on top of the waveform!\n",
    "# This shows exactly when each word appears in the audio\n",
    "\n",
    "# Load audio for visualization\n",
    "y, sr = librosa.load(RECORDING_FILE, sr=None)\n",
    "duration = len(y) / sr\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "# Plot waveform\n",
    "times = np.linspace(0, duration, len(y))\n",
    "ax.plot(times, y, alpha=0.5, color='steelblue')\n",
    "\n",
    "# Collect all words with timestamps\n",
    "all_words = []\n",
    "for segment in result['segments']:\n",
    "    if 'words' in segment:\n",
    "        for word_info in segment['words']:\n",
    "            all_words.append(word_info)\n",
    "\n",
    "# Add colored regions for each word\n",
    "if all_words:\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, max(len(all_words), 1)))\n",
    "    for i, word_info in enumerate(all_words):\n",
    "        start = word_info['start']\n",
    "        end = word_info['end']\n",
    "        word = word_info['word'].strip()\n",
    "\n",
    "        # Highlight the word region\n",
    "        ax.axvspan(start, end, alpha=0.2, color=colors[i])\n",
    "\n",
    "        # Add the word label\n",
    "        mid = (start + end) / 2\n",
    "        ax.text(mid, ax.get_ylim()[1] * 0.85, word,\n",
    "                ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('Waveform with Word-Level Timestamps')\n",
    "ax.set_xlim(0, duration)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Comparing Model Sizes\n",
    "\n",
    "How much difference does model size make? Let's transcribe the same audio with different models and compare speed and accuracy.\n",
    "\n",
    "**Note**: Each model downloads on first use. The `tiny` model is ~75MB, `base` is ~140MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model sizes on the same audio\n",
    "# This will download each model the first time (only happens once!)\n",
    "\n",
    "models_to_compare = [\"tiny\", \"base\"]  # Add \"small\" if you have time and disk space!\n",
    "\n",
    "print(f\"Comparing models on: {RECORDING_FILE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name in models_to_compare:\n",
    "    print(f\"\\nLoading '{name}' model...\")\n",
    "    test_model = whisper.load_model(name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_result = test_model.transcribe(RECORDING_FILE)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'time': elapsed,\n",
    "        'text': test_result['text'].strip(),\n",
    "        'language': test_result['language']\n",
    "    })\n",
    "\n",
    "    print(f\"  Model:    {name}\")\n",
    "    print(f\"  Time:     {elapsed:.2f} seconds\")\n",
    "    print(f\"  Text:     {test_result['text'].strip()}\")\n",
    "    print(f\"  Language: {test_result['language']}\")\n",
    "\n",
    "    # Free memory\n",
    "    del test_model\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Observations:\")\n",
    "print(\"- Larger models are more accurate but slower\")\n",
    "print(\"- For real-time use, 'tiny' or 'base' is recommended\")\n",
    "print(\"- For best quality (offline), 'small' or 'medium' is great\")\n",
    "\n",
    "# Reload our working model\n",
    "model = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Part 5: Working with Longer Audio\n",
    "\n",
    "Whisper can handle audio of any length! Under the hood, it splits longer audio into **30-second chunks** and processes them one at a time. This means you can transcribe podcasts, lectures, interviews - anything!\n",
    "\n",
    "Let's try a longer recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record a longer clip\n",
    "# Try reading a paragraph, telling a story, or just talking for 15 seconds!\n",
    "\n",
    "LONG_DURATION = 15  # seconds\n",
    "LONG_RECORDING_FILE = \"long_recording.wav\"\n",
    "\n",
    "print(f\"Recording for {LONG_DURATION} seconds...\")\n",
    "print(\"Try reading a paragraph or telling a short story!\\n\")\n",
    "\n",
    "recording = sd.rec(\n",
    "    int(LONG_DURATION * SAMPLE_RATE),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=1,\n",
    "    dtype='float32'\n",
    ")\n",
    "sd.wait()\n",
    "sf.write(LONG_RECORDING_FILE, recording, SAMPLE_RATE)\n",
    "print(f\"Recording saved!\\n\")\n",
    "\n",
    "# Transcribe with word timestamps\n",
    "result = model.transcribe(LONG_RECORDING_FILE, word_timestamps=True)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION:\")\n",
    "print(result['text'])\n",
    "\n",
    "print(\"\\n\\nSEGMENTS (with timestamps):\")\n",
    "for seg in result['segments']:\n",
    "    print(f\"  [{seg['start']:6.1f}s -> {seg['end']:6.1f}s] {seg['text']}\")\n",
    "\n",
    "# Play it back\n",
    "display(Audio(LONG_RECORDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the longer transcription as a spectrogram with segment labels\n",
    "\n",
    "y, sr = librosa.load(LONG_RECORDING_FILE, sr=None)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Top: Waveform with segment labels\n",
    "duration = len(y) / sr\n",
    "times = np.linspace(0, duration, len(y))\n",
    "axes[0].plot(times, y, alpha=0.6, color='steelblue')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Waveform with Transcription Segments')\n",
    "\n",
    "# Add segment labels\n",
    "colors = plt.cm.Pastel1(np.linspace(0, 1, max(len(result['segments']), 1)))\n",
    "for i, seg in enumerate(result['segments']):\n",
    "    axes[0].axvspan(seg['start'], seg['end'], alpha=0.3, color=colors[i % len(colors)])\n",
    "    mid = (seg['start'] + seg['end']) / 2\n",
    "    # Truncate long text for the label\n",
    "    label = seg['text'].strip()[:30] + ('...' if len(seg['text'].strip()) > 30 else '')\n",
    "    axes[0].text(mid, axes[0].get_ylim()[1] * 0.8, label,\n",
    "                 ha='center', fontsize=7, style='italic')\n",
    "\n",
    "# Bottom: Mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr, ax=axes[1])\n",
    "axes[1].set_title('Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Part 6: Real-Time Speech Recognition\n",
    "\n",
    "Now for the exciting part - **real-time** speech recognition! We'll set up Whisper to listen to your microphone and transcribe what you say.\n",
    "\n",
    "### How Real-Time Works with Whisper\n",
    "\n",
    "Whisper is **not** a streaming model - it processes complete audio chunks. For \"real-time\" recognition, we use a **record-then-transcribe** approach:\n",
    "\n",
    "1. Record a short chunk of audio (e.g., 3-5 seconds)\n",
    "2. Send it to Whisper for transcription\n",
    "3. Display the result\n",
    "4. Repeat!\n",
    "\n",
    "This introduces a small delay (the time it takes to transcribe each chunk), but with the `tiny` model this is usually under 1 second.\n",
    "\n",
    "### Two Approaches\n",
    "\n",
    "| Approach | How it works | Best for |\n",
    "|----------|-------------|----------|\n",
    "| **Enter-to-Record** | Press Enter, speak, get result | Simple and reliable |\n",
    "| **Continuous Listening** | Auto-detects when you speak | Hands-free, more interactive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPROACH 1: Press Enter to Record and Transcribe\n",
    "# =============================================================================\n",
    "# Simple and reliable - press Enter, speak, get transcription!\n",
    "\n",
    "RECORD_DURATION = 5  # seconds per recording\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PRESS-TO-RECORD SPEECH RECOGNITION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"How it works:\")\n",
    "print(f\"  1. Press Enter when ready\")\n",
    "print(f\"  2. Speak for up to {RECORD_DURATION} seconds\")\n",
    "print(f\"  3. See the transcription!\")\n",
    "print(f\"  4. Type 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record (or 'q' to quit): \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        # Record\n",
    "        print(f\"Recording for {RECORD_DURATION} seconds... Speak now!\")\n",
    "        recording = sd.rec(\n",
    "            int(RECORD_DURATION * SAMPLE_RATE),\n",
    "            samplerate=SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "\n",
    "        # Save to a temporary file\n",
    "        temp_file = \"_temp_realtime.wav\"\n",
    "        sf.write(temp_file, recording, SAMPLE_RATE)\n",
    "\n",
    "        # Transcribe\n",
    "        start_time = time.time()\n",
    "        result = model.transcribe(temp_file)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"\\n  You said: \\\"{result['text'].strip()}\\\"\")\n",
    "        print(f\"  Language: {result['language']}\")\n",
    "        print(f\"  Transcription took: {elapsed:.2f}s\")\n",
    "\n",
    "        # Clean up\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if os.path.exists(\"_temp_realtime.wav\"):\n",
    "        os.remove(\"_temp_realtime.wav\")\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPROACH 2: Continuous Listening with Automatic Speech Detection\n",
    "# =============================================================================\n",
    "# Automatically detects when you start speaking and transcribes!\n",
    "# Uses a rolling buffer to capture the BEGINNING of your speech.\n",
    "\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 16000\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "# Speech detection settings - ADJUST THESE FOR YOUR ENVIRONMENT\n",
    "# If it triggers on background noise: increase VOLUME_THRESHOLD (try 0.05 or 0.08)\n",
    "# If it misses your speech: decrease VOLUME_THRESHOLD (try 0.02)\n",
    "VOLUME_THRESHOLD = 0.03\n",
    "SILENCE_DURATION = 1.5    # Seconds of silence before we process\n",
    "BUFFER_SECONDS = 0.5      # Pre-speech buffer (captures the start of words!)\n",
    "MAX_RECORDING = 10        # Maximum recording length (seconds)\n",
    "MIN_RECORDING = 0.5       # Minimum recording to process (seconds)\n",
    "\n",
    "# Calculate sizes in blocks/samples\n",
    "silence_blocks = int(SILENCE_DURATION * SAMPLE_RATE / BLOCK_SIZE)\n",
    "buffer_blocks = int(BUFFER_SECONDS * SAMPLE_RATE / BLOCK_SIZE)\n",
    "max_samples = int(MAX_RECORDING * SAMPLE_RATE)\n",
    "min_samples = int(MIN_RECORDING * SAMPLE_RATE)\n",
    "\n",
    "# State variables\n",
    "rolling_buffer = deque(maxlen=buffer_blocks)\n",
    "is_recording = False\n",
    "recorded_audio = []\n",
    "silence_counter = 0\n",
    "TEMP_FILE = \"_temp_continuous.wav\"\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Called for each block of audio from the microphone.\"\"\"\n",
    "    global is_recording, recorded_audio, silence_counter, rolling_buffer\n",
    "\n",
    "    audio_block = indata[:, 0].copy()\n",
    "    rms = np.sqrt(np.mean(audio_block**2))\n",
    "\n",
    "    if not is_recording:\n",
    "        # Keep audio in rolling buffer (this captures word beginnings!)\n",
    "        rolling_buffer.append(audio_block)\n",
    "\n",
    "        # Check for speech onset\n",
    "        if rms > VOLUME_THRESHOLD:\n",
    "            is_recording = True\n",
    "            silence_counter = 0\n",
    "            # Include the buffer contents!\n",
    "            recorded_audio = list(np.concatenate(list(rolling_buffer)))\n",
    "            recorded_audio.extend(audio_block.tolist())\n",
    "            print(\"\\r  [Recording...] Speak your sentence!\", end=\"\", flush=True)\n",
    "    else:\n",
    "        # Currently recording\n",
    "        recorded_audio.extend(audio_block.tolist())\n",
    "\n",
    "        if rms < VOLUME_THRESHOLD:\n",
    "            silence_counter += 1\n",
    "        else:\n",
    "            silence_counter = 0\n",
    "\n",
    "        # Stop recording after silence or max duration\n",
    "        total_samples = len(recorded_audio)\n",
    "        if (silence_counter >= silence_blocks and total_samples >= min_samples) or total_samples >= max_samples:\n",
    "            audio_data = np.array(recorded_audio, dtype=np.float32)\n",
    "\n",
    "            # Save and transcribe\n",
    "            sf.write(TEMP_FILE, audio_data, SAMPLE_RATE)\n",
    "\n",
    "            try:\n",
    "                result = model.transcribe(TEMP_FILE)\n",
    "                text = result['text'].strip()\n",
    "\n",
    "                if text:\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"=\" * 50)\n",
    "                    print(\"CONTINUOUS SPEECH RECOGNITION\")\n",
    "                    print(\"=\" * 50)\n",
    "                    print(f\"\\n  You said: \\\"{text}\\\"\")\n",
    "                    print(f\"  Language: {result['language']}\")\n",
    "                    duration = len(audio_data) / SAMPLE_RATE\n",
    "                    print(f\"  Duration: {duration:.1f}s\")\n",
    "                    print(f\"\\nListening... (speak to transcribe)\")\n",
    "                    print(\"Press the STOP button or Kernel > Interrupt to stop\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  Error: {e}\")\n",
    "\n",
    "            # Reset state\n",
    "            is_recording = False\n",
    "            recorded_audio = []\n",
    "            silence_counter = 0\n",
    "            rolling_buffer.clear()\n",
    "\n",
    "# Start listening\n",
    "print(\"=\" * 50)\n",
    "print(\"CONTINUOUS SPEECH RECOGNITION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Volume threshold: {VOLUME_THRESHOLD} RMS\")\n",
    "print(f\"Silence timeout:  {SILENCE_DURATION}s\")\n",
    "print(f\"Max recording:    {MAX_RECORDING}s\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nListening... (speak to transcribe)\")\n",
    "print(\"Press the STOP button or Kernel > Interrupt to stop\")\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        blocksize=BLOCK_SIZE,\n",
    "        channels=1,\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    while True:\n",
    "        sd.sleep(100)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if stream is not None:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        os.remove(TEMP_FILE)\n",
    "    rolling_buffer.clear()\n",
    "    is_recording = False\n",
    "    recorded_audio = []\n",
    "    silence_counter = 0\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Part 7: Triggering Actions with Speech\n",
    "\n",
    "Now that we have real-time transcription, we can use the **text** Whisper outputs to trigger actions in other programs! This is where speech recognition becomes truly interactive.\n",
    "\n",
    "You've already done this in previous classes:\n",
    "- **Class 1**: Pitch detection -> Arduino LED\n",
    "- **Class 2**: Digit recognition -> Actions\n",
    "\n",
    "Now instead of detecting *pitch* or *digits*, we can detect **any word or phrase**!\n",
    "\n",
    "### Ideas for Speech-Triggered Actions\n",
    "\n",
    "| Action | How | Example |\n",
    "|--------|-----|--------|\n",
    "| Print to console | Built-in Python | \"Turn on\" -> prints \"Activating!\" |\n",
    "| Send OSC message | `python-osc` library | \"Red\" -> sends color to p5.js |\n",
    "| Send Serial | `pyserial` library | \"Light on\" -> turns on Arduino LED |\n",
    "| Write to file | Built-in Python | Transcribe a meeting -> save notes |\n",
    "| Play a sound | `sounddevice` | \"Beep\" -> plays a tone |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPEECH COMMAND DETECTOR\n",
    "# =============================================================================\n",
    "# Records audio, transcribes it, and checks for keywords to trigger actions!\n",
    "# This is the foundation for voice-controlled projects.\n",
    "\n",
    "def on_command(command, full_text):\n",
    "    \"\"\"\n",
    "    This function is called when a keyword is detected.\n",
    "    CUSTOMIZE THIS to trigger whatever you want!\n",
    "\n",
    "    Ideas:\n",
    "    - Send OSC to p5.js\n",
    "    - Send serial to Arduino\n",
    "    - Play a sound\n",
    "    - Change a variable\n",
    "    - Write to a file\n",
    "    \"\"\"\n",
    "    print(f\"  >>> COMMAND DETECTED: '{command}' <<<\")\n",
    "    print(f\"  >>> Full text: '{full_text}' <<<\")\n",
    "\n",
    "\n",
    "# Define your keywords/commands - add whatever words you want to detect!\n",
    "COMMANDS = {\n",
    "    \"hello\": \"greeting\",\n",
    "    \"goodbye\": \"farewell\",\n",
    "    \"red\": \"color_red\",\n",
    "    \"blue\": \"color_blue\",\n",
    "    \"green\": \"color_green\",\n",
    "    \"on\": \"activate\",\n",
    "    \"off\": \"deactivate\",\n",
    "    \"yes\": \"affirm\",\n",
    "    \"no\": \"deny\",\n",
    "}\n",
    "\n",
    "RECORD_DURATION = 3  # seconds\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SPEECH COMMAND DETECTOR\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Listening for keywords: {list(COMMANDS.keys())}\")\n",
    "print(f\"Press Enter to record ({RECORD_DURATION}s), 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record: \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        # Record\n",
    "        print(f\"Recording for {RECORD_DURATION}s... Say a command!\")\n",
    "        recording = sd.rec(\n",
    "            int(RECORD_DURATION * SAMPLE_RATE),\n",
    "            samplerate=SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "\n",
    "        # Save and transcribe\n",
    "        temp_file = \"_temp_command.wav\"\n",
    "        sf.write(temp_file, recording, SAMPLE_RATE)\n",
    "        result = model.transcribe(temp_file)\n",
    "        text = result['text'].strip().lower()\n",
    "\n",
    "        print(f\"\\n  Heard: \\\"{text}\\\"\")\n",
    "\n",
    "        # Check for commands\n",
    "        found_command = False\n",
    "        for keyword, action in COMMANDS.items():\n",
    "            if keyword in text:\n",
    "                on_command(keyword, text)\n",
    "                found_command = True\n",
    "\n",
    "        if not found_command:\n",
    "            print(\"  (No command keyword detected)\")\n",
    "\n",
    "        # Clean up\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Example: Sending OSC Messages to p5.js\n",
    "\n",
    "You've already seen how to send OSC messages in previous classes. Here's how to combine speech recognition with OSC to control a p5.js sketch (or any OSC receiver) with your voice!\n",
    "\n",
    "**Install python-osc first:**\n",
    "```bash\n",
    "uv pip install python-osc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPEECH -> OSC: Control p5.js (or any OSC receiver) with your voice!\n",
    "# =============================================================================\n",
    "# This sends OSC messages based on what Whisper hears.\n",
    "# You'll need a p5.js sketch or other program listening for OSC messages.\n",
    "\n",
    "# Uncomment the lines below when you're ready to use OSC:\n",
    "# from pythonosc import udp_client\n",
    "# OSC_IP = \"127.0.0.1\"   # localhost (same computer)\n",
    "# OSC_PORT = 12000        # match this to your p5.js sketch\n",
    "# osc_client = udp_client.SimpleUDPClient(OSC_IP, OSC_PORT)\n",
    "\n",
    "def send_osc_from_speech(text):\n",
    "    \"\"\"\n",
    "    Analyze transcribed text and send OSC messages.\n",
    "    Customize this for your project!\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # Example 1: Send the full transcription as a string\n",
    "    # osc_client.send_message(\"/speech/text\", text)\n",
    "    print(f\"  [OSC] /speech/text -> '{text}'\")\n",
    "\n",
    "    # Example 2: Detect colors and send RGB values\n",
    "    colors = {\n",
    "        \"red\": [255, 0, 0],\n",
    "        \"blue\": [0, 0, 255],\n",
    "        \"green\": [0, 255, 0],\n",
    "        \"yellow\": [255, 255, 0],\n",
    "        \"purple\": [128, 0, 128],\n",
    "        \"white\": [255, 255, 255],\n",
    "        \"black\": [0, 0, 0],\n",
    "    }\n",
    "\n",
    "    for color_name, rgb in colors.items():\n",
    "        if color_name in text:\n",
    "            # osc_client.send_message(\"/speech/color\", rgb)\n",
    "            print(f\"  [OSC] /speech/color -> {rgb} ({color_name})\")\n",
    "\n",
    "    # Example 3: Detect numbers and send them\n",
    "    number_words = {\n",
    "        \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "        \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "    }\n",
    "\n",
    "    for word, num in number_words.items():\n",
    "        if word in text:\n",
    "            # osc_client.send_message(\"/speech/number\", num)\n",
    "            print(f\"  [OSC] /speech/number -> {num}\")\n",
    "\n",
    "\n",
    "# Quick test with some example sentences\n",
    "print(\"Testing OSC speech commands:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Make it red\",\n",
    "    \"Change to blue please\",\n",
    "    \"Set the number to five\",\n",
    "    \"Hello world\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    send_osc_from_speech(sentence)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"To use with real speech, combine with the\")\n",
    "print(\"real-time recording code from Part 6!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Example: Controlling Arduino with Speech\n",
    "\n",
    "Just like we controlled the LED with pitch in Class 1, we can now control it with **words**!\n",
    "\n",
    "Say \"on\" or \"light\" to turn the LED on, \"off\" or \"dark\" to turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPEECH -> SERIAL: Control Arduino with your voice!\n",
    "# =============================================================================\n",
    "# Says \"on\" or \"light\" -> sends '1' to Arduino (LED on)\n",
    "# Says \"off\" or \"dark\" -> sends '0' to Arduino (LED off)\n",
    "\n",
    "# Uncomment and configure when you have Arduino connected:\n",
    "# import serial\n",
    "# SERIAL_PORT = '/dev/cu.usbmodem...'  # Find yours: ls /dev/tty* | grep usb\n",
    "# BAUD_RATE = 9600\n",
    "# arduino = serial.Serial(SERIAL_PORT, BAUD_RATE)\n",
    "# time.sleep(2)  # Wait for Arduino reset\n",
    "\n",
    "def send_serial_from_speech(text):\n",
    "    \"\"\"\n",
    "    Analyze transcribed text and send serial commands to Arduino.\n",
    "    Customize this for your project!\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    if \"on\" in text or \"light\" in text:\n",
    "        # arduino.write(b'1')\n",
    "        print(f\"  [Serial] Sending '1' (LED ON)\")\n",
    "    elif \"off\" in text or \"dark\" in text:\n",
    "        # arduino.write(b'0')\n",
    "        print(f\"  [Serial] Sending '0' (LED OFF)\")\n",
    "    else:\n",
    "        print(f\"  [Serial] No command recognized in: '{text}'\")\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing Serial speech commands:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_sentences = [\"Turn on the light\", \"Turn it off\", \"Hello world\"]\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    send_serial_from_speech(sentence)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Combine with real-time recording from Part 6\")\n",
    "print(\"to control Arduino with your voice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PUTTING IT ALL TOGETHER: Live Speech -> Action\n",
    "# =============================================================================\n",
    "# This combines real-time recording with action triggering.\n",
    "# Customize the on_speech() function to do whatever you want!\n",
    "\n",
    "def on_speech(text):\n",
    "    \"\"\"\n",
    "    Called every time Whisper transcribes something.\n",
    "    This is YOUR function to customize!\n",
    "\n",
    "    The 'text' parameter is the transcribed speech as a string.\n",
    "    You can:\n",
    "      - Check for keywords\n",
    "      - Send OSC messages\n",
    "      - Send serial to Arduino\n",
    "      - Play sounds\n",
    "      - Write to a file\n",
    "      - Anything you can do with Python!\n",
    "    \"\"\"\n",
    "    text_lower = text.strip().lower()\n",
    "    print(f\"  Transcription: \\\"{text}\\\"\")\n",
    "\n",
    "    # === ADD YOUR CUSTOM ACTIONS HERE ===\n",
    "\n",
    "    # Example: keyword detection\n",
    "    if \"hello\" in text_lower:\n",
    "        print(\"  -> Hello to you too!\")\n",
    "    if \"red\" in text_lower:\n",
    "        print(\"  -> Color: RED detected!\")\n",
    "        # osc_client.send_message(\"/color\", [255, 0, 0])\n",
    "    if \"stop\" in text_lower:\n",
    "        print(\"  -> STOP command received!\")\n",
    "\n",
    "\n",
    "RECORD_DURATION = 4  # seconds per recording\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LIVE SPEECH -> ACTION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Speak and watch your custom actions trigger!\")\n",
    "print(\"Press Enter to record, 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record: \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        print(f\"Recording {RECORD_DURATION}s... Speak now!\")\n",
    "        recording = sd.rec(\n",
    "            int(RECORD_DURATION * SAMPLE_RATE),\n",
    "            samplerate=SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "\n",
    "        temp_file = \"_temp_action.wav\"\n",
    "        sf.write(temp_file, recording, SAMPLE_RATE)\n",
    "        result = model.transcribe(temp_file)\n",
    "\n",
    "        # Call YOUR custom function!\n",
    "        on_speech(result['text'])\n",
    "\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Part 8: Fine-Tuning Whisper (Stretch Goal)\n",
    "\n",
    "**What is fine-tuning?** Taking a pre-trained model (like Whisper) and training it further on your own specific data to make it better at a particular task.\n",
    "\n",
    "Think of it like this: Whisper already knows how to understand speech in general. Fine-tuning teaches it to be *extra good* at understanding a specific type of speech.\n",
    "\n",
    "### Why Fine-Tune?\n",
    "\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| **Specialized vocabulary** | Medical terms, legal jargon, technical language |\n",
    "| **Specific accents** | Regional dialects, non-native speakers |\n",
    "| **Low-resource languages** | Languages with limited Whisper training data |\n",
    "| **Noisy environments** | Factories, concerts, underwater |\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Fine-tuning is **computationally expensive**. You'll need:\n",
    "- An **NVIDIA GPU** with CUDA support, OR\n",
    "- **Google Colab** (free GPU access!)\n",
    "- The `transformers` and `datasets` libraries from HuggingFace\n",
    "\n",
    "### Finding Training Datasets\n",
    "\n",
    "HuggingFace has thousands of speech datasets you can use:\n",
    "\n",
    "- [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_16_1) - Mozilla's crowdsourced speech dataset (100+ languages)\n",
    "- [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - English audiobook recordings\n",
    "- [FLEURS](https://huggingface.co/datasets/google/fleurs) - Google's multilingual speech benchmark\n",
    "- [Browse all ASR datasets](https://huggingface.co/datasets?task_categories=automatic-speech-recognition)\n",
    "\n",
    "### Running on Google Colab\n",
    "\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Create a new notebook\n",
    "3. Go to **Runtime -> Change runtime type -> GPU (T4)**\n",
    "4. Copy the code from the cell below into Colab\n",
    "5. Uncomment everything and run!\n",
    "\n",
    "**Detailed guide**: [HuggingFace Fine-Tune Whisper Blog Post](https://huggingface.co/blog/fine-tune-whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINE-TUNING WHISPER (GPU / GOOGLE COLAB RECOMMENDED)\n",
    "# =============================================================================\n",
    "# This code shows how to fine-tune Whisper on a custom dataset\n",
    "# using HuggingFace Transformers.\n",
    "#\n",
    "# To run this:\n",
    "#   1. Go to colab.research.google.com\n",
    "#   2. Runtime -> Change runtime type -> GPU (T4)\n",
    "#   3. Install: !pip install transformers datasets evaluate jiwer accelerate\n",
    "#   4. Uncomment the code below and run!\n",
    "\n",
    "# --- Uncomment everything below to run on Google Colab ---\n",
    "\n",
    "# from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "# from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# from datasets import load_dataset, Audio\n",
    "# import torch\n",
    "\n",
    "# # Step 1: Load the pre-trained Whisper model and processor\n",
    "# model_name = \"openai/whisper-tiny\"  # Start small for faster experiments!\n",
    "# processor = WhisperProcessor.from_pretrained(model_name)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# # Step 2: Load a dataset from HuggingFace\n",
    "# # This example uses Common Voice for Hindi - change the language code!\n",
    "# dataset = load_dataset(\n",
    "#     \"mozilla-foundation/common_voice_16_1\",\n",
    "#     \"hi\",  # Language code: \"hi\"=Hindi, \"es\"=Spanish, \"fr\"=French, etc.\n",
    "#     split=\"train[:100]\",  # Start with a small subset for testing\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# # Step 3: Preprocess the audio data\n",
    "# def prepare_dataset(batch):\n",
    "#     audio = batch[\"audio\"]\n",
    "#     # Convert audio to the format Whisper expects\n",
    "#     input_features = processor(\n",
    "#         audio[\"array\"],\n",
    "#         sampling_rate=audio[\"sampling_rate\"],\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).input_features[0]\n",
    "#     # Encode the target text\n",
    "#     batch[\"input_features\"] = input_features\n",
    "#     batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "# # Resample all audio to 16kHz (what Whisper expects)\n",
    "# dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "# dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
    "\n",
    "# # Step 4: Set up training\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-finetuned\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     learning_rate=1e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     fp16=True,  # Mixed precision (faster on GPU)\n",
    "#     logging_steps=10,\n",
    "#     save_steps=100,\n",
    "#     predict_with_generate=True,\n",
    "# )\n",
    "\n",
    "# # Step 5: Create trainer and fine-tune!\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "# )\n",
    "\n",
    "# # This starts fine-tuning! It will take a few minutes on a T4 GPU.\n",
    "# # trainer.train()\n",
    "\n",
    "# # Step 6: Save the fine-tuned model\n",
    "# # model.save_pretrained(\"./whisper-finetuned-final\")\n",
    "# # processor.save_pretrained(\"./whisper-finetuned-final\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING WHISPER - STRETCH GOAL\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"This code is designed to run on Google Colab with a GPU.\")\n",
    "print()\n",
    "print(\"To try it:\")\n",
    "print(\"  1. Go to colab.research.google.com\")\n",
    "print(\"  2. Create a new notebook\")\n",
    "print(\"  3. Runtime -> Change runtime type -> GPU (T4)\")\n",
    "print(\"  4. !pip install transformers datasets evaluate jiwer accelerate\")\n",
    "print(\"  5. Copy this cell's code, uncomment everything, and run!\")\n",
    "print()\n",
    "print(\"Resources:\")\n",
    "print(\"  - HuggingFace guide: https://huggingface.co/blog/fine-tune-whisper\")\n",
    "print(\"  - Google Colab: https://colab.research.google.com\")\n",
    "print(\"  - Datasets: https://huggingface.co/datasets?task_categories=automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Part 9: Exploring Other ASR Systems\n",
    "\n",
    "Whisper is amazing, but it's just one of many ASR systems available today. Here's a tour of what's out there!\n",
    "\n",
    "## Open Source (Run Locally - No API Key Needed)\n",
    "\n",
    "| System | By | Highlights |\n",
    "|--------|----|------------|\n",
    "| [**Whisper**](https://github.com/openai/whisper) | OpenAI | 99+ languages, translation, timestamps |\n",
    "| [**Faster Whisper**](https://github.com/SYSTRAN/faster-whisper) | SYSTRAN | Same as Whisper but 4x faster! Uses CTranslate2 |\n",
    "| [**Distil-Whisper**](https://github.com/huggingface/distil-whisper) | HuggingFace | 6x faster than Whisper, nearly same accuracy |\n",
    "| [**Wav2Vec 2.0**](https://github.com/facebookresearch/fairseq) | Meta | Self-supervised learning, great for low-resource languages |\n",
    "| [**NeMo Canary/Parakeet**](https://github.com/NVIDIA/NeMo) | NVIDIA | State-of-the-art English ASR, extremely accurate |\n",
    "| [**Vosk**](https://alphacephei.com/vosk/) | Alpha Cephei | Lightweight, offline, runs on Raspberry Pi! |\n",
    "\n",
    "## Cloud APIs (Requires Internet + API Key)\n",
    "\n",
    "| Service | By | Highlights |\n",
    "|---------|----|------------|\n",
    "| [**AssemblyAI**](https://www.assemblyai.com) | AssemblyAI | Excellent accuracy, speaker diarization, sentiment |\n",
    "| [**Deepgram**](https://deepgram.com) | Deepgram | Ultra-fast, real-time streaming, custom models |\n",
    "| [**Voxtral**](https://mistral.ai) | Mistral | Multimodal model - understands speech AND context |\n",
    "| [**Google Cloud Speech**](https://cloud.google.com/speech-to-text) | Google | 125+ languages, streaming, specialized models |\n",
    "| [**Azure Speech**](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) | Microsoft | Real-time, batch, custom pronunciation |\n",
    "| [**Amazon Transcribe**](https://aws.amazon.com/transcribe/) | Amazon | Real-time, medical transcription, call analytics |\n",
    "| [**Speechmatics**](https://www.speechmatics.com) | Speechmatics | 50+ languages, excellent accuracy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## AssemblyAI\n",
    "\n",
    "[AssemblyAI](https://www.assemblyai.com) is one of the most popular cloud ASR APIs. Beyond basic transcription, it offers:\n",
    "- **Speaker diarization** (who said what)\n",
    "- **Sentiment analysis** (positive/negative/neutral)\n",
    "- **Topic detection**\n",
    "- **Content moderation**\n",
    "- Free tier available!\n",
    "\n",
    "```bash\n",
    "uv pip install assemblyai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ASSEMBLYAI EXAMPLE (Requires free API Key)\n",
    "# =============================================================================\n",
    "# Sign up at https://www.assemblyai.com for a free API key\n",
    "# Then uncomment the code below!\n",
    "\n",
    "# import assemblyai as aai\n",
    "\n",
    "# # Set your API key\n",
    "# aai.settings.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# # Create a transcriber\n",
    "# transcriber = aai.Transcriber()\n",
    "\n",
    "# # Transcribe a local audio file\n",
    "# transcript = transcriber.transcribe(RECORDING_FILE)\n",
    "# print(f\"Text: {transcript.text}\")\n",
    "\n",
    "# # AssemblyAI can also identify different speakers!\n",
    "# config = aai.TranscriptionConfig(speaker_labels=True)\n",
    "# transcript = transcriber.transcribe(RECORDING_FILE, config=config)\n",
    "# if transcript.utterances:\n",
    "#     for utterance in transcript.utterances:\n",
    "#         print(f\"Speaker {utterance.speaker}: {utterance.text}\")\n",
    "\n",
    "print(\"AssemblyAI example (uncomment code and add API key to try!)\")\n",
    "print(\"Sign up for free: https://www.assemblyai.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Deepgram\n",
    "\n",
    "[Deepgram](https://deepgram.com) is known for **speed** and **real-time streaming**. It's great for:\n",
    "- Ultra-low latency transcription\n",
    "- Live streaming (WebSocket-based)\n",
    "- Custom model training on your data\n",
    "- Free tier available!\n",
    "\n",
    "```bash\n",
    "uv pip install deepgram-sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEEPGRAM EXAMPLE (Requires free API Key)\n",
    "# =============================================================================\n",
    "# Sign up at https://deepgram.com for a free API key\n",
    "# Then uncomment the code below!\n",
    "\n",
    "# from deepgram import DeepgramClient, PrerecordedOptions\n",
    "\n",
    "# DEEPGRAM_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "# deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n",
    "\n",
    "# # Read audio file\n",
    "# with open(RECORDING_FILE, \"rb\") as audio_file:\n",
    "#     buffer_data = audio_file.read()\n",
    "\n",
    "# # Configure transcription\n",
    "# options = PrerecordedOptions(\n",
    "#     model=\"nova-3\",         # Deepgram's latest model\n",
    "#     smart_format=True,      # Auto-punctuation and formatting\n",
    "#     language=\"en\",\n",
    "# )\n",
    "\n",
    "# # Transcribe\n",
    "# payload = {\"buffer\": buffer_data}\n",
    "# response = deepgram.listen.rest.v(\"1\").transcribe_file(payload, options)\n",
    "# transcript = response.results.channels[0].alternatives[0].transcript\n",
    "# print(f\"Text: {transcript}\")\n",
    "\n",
    "print(\"Deepgram example (uncomment code and add API key to try!)\")\n",
    "print(\"Sign up for free: https://deepgram.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## More Systems to Explore\n",
    "\n",
    "### Faster Whisper\n",
    "\n",
    "If Whisper feels slow on your machine, try [Faster Whisper](https://github.com/SYSTRAN/faster-whisper). It uses the same model weights but runs them through CTranslate2, which is optimized for CPU inference. The result: **~4x faster** with almost identical accuracy!\n",
    "\n",
    "```bash\n",
    "uv pip install faster-whisper\n",
    "```\n",
    "\n",
    "```python\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\"tiny\", device=\"cpu\", compute_type=\"int8\")\n",
    "segments, info = model.transcribe(\"audio.wav\")\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "```\n",
    "\n",
    "### Voxtral (Mistral)\n",
    "\n",
    "[Voxtral](https://mistral.ai) is Mistral's multimodal model that can understand speech. Unlike traditional ASR systems that just convert speech to text, Voxtral can answer *questions* about audio content - \"What language is being spoken?\" \"What is the speaker's mood?\" \"Summarize this conversation.\"\n",
    "\n",
    "### NVIDIA NeMo (Canary / Parakeet)\n",
    "\n",
    "If you have an NVIDIA GPU, [NeMo](https://github.com/NVIDIA/NeMo) offers some of the most accurate ASR models available, especially for English. The Parakeet models consistently top the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).\n",
    "\n",
    "### Vosk\n",
    "\n",
    "[Vosk](https://alphacephei.com/vosk/) is perfect for **embedded** and **edge** devices. It runs on Raspberry Pi, Android, iOS, and more with very small model sizes (as small as 50MB). Great for offline applications where you need speech recognition without internet access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Quick Reference: Which ASR System Should I Use?\n",
    "\n",
    "| If you need... | Use this |\n",
    "|---------------|----------|\n",
    "| Free, local, multilingual | **Whisper** |\n",
    "| Fast local transcription | **Faster Whisper** or **Distil-Whisper** |\n",
    "| Best cloud accuracy | **AssemblyAI** or **Deepgram** |\n",
    "| Real-time streaming (cloud) | **Deepgram** |\n",
    "| Speaker identification | **AssemblyAI** or **Google Cloud** |\n",
    "| Runs on Raspberry Pi | **Vosk** |\n",
    "| Best English accuracy (GPU) | **NVIDIA NeMo Parakeet** |\n",
    "| Multimodal understanding | **Voxtral** |\n",
    "| Fine-tuning on custom data | **Whisper** via HuggingFace |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Exercises and Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Transcribe and Visualize\n",
    "#\n",
    "# Record yourself reading a paragraph or poem.\n",
    "# Use word-level timestamps to create a visualization that highlights\n",
    "# each word on the waveform as it appears in time.\n",
    "#\n",
    "# Hint: Use the word timestamp visualization from Part 4 as a starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Voice-Controlled p5.js\n",
    "#\n",
    "# Create a p5.js sketch that responds to voice commands sent over OSC.\n",
    "# Use the speech -> OSC example from Part 7 as a starting point.\n",
    "#\n",
    "# Ideas:\n",
    "#   - \"Circle\" / \"Square\" / \"Triangle\" -> draw different shapes\n",
    "#   - Color names -> change the background color\n",
    "#   - \"Bigger\" / \"Smaller\" -> resize elements\n",
    "#   - \"Clear\" -> reset the canvas\n",
    "#   - \"Fast\" / \"Slow\" -> change animation speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Language Quiz\n",
    "#\n",
    "# Build a \"language detection quiz\":\n",
    "#   1. Record yourself saying a phrase in different languages\n",
    "#      (use Google Translate to learn phrases if needed!)\n",
    "#   2. See if Whisper correctly identifies each language\n",
    "#   3. Track accuracy in a dictionary and display results\n",
    "#\n",
    "# Bonus: Try the same phrases with tiny vs base models - which is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (Stretch): Real-Time Subtitles\n",
    "#\n",
    "# Create a real-time subtitle system that:\n",
    "#   1. Continuously listens to your microphone\n",
    "#   2. Transcribes speech in chunks\n",
    "#   3. Displays a scrolling transcript (like live captions on TV)\n",
    "#\n",
    "# Hint: Use the continuous listening code from Part 6 and keep\n",
    "#       a running list of all transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 (Stretch): Compare ASR Accuracy\n",
    "#\n",
    "# Record the same 5 sentences and transcribe them with:\n",
    "#   - Whisper tiny\n",
    "#   - Whisper base\n",
    "#   - Whisper small (if your machine can handle it)\n",
    "#\n",
    "# Compare the results:\n",
    "#   - Which words does each model get wrong?\n",
    "#   - How much slower are the larger models?\n",
    "#   - Is the accuracy improvement worth the speed cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 (Stretch - GPU Required): Fine-Tune Whisper\n",
    "#\n",
    "# Follow the fine-tuning guide in Part 8 on Google Colab.\n",
    "# Pick a language or domain and fine-tune Whisper on a small dataset.\n",
    "# Compare the fine-tuned model's accuracy to the original!\n",
    "#\n",
    "# Resources:\n",
    "#   - HuggingFace guide: https://huggingface.co/blog/fine-tune-whisper\n",
    "#   - Google Colab: https://colab.research.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "- Try [Faster Whisper](https://github.com/SYSTRAN/faster-whisper) and compare its speed to regular Whisper\n",
    "- Build a voice-controlled Arduino project using full speech recognition (not just pitch!)\n",
    "- Create a real-time translation demo: speak in one language, display text in English\n",
    "- Sign up for [AssemblyAI](https://assemblyai.com) or [Deepgram](https://deepgram.com) and compare their results to Whisper\n",
    "- Explore [HuggingFace's ASR models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition) - there are hundreds!\n",
    "- Build a \"speech diary\" that records and transcribes throughout the day\n",
    "- Use word-level timestamps to create a karaoke-style lyric display\n",
    "- Combine Whisper with the emotion recognition from Class 3: detect *what* someone says AND *how* they say it!\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook you learned:\n",
    "\n",
    "- **How modern ASR works**: audio -> mel spectrogram -> encoder -> decoder -> text\n",
    "- **Whisper**: OpenAI's open-source, multilingual ASR model\n",
    "- **Transcription**: Converting speech to text with a single function call\n",
    "- **Features**: Language detection, translation, word-level timestamps\n",
    "- **Model sizes**: Trading off speed vs accuracy (tiny -> large)\n",
    "- **Real-time recognition**: Record-then-transcribe approach for live speech\n",
    "- **Triggering actions**: Using transcribed text to control other systems (OSC, Serial)\n",
    "- **Fine-tuning**: Customizing Whisper for specific tasks (stretch goal)\n",
    "- **The ASR landscape**: Whisper, Faster Whisper, AssemblyAI, Deepgram, and more\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you can convert speech to text, the possibilities are endless. You can build voice-controlled art installations, real-time captioning systems, multilingual translation tools, and interactive speech-driven experiences. The `on_speech()` function from Part 7 is your starting point - customize it and make something amazing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
