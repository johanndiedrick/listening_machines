{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Audrey - Building a Speech Recognition System\n",
    "\n",
    "In 1952, Bell Labs created **AUDREY** (Automatic Digit Recognition), the first speech recognition system. It could recognize spoken digits 0-9 with ~90% accuracy - but only for a single speaker!\n",
    "\n",
    "In this notebook, we'll build our own version of Audrey using modern deep learning. Along the way, you'll learn:\n",
    "\n",
    "- **Data Collection**: Recording your own speech dataset\n",
    "- **Data Augmentation**: Creating variations to make your model more robust  \n",
    "- **Neural Networks**: Training both simple (MLP) and convolutional (CNN) models\n",
    "- **Inference**: Using your trained model for real-time digit recognition\n",
    "\n",
    "**Key Concept**: Machine learning models learn patterns from data. The more varied and representative your training data, the better your model will generalize to new inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Part 1: Recording Your Dataset\n",
    "\n",
    "Every machine learning project starts with **data**. We need examples of what we want our model to recognize.\n",
    "\n",
    "For speech recognition, we need audio recordings of each digit (0-9). You'll record yourself saying each digit, creating 10 audio files that will form the basis of our training data.\n",
    "\n",
    "**Why record yourself?** Like the original Audrey, our model will be \"speaker-dependent\" - trained on your voice. This makes the problem easier to solve with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our own speech dataset! This work with numbers first\n",
    "\n",
    "# Recording audio with sounddevice and soundfile\n",
    "# \n",
    "# https://python-soundfile.readthedocs.io/\n",
    "# https://python-sounddevice.readthedocs.io\n",
    "\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def record_audio(filename: str, duration: int):\n",
    "\n",
    "    # config\n",
    "    samplerate = 44100\n",
    "    duration = duration\n",
    "    channels = 1\n",
    "\n",
    "    print(f\"Recording for {duration} seconds at {samplerate} Hz...\")\n",
    "\n",
    "    # record audio from the microphone into a numpy array with sounddevice\n",
    "    recording = sd.rec(\n",
    "        int(duration * samplerate),\n",
    "        samplerate=samplerate,\n",
    "        channels=channels,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    sd.wait()\n",
    "\n",
    "    print(f\"Recording finished. Saving to {filename}...\")\n",
    "\n",
    "    # save the file with soundfile\n",
    "    sf.write(\n",
    "        filename,\n",
    "        recording,\n",
    "        samplerate,\n",
    "        subtype='PCM_16'\n",
    "        )\n",
    "\n",
    "    print(f\"File '{filename}' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record yourself saying the digits 0-9. Say each number clearly and distinctly. This will matter a lot later on!\n",
    "\n",
    "record_audio(\n",
    "    filename='unprocessed/0.wav', # rename the file to 1.wav, 2.wav, 3.wav, etc.\n",
    "    duration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Part 2: Data Augmentation\n",
    "\n",
    "We only have 10 recordings, but neural networks typically need thousands of examples to learn well. \n",
    "\n",
    "**Data augmentation** solves this by creating variations of our original data:\n",
    "- **Noise**: Adding random background noise (simulates different environments)\n",
    "- **Time Stretch**: Making audio faster/slower (simulates speaking pace variation)\n",
    "- **Pitch Shift**: Raising/lowering pitch (simulates voice variation)\n",
    "- **Time Shift**: Moving audio left/right (simulates different recording starts)\n",
    "\n",
    "From 10 original recordings, we'll create **10,000 augmented samples** (1,000 variations per digit)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation functions - each creates a different variation of the audio\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def noise(data, noise_amt=0.035):\n",
    "    \"\"\"Add random background noise to simulate different recording environments\"\"\"\n",
    "    noise_amp = noise_amt * np.random.uniform() * np.amax(data)\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    \"\"\"Speed up or slow down the audio (rate < 1 = slower, rate > 1 = faster)\"\"\"\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "def shift(data):\n",
    "    \"\"\"Shift audio left or right in time (simulates different recording starts)\"\"\"\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, n_steps=2):\n",
    "    \"\"\"Shift pitch up or down (n_steps = semitones, + = higher, - = lower)\"\"\"\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take our recorded digits, and augment them to create a larger dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# get all files in the 'unprocessed' directory (only .wav files)\n",
    "files = glob.glob('unprocessed/*.wav')\n",
    "print(files)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    # get the digit from the file name\n",
    "    digit = file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # create the directory if it doesn't exist\n",
    "    os.makedirs(f'processed/{digit}', exist_ok=True)\n",
    "    # load file with sf\n",
    "    audio, sample_rate = sf.read(file)\n",
    "\n",
    "    for i in tqdm(range(1000)):\n",
    "        \n",
    "        processed_audio = noise(audio, np.random.uniform(0.001, 0.01))\n",
    "        processed_audio = stretch(processed_audio, rate=np.random.uniform(0.8, 1.2))\n",
    "        processed_audio = shift(processed_audio)\n",
    "        processed_audio = pitch(processed_audio, sample_rate, n_steps=np.random.randint(-3, 3))\n",
    "\n",
    "        sf.write(f'processed/{digit}/{digit}_{i}.wav', processed_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the fies in speech_digits with glob\n",
    "import glob\n",
    "\n",
    "files = glob.glob('processed/*/*')\n",
    "\n",
    "print(len(files))\n",
    "print(files[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing\n",
    "\n",
    "Before training, we need to prepare our data:\n",
    "\n",
    "1. **Consistent Length**: Neural networks expect fixed-size inputs. We'll pad shorter audio files to match the longest one.\n",
    "\n",
    "2. **Labels**: Each file needs a label (0-9) so the model knows what it should predict.\n",
    "\n",
    "3. **Verification**: Check that all files are the same length after processing.\n",
    "\n",
    "**Why padding?** Think of it like standardizing paper sizes before putting them in a binder - everything needs to fit the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "digit_dir = 'processed'\n",
    "\n",
    "# Get all files in the digit directory\n",
    "files = glob.glob('processed/*/*')\n",
    "print(files)\n",
    "\n",
    "# Display spectogram and audio player for each file\n",
    "for file in files[:5]:\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    \n",
    "    # Display the spectogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f'Waveform for {os.path.basename(file)}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the spectogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Spectrogram for {os.path.basename(file)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    display(Audio(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass: preprocess audio files so that they are all the same length\n",
    "\n",
    "import glob\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "files = glob.glob('processed/*/*')\n",
    "print(f\"Total files: {len(files)}\")\n",
    "\n",
    "audio_data = []\n",
    "labels = [] # here is where we create our labels\n",
    "longest_audio_file_length = 0\n",
    "\n",
    "# First pass: load data and find longest audio file\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f)\n",
    "        if len(audio) == 0:\n",
    "            print(f\"Warning: Empty audio file: {f}\")\n",
    "            continue\n",
    "        labels.append(int(f.split('/')[-2]))  # Adjust this based on your file structure\n",
    "        longest_audio_file_length = max(longest_audio_file_length, len(audio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")\n",
    "\n",
    "print(f\"Longest audio size: {longest_audio_file_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass: Pad audio files and resave them\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f)\n",
    "        if len(audio) == 0:\n",
    "            print(f\"Warning: Empty audio file: {f}\")\n",
    "            continue\n",
    "        current_size = len(audio)\n",
    "        pad_size = longest_audio_file_length - current_size\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        padded_audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "        sf.write(f, padded_audio, sample_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third pass: Verify that all files have the same size\n",
    "file_sizes = []\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, _ = librosa.load(f)\n",
    "        file_sizes.append(len(audio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")\n",
    "\n",
    "if len(set(file_sizes)) == 1:\n",
    "    print(f\"All files have the same size: {file_sizes[0]} samples\")\n",
    "else:\n",
    "    print(\"Warning: Not all files have the same size\")\n",
    "    print(f\"Unique file sizes: {set(file_sizes)}\")\n",
    "    print(f\"Min size: {min(file_sizes)}, Max size: {max(file_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[1000:1005])\n",
    "print(labels[1000:1005])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Part 4: Creating the Dataset and DataLoader\n",
    "\n",
    "Now we convert our audio files into a format PyTorch can use for training.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Mel Spectrogram**: Instead of feeding raw audio waveforms to our model, we convert them to **mel spectrograms** - visual representations of sound that show frequency content over time. This is similar to how humans perceive sound!\n",
    "\n",
    "**Dataset**: A PyTorch class that holds our data and knows how to load individual samples.\n",
    "\n",
    "**DataLoader**: Handles batching (grouping samples together) and shuffling during training.\n",
    "\n",
    "### Train/Validation/Test Split (Critical!)\n",
    "\n",
    "- **Training set (70%)**: What the model learns from\n",
    "- **Validation set (20%)**: Used to check for overfitting during training\n",
    "- **Test set (10%)**: Held out completely - only used for final evaluation\n",
    "\n",
    "**Avoiding Data Leakage**: All 10,000 augmented files come from just 10 original recordings. If we randomly split, variations of the same source end up in both train AND test - the model learns to recognize the specific recording rather than the digit!\n",
    "\n",
    "**Our solution**: Split **proportionally within each digit**. For each digit's 1,000 files, the first 700 go to train, next 200 to validation, last 100 to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class - tells PyTorch how to load our audio data\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transforms=transforms.MelSpectrogram()):\n",
    "        self.file_paths = file_paths  # List of paths to audio files\n",
    "        self.labels = labels          # List of corresponding labels (0-9)\n",
    "        self.transforms = transforms  # MelSpectrogram converts audio to spectrogram\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"How many samples in the dataset?\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load and return a single sample (called by DataLoader)\"\"\"\n",
    "        audio_path = self.file_paths[idx]\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Ensure mono audio (single channel)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "        # Convert waveform to mel spectrogram\n",
    "        if self.transforms:\n",
    "            spec = self.transforms(waveform)\n",
    "        return spec, self.labels[idx]\n",
    "\n",
    "\n",
    "# Split files BY DIGIT to avoid data leakage\n",
    "# (Augmentations of the same source recording stay in the same set)\n",
    "import glob\n",
    "\n",
    "train_files, train_labels = [], []\n",
    "val_files, val_labels = [], []\n",
    "test_files, test_labels = [], []\n",
    "\n",
    "for digit in range(10):\n",
    "    # Get all files for this digit, sorted by name (keeps similar augmentations together)\n",
    "    digit_files = sorted(glob.glob(f'processed/{digit}/*.wav'))\n",
    "    n = len(digit_files)\n",
    "    \n",
    "    # Split: first 70% train, next 20% val, last 10% test\n",
    "    train_end = int(0.7 * n)\n",
    "    val_end = int(0.9 * n)\n",
    "    \n",
    "    train_files.extend(digit_files[:train_end])\n",
    "    train_labels.extend([digit] * train_end)\n",
    "    \n",
    "    val_files.extend(digit_files[train_end:val_end])\n",
    "    val_labels.extend([digit] * (val_end - train_end))\n",
    "    \n",
    "    test_files.extend(digit_files[val_end:])\n",
    "    test_labels.extend([digit] * (n - val_end))\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Test samples: {len(test_files)}\")\n",
    "\n",
    "# Create separate datasets for each split (no random_split needed!)\n",
    "train_dataset = AudioDataset(train_files, train_labels, transforms=transforms.MelSpectrogram())\n",
    "validation_dataset = AudioDataset(val_files, val_labels, transforms=transforms.MelSpectrogram())\n",
    "test_dataset = AudioDataset(test_files, test_labels, transforms=transforms.MelSpectrogram())\n",
    "\n",
    "# DataLoaders handle batching and shuffling\n",
    "# batch_size=32 means we process 32 samples at a time\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(validation_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see a batch\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(inputs.shape)\n",
    "    print(inputs[0][0].shape)\n",
    "    print(targets)\n",
    "    break\n",
    "\n",
    "\n",
    "mel_freq_bins = inputs[0][0].shape[0]\n",
    "time_steps = inputs[0][0].shape[1]\n",
    "\n",
    "print(\"mel freq bins: \", mel_freq_bins)\n",
    "print(\"time steps: \", time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with a simple Multi-Layer Perceptron (MLP) - Fully-Connected Neural Network\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Flatten(),\n",
    "    t.nn.Linear(mel_freq_bins*time_steps, 512), # 128 mel bins, 366 time steps\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(512, 512),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(512, 10),\n",
    "    t.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# train our model\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(f\"Training for {epochs} epochs\")\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# evaluate our model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with t.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = t.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the model on the test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Understanding the MLP Above\n",
    "\n",
    "The cell above trained a **Multi-Layer Perceptron (MLP)** - the simplest neural network architecture.\n",
    "\n",
    "**How it works:**\n",
    "1. **Flatten** the 2D spectrogram into a 1D vector (loses spatial structure)\n",
    "2. Pass through **linear layers** that learn weights for each input\n",
    "3. **ReLU activation** adds non-linearity: `output = max(0, input)`\n",
    "4. Final layer outputs 10 values (one per digit)\n",
    "\n",
    "**Training Loop:**\n",
    "1. **Forward pass**: Feed data through the network to get predictions\n",
    "2. **Calculate loss**: Measure how wrong the predictions are (CrossEntropyLoss)\n",
    "3. **Backward pass**: Calculate gradients (how to adjust weights)\n",
    "4. **Update weights**: Use optimizer (Adam) to improve the model\n",
    "\n",
    "MLPs work, but they don't understand spatial relationships in the spectrogram. Let's try something better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network for audio classification\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "# Use GPU if available (much faster training!)\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, mel_freq_bins, time_steps, num_classes=10):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size of the flattened features (using explicit parameters)\n",
    "        self.flat_features = 128 * (mel_freq_bins // 8) * (time_steps // 8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, 128, 366)\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, self.flat_features) # rewrite this line with einops / ARENA\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Understanding the CNN Above\n",
    "\n",
    "The cell above defined a **Convolutional Neural Network (CNN)** - designed for data with spatial structure like images and spectrograms.\n",
    "\n",
    "**Key Components in ConvModel:**\n",
    "\n",
    "| Layer | Purpose |\n",
    "|-------|---------|\n",
    "| `Conv2d` | Slides small filters across the input, detecting local patterns |\n",
    "| `ReLU` | Activation function: `output = max(0, input)` |\n",
    "| `MaxPool2d` | Reduces dimensions by keeping max value in each region |\n",
    "| `Dropout` | Randomly disables neurons during training (prevents overfitting) |\n",
    "| `Linear` | Fully connected layers for final classification |\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "Spectrogram → [Conv→ReLU→Pool] x3 → Flatten → Linear → Linear → 10 digit scores\n",
    "```\n",
    "\n",
    "**Why CNNs beat MLPs:** Convolutions can detect patterns (like frequency bands) regardless of where they appear in the spectrogram. The pooling layers make the network robust to small shifts in timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model (passing mel_freq_bins and time_steps explicitly)\n",
    "\n",
    "conv_model = ConvModel(mel_freq_bins=mel_freq_bins, time_steps=time_steps)\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN with proper metric tracking\n",
    "# We'll track TRAINING LOSS, VALIDATION LOSS, and VALIDATION ACCURACY each epoch\n",
    "\n",
    "conv_model = conv_model.to(device)\n",
    "\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# Track metrics for each epoch\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "print(f\"Training for {epochs} epochs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # =====================\n",
    "    # TRAINING PHASE\n",
    "    # =====================\n",
    "    conv_model.train()  # Enable dropout, batch norm training mode\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()       # Clear gradients from last batch\n",
    "        outputs = conv_model(inputs) # Forward pass\n",
    "        loss = loss_fn(outputs, targets)  # Calculate loss\n",
    "        loss.backward()             # Backpropagation (compute gradients)\n",
    "        optimizer.step()            # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    \n",
    "    # =====================\n",
    "    # VALIDATION PHASE\n",
    "    # =====================\n",
    "    conv_model.eval()  # Disable dropout, use running stats for batch norm\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with t.no_grad():  # Don't compute gradients for validation\n",
    "        for batch in validation_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = conv_model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = t.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy_history.append(val_accuracy)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_accuracy:.1f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Finished training!\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_loss_history, label='Training Loss', marker='o')\n",
    "axes[0].plot(val_loss_history, label='Validation Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training vs Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(val_accuracy_history, label='Validation Accuracy', marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {val_accuracy_history[-1]:.1f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Understanding the Training Curves\n",
    "\n",
    "The plots above show the **learning progress** of our neural network. Here's how to read them:\n",
    "\n",
    "#### Training Loss vs Validation Loss\n",
    "\n",
    "| Pattern | What it means | What to do |\n",
    "|---------|---------------|------------|\n",
    "| Both decreasing together | Model is learning well! | Keep training |\n",
    "| Training ↓, Validation ↑ | **Overfitting!** Model memorizes training data | Stop earlier, add regularization, or get more data |\n",
    "| Both high and flat | **Underfitting.** Model can't learn patterns | Train longer, use bigger model, or check data |\n",
    "\n",
    "#### Why Two Losses?\n",
    "\n",
    "- **Training Loss**: How wrong the model is on data it's *actively learning from*\n",
    "- **Validation Loss**: How wrong the model is on data it has *never seen*\n",
    "\n",
    "We care most about validation loss because it predicts real-world performance!\n",
    "\n",
    "#### Validation Accuracy\n",
    "\n",
    "- Shows what percentage of validation samples the model classifies correctly\n",
    "- Should increase as training progresses\n",
    "- If it plateaus early while loss keeps decreasing, the model may be \"over-confident\" on wrong answers\n",
    "\n",
    "#### The Overfitting Gap\n",
    "\n",
    "Watch the **gap** between training and validation metrics:\n",
    "- Small gap = Good generalization (model learned the concept)\n",
    "- Large gap = Overfitting (model memorized the examples)\n",
    "\n",
    "For our digit recognition task, we want validation accuracy > 90% with training and validation loss staying close together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### What Are Model Weights?\n",
    "\n",
    "When we say we \"trained\" a neural network, we mean we adjusted millions of numbers (called **weights** or **parameters**) until the network could recognize patterns in our data.\n",
    "\n",
    "The cell below shows you what these weights actually look like - just arrays of decimal numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek inside the model weights - what did the network actually learn?\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL WEIGHTS - The numbers the network learned!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show all the layer names and their shapes\n",
    "print(\"\\nLayers in the model:\\n\")\n",
    "for name, param in conv_model.named_parameters():\n",
    "    print(f\"  {name:30} shape: {str(list(param.shape)):20} ({param.numel():,} numbers)\")\n",
    "\n",
    "total_params = sum(p.numel() for p in conv_model.parameters())\n",
    "print(f\"\\n  TOTAL: {total_params:,} learnable parameters!\")\n",
    "\n",
    "# Peek at actual values from the first conv layer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Zooming into conv1 weights (3x3 slice of the first filter):\")\n",
    "print(\"=\" * 60)\n",
    "conv1_weights = conv_model.conv1.weight.data[0, 0, :3, :3]  # First filter, 3x3 slice\n",
    "print(conv1_weights)\n",
    "\n",
    "print(\"\\nThese numbers were RANDOM before training!\")\n",
    "print(\"Training adjusted them to recognize patterns in spectrograms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with today's datetime\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create config dict with all parameters needed for inference\n",
    "config = {\n",
    "    # Model architecture parameters\n",
    "    \"mel_freq_bins\": mel_freq_bins,\n",
    "    \"time_steps\": time_steps,\n",
    "    \"num_classes\": 10,\n",
    "    \n",
    "    # Audio preprocessing parameters (needed to recreate the same spectrogram shape)\n",
    "    \"sample_rate\": 22050,  # librosa default\n",
    "    \"longest_audio_file_length\": longest_audio_file_length,  # in samples\n",
    "    \n",
    "    # MelSpectrogram params (torchaudio defaults)\n",
    "    \"n_mels\": 128,\n",
    "    \"n_fft\": 400,\n",
    "    \"hop_length\": 512,\n",
    "}\n",
    "\n",
    "# Extract the TRUE test set file paths (files the model never saw during training!)\n",
    "# test_dataset is now a direct AudioDataset with file_paths and labels attributes\n",
    "test_file_paths = test_dataset.file_paths\n",
    "test_file_labels = test_dataset.labels\n",
    "\n",
    "print(f\"Saving {len(test_file_paths)} test set file paths (held out from training)\")\n",
    "\n",
    "# Save config, model weights, AND test set info\n",
    "checkpoint = {\n",
    "    \"config\": config,\n",
    "    \"model_state_dict\": conv_model.state_dict(),\n",
    "    \"test_file_paths\": test_file_paths,\n",
    "    \"test_file_labels\": test_file_labels,\n",
    "}\n",
    "\n",
    "#make dir called model_weights\n",
    "os.makedirs('model_weights', exist_ok=True) \n",
    "saved_model_path = f'model_weights/audrey_model_weights_{timestamp}.pth'\n",
    "\n",
    "t.save(checkpoint, saved_model_path)\n",
    "print(f\"Saved model checkpoint to: {saved_model_path}\")\n",
    "print(f\"(The inference cell will automatically use this path)\")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### What Did We Just Save?\n",
    "\n",
    "The cell above created a **checkpoint** file containing:\n",
    "\n",
    "- **Model weights**: The learned parameters (millions of numbers the network learned during training)\n",
    "- **Config**: Architecture details (input dimensions, etc.) needed to reconstruct the model\n",
    "- **Test set paths**: The exact files held out from training, for fair evaluation later\n",
    "\n",
    "This checkpoint lets us load the trained model after restarting the notebook - no need to retrain!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 9: Using the Trained Model\n",
    "\n",
    "Everything below this point can be run **after restarting the notebook**. You just need to run the inference cell to load your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE ONLY - Run this cell after restart to load model\n",
    "# ============================================================\n",
    "# This cell is self-contained and can be run after clearing \n",
    "# all variables or restarting the notebook kernel.\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "\n",
    "# 1. Define the model architecture (must match training)\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, mel_freq_bins, time_steps, num_classes=10):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flat_features = 128 * (mel_freq_bins // 8) * (time_steps // 8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, self.flat_features)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. Load checkpoint (contains both config and weights)\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "# Use the path from the save cell if available, otherwise specify manually\n",
    "if 'saved_model_path' not in dir():\n",
    "    # Update this path if running after kernel restart\n",
    "    saved_model_path = 'model_weights/audrey_model_weights_2026-02-01_18-00-30.pth'\n",
    "print(f\"Loading model from: {saved_model_path}\")\n",
    "\n",
    "checkpoint = t.load(saved_model_path, map_location=device)\n",
    "config = checkpoint['config']\n",
    "\n",
    "print(f\"Loaded config: {config}\")\n",
    "\n",
    "# Load the true test set (files the model never saw during training)\n",
    "test_file_paths = checkpoint.get('test_file_paths', [])\n",
    "test_file_labels = checkpoint.get('test_file_labels', [])\n",
    "print(f\"Loaded {len(test_file_paths)} held-out test files\")\n",
    "\n",
    "# 3. Initialize model with saved config and load weights\n",
    "model = ConvModel(\n",
    "    mel_freq_bins=config['mel_freq_bins'],\n",
    "    time_steps=config['time_steps'],\n",
    "    num_classes=config['num_classes']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 4. Define preprocessing function\n",
    "def preprocess_audio(audio_path, config):\n",
    "    \"\"\"Load and preprocess audio to match training data shape.\"\"\"\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample to match training sample rate (librosa default is 22050)\n",
    "    target_sr = config['sample_rate']  # 22050\n",
    "    if sr != target_sr:\n",
    "        resampler = transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Ensure mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Pad or truncate to match training audio length\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = waveform.shape[1]\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        # Pad (center padding like training)\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        waveform = t.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "    elif current_length > target_length:\n",
    "        # Truncate (center crop)\n",
    "        start = (current_length - target_length) // 2\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    \n",
    "    # Use default MelSpectrogram (matches AudioDataset training setup)\n",
    "    mel_transform = transforms.MelSpectrogram()\n",
    "    spec = mel_transform(waveform)\n",
    "    \n",
    "    return spec.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# 5. Define prediction function\n",
    "def predict_digit(audio_path):\n",
    "    \"\"\"Predict the digit from an audio file.\"\"\"\n",
    "    spec = preprocess_audio(audio_path, config).to(device)\n",
    "    \n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "print(f\"Model loaded successfully! Ready for inference.\")\n",
    "print(f\"Use predict_digit('path/to/audio.wav') to make predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Understanding the Inference Cell Above\n",
    "\n",
    "The large cell above is **self-contained** - it can be run after restarting the notebook to load your trained model. It includes:\n",
    "\n",
    "1. **Model architecture** (must match what was trained)\n",
    "2. **Checkpoint loading** (weights + config)\n",
    "3. **`preprocess_audio()`** - Prepares new audio to match training format\n",
    "4. **`predict_digit()`** - Takes an audio file path, returns predicted digit\n",
    "\n",
    "**Why preprocessing matters:** The model expects input in a very specific format (sample rate, length, spectrogram shape). If we preprocess differently than during training, the model will fail!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Part 10: Evaluating Model Performance\n",
    "\n",
    "### Why Test Sets Matter\n",
    "\n",
    "The cells below demonstrate a critical ML concept: **proper evaluation**.\n",
    "\n",
    "| Test Type | What It Tests | Potential Issue |\n",
    "|-----------|---------------|-----------------|\n",
    "| **True Test Set** | Files NEVER seen during training | Fair evaluation |\n",
    "| **Random Files** | Any files from `processed/` | May include training data! |\n",
    "\n",
    "**Overfitting** happens when a model memorizes training examples instead of learning general patterns. Signs of overfitting:\n",
    "- High accuracy on training data\n",
    "- Lower accuracy on truly new data (test set)\n",
    "\n",
    "If random files show much higher accuracy than the true test set, your model is overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the TRUE HELD-OUT TEST SET (model never saw these during training!)\n",
    "import random\n",
    "\n",
    "if not test_file_paths:\n",
    "    print(\"No test set found in checkpoint. Re-run training with the updated save cell.\")\n",
    "else:\n",
    "    # Test on a sample of the held-out test set\n",
    "    num_tests = min(20, len(test_file_paths))\n",
    "    test_indices = random.sample(range(len(test_file_paths)), num_tests)\n",
    "    correct = 0\n",
    "    \n",
    "    print(f\"Testing on {num_tests} files from the TRUE HELD-OUT TEST SET\")\n",
    "    print(f\"(Total held-out files: {len(test_file_paths)})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"These files were NEVER seen during training!\\n\")\n",
    "    \n",
    "    for i, idx in enumerate(test_indices):\n",
    "        test_file = test_file_paths[idx]\n",
    "        true_label = test_file_labels[idx]\n",
    "        predicted_digit, confidence = predict_digit(test_file)\n",
    "        \n",
    "        is_correct = predicted_digit == true_label\n",
    "        correct += is_correct\n",
    "        \n",
    "        status = \"correct\" if is_correct else \"WRONG\"\n",
    "        print(f\"{i+1}. True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"TRUE TEST SET Accuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a random file from the processed dataset\n",
    "# (Works after kernel restart - just needs the inference cell to be run first)\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# Get all processed audio files\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "print(f\"Found {len(processed_files)} files in processed/\")\n",
    "\n",
    "# Pick a random file\n",
    "test_file = random.choice(processed_files)\n",
    "\n",
    "# Extract true label from folder name (e.g., \"processed/3/3_123.wav\" -> 3)\n",
    "true_label = int(test_file.split('/')[-2])\n",
    "\n",
    "# Make prediction\n",
    "predicted_digit, confidence = predict_digit(test_file)\n",
    "\n",
    "print(f\"\\nTest file: {test_file}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted: {predicted_digit}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n",
    "print(f\"Correct: {'Yes' if predicted_digit == true_label else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on RANDOM files from processed/ (MAY INCLUDE TRAINING DATA!)\n",
    "# Compare this accuracy to the true test set above to see if the model is overfitting\n",
    "import random\n",
    "import glob\n",
    "\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "num_tests = min(20, len(processed_files))\n",
    "correct = 0\n",
    "\n",
    "# Get random files\n",
    "test_files = random.sample(processed_files, num_tests)\n",
    "\n",
    "print(f\"Testing on {num_tests} RANDOM files from processed/\")\n",
    "print(\"=\" * 50)\n",
    "print(\"WARNING: Some of these may have been in the training set!\\n\")\n",
    "\n",
    "for i, test_file in enumerate(test_files):\n",
    "    true_label = int(test_file.split('/')[-2])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{i+1}. True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"RANDOM FILES Accuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")\n",
    "print(\"\\nCompare this to the TRUE TEST SET accuracy above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 10 random files from the processed dataset\n",
    "import glob\n",
    "import random\n",
    "\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "print(f\"Found {len(processed_files)} files in processed/\\n\")\n",
    "\n",
    "num_tests = 10\n",
    "test_files = random.sample(processed_files, num_tests)\n",
    "correct = 0\n",
    "\n",
    "for i, test_file in enumerate(test_files):\n",
    "    true_label = int(test_file.split('/')[-2])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{i+1}. {test_file.split('/')[-1]}: True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on ALL original recordings from the unprocessed dataset\n",
    "import glob\n",
    "\n",
    "unprocessed_files = sorted(glob.glob('unprocessed/*.wav'))\n",
    "print(f\"Found {len(unprocessed_files)} original recordings in unprocessed/\\n\")\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for test_file in unprocessed_files:\n",
    "    # Extract true label from filename (e.g., \"0.wav\" -> 0)\n",
    "    true_label = int(test_file.split('/')[-1].split('.')[0])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{test_file}: True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(f\"\\nAccuracy on original recordings: {correct}/{len(unprocessed_files)} = {100*correct/len(unprocessed_files):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Part 11: Real-time Digit Recognition\n",
    "\n",
    "Now for the fun part - using your trained model in real-time!\n",
    "\n",
    "### Critical Insight: Processing Pipeline Must Match Training\n",
    "\n",
    "For real-time recognition to work accurately, the audio must be processed **exactly the same way** as during training. This means:\n",
    "\n",
    "1. **Save to file** (even though it adds a bit of latency)\n",
    "2. **Load with librosa** (automatically resamples to 22050 Hz, just like training)\n",
    "3. **Pad to target length** (center padding, same as preprocessing)\n",
    "4. **Save padded file, then load with torchaudio** (same as AudioDataset)\n",
    "5. **Apply MelSpectrogram** (same transform as training)\n",
    "\n",
    "This file-based approach ensures perfect consistency between training and inference!\n",
    "\n",
    "### Two Working Approaches\n",
    "\n",
    "We provide two real-time recognition methods below:\n",
    "\n",
    "| Method | How it works | Best for |\n",
    "|--------|--------------|----------|\n",
    "| **Enter-to-Record** | Press Enter, speak for 1 second, get prediction | Simple, reliable |\n",
    "| **Buffered Onset Detection** | Hands-free, auto-triggers when you speak | More interactive |\n",
    "\n",
    "**Requirements:** Run the inference cell first to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENTER-TO-RECORD: Simple, reliable real-time recognition\n",
    "# =============================================================================\n",
    "# Press Enter to record for 1 second, then get a prediction.\n",
    "# Uses the EXACT same pipeline as training for maximum accuracy.\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Temporary files for the file-based pipeline\n",
    "TEMP_RAW = '_temp_recording.wav'\n",
    "TEMP_PADDED = '_temp_padded.wav'\n",
    "\n",
    "# Recording config - 44100 Hz is common for most microphones\n",
    "RECORD_SAMPLE_RATE = 44100\n",
    "RECORD_DURATION = 1.0  # 1 second, matching our training data\n",
    "\n",
    "def predict_from_recording(raw_audio):\n",
    "    \"\"\"\n",
    "    Process audio through the EXACT same pipeline as training data.\n",
    "    This is the key to accurate real-time recognition!\n",
    "    \"\"\"\n",
    "    # Step 1: Save raw recording to file\n",
    "    sf.write(TEMP_RAW, raw_audio, RECORD_SAMPLE_RATE, subtype='PCM_16')\n",
    "    \n",
    "    # Step 2: Load with librosa (resamples to 22050 Hz - same as training!)\n",
    "    audio, sr = librosa.load(TEMP_RAW)  # Default sr=22050\n",
    "    \n",
    "    # Step 3: Pad to target length (center padding - same as preprocessing!)\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = len(audio)\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "    elif current_length > target_length:\n",
    "        # Center crop if too long\n",
    "        start = (current_length - target_length) // 2\n",
    "        audio = audio[start:start + target_length]\n",
    "    \n",
    "    # Step 4: Save padded audio and reload with torchaudio (like AudioDataset!)\n",
    "    sf.write(TEMP_PADDED, audio, sr)\n",
    "    waveform, loaded_sr = torchaudio.load(TEMP_PADDED)\n",
    "    \n",
    "    # Step 5: Apply MelSpectrogram (same as AudioDataset!)\n",
    "    mel = torchaudio.transforms.MelSpectrogram()\n",
    "    spec = mel(waveform)\n",
    "    \n",
    "    # Step 6: Add batch dimension and move to device\n",
    "    spec = spec.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Step 7: Run inference\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "# Main loop\n",
    "print(\"=\" * 50)\n",
    "print(\"ENTER-TO-RECORD DIGIT RECOGNITION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Instructions:\")\n",
    "print(\"  1. Press Enter when ready to record\")\n",
    "print(\"  2. Say a digit clearly\")\n",
    "print(f\"  3. Recording lasts {RECORD_DURATION} seconds\")\n",
    "print(\"  4. Type 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record (or 'q' to quit): \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "        \n",
    "        # Record audio\n",
    "        print(\"Recording... Say a digit NOW!\")\n",
    "        recording = sd.rec(\n",
    "            int(RECORD_DURATION * RECORD_SAMPLE_RATE),\n",
    "            samplerate=RECORD_SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "        \n",
    "        # Process and predict\n",
    "        print(\"Processing...\")\n",
    "        recording = recording.flatten()\n",
    "        predicted, confidence = predict_from_recording(recording)\n",
    "        \n",
    "        print(f\"\\n  Predicted: {predicted}\")\n",
    "        print(f\"  Confidence: {confidence:.1%}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    # Clean up temp files\n",
    "    for f in [TEMP_RAW, TEMP_PADDED]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    print(\"\\nGoodbye!\")\n",
    "\n",
    "def process_and_predict(audio_data):\n",
    "    \"\"\"Process accumulated audio and run prediction.\"\"\"\n",
    "    # Normalize audio (peak normalization to match training data levels)\n",
    "    audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-8)\n",
    "    audio_data = audio_data * 0.9  # Scale to ~90% to avoid clipping\n",
    "    \n",
    "    # Convert to tensor\n",
    "    waveform = t.tensor(audio_data, dtype=t.float32).unsqueeze(0)\n",
    "    \n",
    "    # Resample to match training\n",
    "    resampler = transforms.Resample(orig_freq=SAMPLE_RATE, new_freq=TARGET_SAMPLE_RATE)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    # Pad or truncate to match training audio length\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = waveform.shape[1]\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        waveform = t.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "    elif current_length > target_length:\n",
    "        start = (current_length - target_length) // 2\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    \n",
    "    # Compute mel spectrogram (using defaults to match training)\n",
    "    mel_transform = transforms.MelSpectrogram()\n",
    "    spec = mel_transform(waveform).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Called for each block of audio from the microphone.\"\"\"\n",
    "    global audio_buffer, is_recording, silence_samples\n",
    "    \n",
    "    if status:\n",
    "        print(f\"Status: {status}\")\n",
    "    \n",
    "    # Calculate RMS volume for this block\n",
    "    audio_block = indata[:, 0]\n",
    "    rms = np.sqrt(np.mean(audio_block**2))\n",
    "    \n",
    "    if not is_recording:\n",
    "        # Waiting for speech to start\n",
    "        if rms > VOLUME_THRESHOLD:\n",
    "            is_recording = True\n",
    "            silence_samples = 0\n",
    "            audio_buffer = list(audio_block)  # Start with this block\n",
    "            clear_output(wait=True)\n",
    "            print(\"Recording... (speak your digit)\")\n",
    "    else:\n",
    "        # Currently recording\n",
    "        audio_buffer.extend(audio_block.tolist())\n",
    "        \n",
    "        if rms < VOLUME_THRESHOLD:\n",
    "            silence_samples += 1\n",
    "        else:\n",
    "            silence_samples = 0  # Reset silence counter if sound detected\n",
    "        \n",
    "        # Check if we should stop recording\n",
    "        should_stop = False\n",
    "        \n",
    "        if len(audio_buffer) >= max_samples:\n",
    "            should_stop = True  # Hit max duration\n",
    "        elif silence_samples >= silence_samples_threshold and len(audio_buffer) >= min_samples:\n",
    "            should_stop = True  # Silence detected after minimum recording\n",
    "        \n",
    "        if should_stop:\n",
    "            # Process the recording\n",
    "            audio_data = np.array(audio_buffer)\n",
    "            duration = len(audio_data) / SAMPLE_RATE\n",
    "            \n",
    "            predicted, confidence = process_and_predict(audio_data)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"  Predicted Digit:  {predicted}\")\n",
    "            print(f\"  Confidence:       {confidence:.1%}\")\n",
    "            print(f\"  Audio duration:   {duration:.2f}s\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"\\nListening... (speak a digit to start)\")\n",
    "            \n",
    "            # Reset state\n",
    "            audio_buffer = []\n",
    "            is_recording = False\n",
    "            silence_samples = 0\n",
    "\n",
    "# Start listening\n",
    "print(\"Real-time Digit Recognition (Voice Activated)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Volume threshold: {VOLUME_THRESHOLD} RMS\")\n",
    "print(f\"Silence timeout:  {SILENCE_DURATION}s\")\n",
    "print(f\"Max recording:    {MAX_RECORDING_DURATION}s\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\nListening... (speak a digit to start)\")\n",
    "print(\"\\n>>> Click the STOP button (square) or press 'i' twice to stop <<<\")\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        blocksize=BLOCK_SIZE,\n",
    "        channels=1,\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "    \n",
    "    # Loop with short sleeps - much easier to interrupt\n",
    "    while True:\n",
    "        sd.sleep(100)  # 100ms intervals - responsive to interrupts\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if stream is not None:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "    audio_buffer = []\n",
    "    is_recording = False\n",
    "    silence_samples = 0\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUFFERED ONSET DETECTION: Hands-free automatic recognition\n",
    "# =============================================================================\n",
    "# Listens continuously and auto-triggers when you speak.\n",
    "# Uses a rolling buffer to capture the START of your speech (not just the middle!).\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Temporary files for the file-based pipeline\n",
    "TEMP_RAW = '_temp_onset_recording.wav'\n",
    "TEMP_PADDED = '_temp_onset_padded.wav'\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 44100\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "# Onset detection settings - ADJUST FOR YOUR ENVIRONMENT\n",
    "# If too sensitive (triggers on noise): increase ONSET_THRESHOLD to 0.05 or higher\n",
    "# If not sensitive enough (misses words): decrease to 0.02\n",
    "ONSET_THRESHOLD = 0.04   # RMS level to detect speech (default: 0.04, noisy rooms: 0.06-0.08)\n",
    "RECORD_AFTER_ONSET = 1.0 # Seconds to record after onset detected\n",
    "BUFFER_SECONDS = 0.5     # Seconds of pre-onset audio to keep (captures word start!)\n",
    "COOLDOWN_SECONDS = 0.5   # Wait this long after prediction before listening again\n",
    "\n",
    "# Calculate buffer sizes\n",
    "buffer_blocks = int(BUFFER_SECONDS * SAMPLE_RATE / BLOCK_SIZE)\n",
    "record_samples_after_onset = int(RECORD_AFTER_ONSET * SAMPLE_RATE)\n",
    "cooldown_blocks = int(COOLDOWN_SECONDS * SAMPLE_RATE / BLOCK_SIZE)\n",
    "\n",
    "# Rolling buffer to capture audio BEFORE onset\n",
    "rolling_buffer = deque(maxlen=buffer_blocks)\n",
    "\n",
    "# State\n",
    "is_recording = False\n",
    "recorded_audio = []\n",
    "samples_recorded = 0\n",
    "cooldown_remaining = 0  # Blocks to wait before listening again\n",
    "\n",
    "def predict_from_audio(audio_data):\n",
    "    \"\"\"\n",
    "    Process audio through the EXACT same pipeline as training data.\n",
    "    \"\"\"\n",
    "    # Step 1: Save raw recording\n",
    "    sf.write(TEMP_RAW, audio_data, SAMPLE_RATE, subtype='PCM_16')\n",
    "    \n",
    "    # Step 2: Load with librosa (resamples to 22050 Hz)\n",
    "    audio, sr = librosa.load(TEMP_RAW)\n",
    "    \n",
    "    # Step 3: Pad to target length\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = len(audio)\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "    elif current_length > target_length:\n",
    "        start = (current_length - target_length) // 2\n",
    "        audio = audio[start:start + target_length]\n",
    "    \n",
    "    # Step 4: Save and reload with torchaudio\n",
    "    sf.write(TEMP_PADDED, audio, sr)\n",
    "    waveform, _ = torchaudio.load(TEMP_PADDED)\n",
    "    \n",
    "    # Step 5: MelSpectrogram\n",
    "    mel = torchaudio.transforms.MelSpectrogram()\n",
    "    spec = mel(waveform).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Step 6: Inference\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Called for each block of audio from the microphone.\"\"\"\n",
    "    global is_recording, recorded_audio, samples_recorded, rolling_buffer, cooldown_remaining\n",
    "    \n",
    "    audio_block = indata[:, 0].copy()\n",
    "    rms = np.sqrt(np.mean(audio_block**2))\n",
    "    \n",
    "    # Handle cooldown period (prevents rapid re-triggering)\n",
    "    if cooldown_remaining > 0:\n",
    "        cooldown_remaining -= 1\n",
    "        return\n",
    "    \n",
    "    if not is_recording:\n",
    "        # Keep audio in rolling buffer (captures pre-onset audio!)\n",
    "        rolling_buffer.append(audio_block)\n",
    "        \n",
    "        # Check for onset\n",
    "        if rms > ONSET_THRESHOLD:\n",
    "            is_recording = True\n",
    "            # Include the buffer contents (this captures the word START!)\n",
    "            recorded_audio = list(np.concatenate(list(rolling_buffer)))\n",
    "            recorded_audio.extend(audio_block.tolist())\n",
    "            samples_recorded = len(recorded_audio)\n",
    "            clear_output(wait=True)\n",
    "            print(\"Recording... (detected speech)\")\n",
    "    else:\n",
    "        # Continue recording\n",
    "        recorded_audio.extend(audio_block.tolist())\n",
    "        samples_recorded += len(audio_block)\n",
    "        \n",
    "        # Check if we've recorded enough\n",
    "        if samples_recorded >= record_samples_after_onset:\n",
    "            # Process the recording\n",
    "            audio_data = np.array(recorded_audio, dtype=np.float32)\n",
    "            \n",
    "            try:\n",
    "                predicted, confidence = predict_from_audio(audio_data)\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                print(\"=\" * 50)\n",
    "                print(f\"  Predicted Digit: {predicted}\")\n",
    "                print(f\"  Confidence: {confidence:.1%}\")\n",
    "                print(\"=\" * 50)\n",
    "                print(\"\\nListening... (speak a digit)\")\n",
    "            except Exception as e:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"\\nListening... (speak a digit)\")\n",
    "            \n",
    "            # Reset state with cooldown to prevent immediate re-trigger\n",
    "            is_recording = False\n",
    "            recorded_audio = []\n",
    "            samples_recorded = 0\n",
    "            cooldown_remaining = cooldown_blocks  # Wait before listening again\n",
    "\n",
    "# Start listening\n",
    "print(\"=\" * 50)\n",
    "print(\"BUFFERED ONSET DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Onset threshold: {ONSET_THRESHOLD} RMS\")\n",
    "print(f\"  (If glitchy/too sensitive, increase to 0.06 or 0.08)\")\n",
    "print(f\"Pre-onset buffer: {BUFFER_SECONDS}s\")\n",
    "print(f\"Recording duration: {RECORD_AFTER_ONSET}s after onset\")\n",
    "print(f\"Cooldown between detections: {COOLDOWN_SECONDS}s\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nListening... (speak a digit)\")\n",
    "print(\"\\n>>> Press STOP (square button) or Kernel > Interrupt to stop <<<\")\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        blocksize=BLOCK_SIZE,\n",
    "        channels=1,\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "    \n",
    "    while True:\n",
    "        sd.sleep(100)  # Short sleep for responsive interrupts\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if stream is not None:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "    # Clean up temp files and reset state\n",
    "    for f in [TEMP_RAW, TEMP_PADDED]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    rolling_buffer.clear()\n",
    "    is_recording = False\n",
    "    recorded_audio = []\n",
    "    samples_recorded = 0\n",
    "    cooldown_remaining = 0\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Exercises and Next Steps\n",
    "\n",
    "Now that you've built your own Audrey, try these challenges:\n",
    "\n",
    "### Exploration\n",
    "1. **Test robustness**: What happens if you say a digit in a different accent or tone?\n",
    "2. **Out-of-vocabulary**: What does the model predict when you say a word that isn't a digit?\n",
    "3. **Different device**: Play a digit from your phone speaker - does it still work?\n",
    "\n",
    "### Extensions\n",
    "4. **New vocabulary**: Train on different sounds (you might need to change how the files get saved and processed in the dataset creation step):\n",
    "   - Yes / No\n",
    "   - Colors (red, blue, green...)\n",
    "   - Letters (A, B, C...)\n",
    "   - Environmental sounds (clap, snap, whistle...)\n",
    "   \n",
    "5. **Improve the model**: \n",
    "   - Record more original samples\n",
    "   - Adjust augmentation parameters\n",
    "   - Try different network architectures\n",
    "\n",
    "6. **Investigate bias**: \n",
    "   - Train on recordings from multiple people\n",
    "   - Test in different acoustic environments\n",
    "   - What makes a \"fair\" speech recognition dataset?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook you learned:\n",
    "- **Data collection** and the importance of representative training data\n",
    "- **Data augmentation** to create variations and improve robustness\n",
    "- **Neural network architectures**: MLP vs CNN for audio classification\n",
    "- **Training loop**: forward pass → loss → backpropagation → optimization\n",
    "- **Evaluation**: Why held-out test sets matter\n",
    "- **Inference**: Using trained models on new data\n",
    "- **Dataset bias**: How recording conditions affect model performance\n",
    "\n",
    "Congratulations - you've built a speech recognition system from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Debugging Real-time Recognition\n",
    "\n",
    "If real-time recognition isn't working well, the cells below can help diagnose why.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Symptom | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Always predicts same digit | Sample rate mismatch | Use file-based pipeline |\n",
    "| Very low confidence | Spectrogram shape mismatch | Check MelSpectrogram params |\n",
    "| Works on files, fails live | Processing pipeline differs from training | Use the file-save/load approach |\n",
    "| Inconsistent predictions | Background noise | Increase onset threshold |\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "**If file-based evaluation works (100% on test set) but real-time fails, the problem is in the real-time preprocessing, not the model.**\n",
    "\n",
    "The diagnostic cells below help verify that real-time audio is being processed identically to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSTIC: Compare Real-time vs Training Pipeline\n",
    "# =============================================================================\n",
    "# This cell helps you verify that real-time audio is processed the same way\n",
    "# as training data. If both predict the same digit, your pipeline is correct!\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "TEMP_FILE = '_diagnostic_recording.wav'\n",
    "TEMP_PADDED = '_diagnostic_padded.wav'\n",
    "\n",
    "print(\"DIAGNOSTIC: Recording 2 seconds... Say a digit!\")\n",
    "recording = sd.rec(int(2 * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "sd.wait()\n",
    "recording = recording.flatten()\n",
    "print(\"Recording done!\\n\")\n",
    "\n",
    "# Save and process through file-based pipeline (like training)\n",
    "sf.write(TEMP_FILE, recording, 44100, subtype='PCM_16')\n",
    "\n",
    "# Step 1: Load with librosa (this is what training uses!)\n",
    "audio, sr = librosa.load(TEMP_FILE)  # Resamples to 22050 by default\n",
    "print(f\"After librosa.load: SR={sr}, length={len(audio)}\")\n",
    "\n",
    "# Step 2: Pad to target length\n",
    "target_length = config['longest_audio_file_length']\n",
    "if len(audio) < target_length:\n",
    "    pad_size = target_length - len(audio)\n",
    "    audio = np.pad(audio, (pad_size // 2, pad_size - pad_size // 2), mode='constant')\n",
    "elif len(audio) > target_length:\n",
    "    start = (len(audio) - target_length) // 2\n",
    "    audio = audio[start:start + target_length]\n",
    "print(f\"After padding: length={len(audio)} (target={target_length})\")\n",
    "\n",
    "# Step 3: Save and reload with torchaudio (like AudioDataset)\n",
    "sf.write(TEMP_PADDED, audio, sr)\n",
    "waveform, loaded_sr = torchaudio.load(TEMP_PADDED)\n",
    "print(f\"After torchaudio.load: shape={waveform.shape}, SR={loaded_sr}\")\n",
    "\n",
    "# Step 4: Create spectrogram\n",
    "mel = torchaudio.transforms.MelSpectrogram()\n",
    "spec = mel(waveform)\n",
    "print(f\"Spectrogram shape: {spec.shape}\")\n",
    "print(f\"Expected shape: [1, 128, {config['time_steps']}]\")\n",
    "\n",
    "# Step 5: Run inference\n",
    "spec_batch = spec.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    output = model(spec_batch)\n",
    "    predicted = t.argmax(output, dim=1).item()\n",
    "    confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"PREDICTION: {predicted} (confidence: {confidence:.1%})\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Clean up\n",
    "for f in [TEMP_FILE, TEMP_PADDED]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"\\nIf this prediction is correct but real-time recognition is wrong,\")\n",
    "print(\"then the real-time cell is not using the same processing pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSTIC: Visualize Training vs Live Audio Spectrograms\n",
    "# =============================================================================\n",
    "# Compare spectrograms from a training file vs your live recording.\n",
    "# They should look similar in structure (frequency patterns, energy distribution).\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Get a random training file\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "random_file = random.choice(processed_files)\n",
    "true_label = int(random_file.split('/')[-2])\n",
    "\n",
    "# Load training file spectrogram (exactly how the model sees it)\n",
    "train_waveform, _ = torchaudio.load(random_file)\n",
    "train_spec = torchaudio.transforms.MelSpectrogram()(train_waveform)\n",
    "\n",
    "print(f\"Training file: {random_file}\")\n",
    "print(f\"Training spec shape: {train_spec.shape}\")\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training spectrogram\n",
    "axes[0].imshow(train_spec[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title(f'Training File (digit {true_label})')\n",
    "axes[0].set_xlabel('Time frames')\n",
    "axes[0].set_ylabel('Mel frequency bins')\n",
    "\n",
    "# Instructions for live comparison\n",
    "axes[1].text(0.5, 0.5, 'Run the diagnostic cell above\\nto compare with live audio', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "axes[1].set_title('Live Recording (compare here)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTip: The frequency patterns (horizontal bands) should be similar between\")\n",
    "print(\"training and live audio. Big differences indicate a preprocessing mismatch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
