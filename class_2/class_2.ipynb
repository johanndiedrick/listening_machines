{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Audrey - The First Speech Recognition System\n",
    "\n",
    "Motivation - we need a large dataset with variations of speech that we used for traning and eventual speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our own speech dataset! This work with numbers first\n",
    "\n",
    "# Recording audio with sounddevice and soundfile\n",
    "# \n",
    "# https://python-soundfile.readthedocs.io/\n",
    "# https://python-sounddevice.readthedocs.io\n",
    "\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def record_audio(filename: str, duration: int):\n",
    "\n",
    "    # config\n",
    "    samplerate = 44100\n",
    "    duration = duration\n",
    "    channels = 1\n",
    "\n",
    "    print(f\"Recording for {duration} seconds at {samplerate} Hz...\")\n",
    "\n",
    "    # record audio from the microphone into a numpy array with sounddevice\n",
    "    recording = sd.rec(\n",
    "        int(duration * samplerate),\n",
    "        samplerate=samplerate,\n",
    "        channels=channels,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    sd.wait()\n",
    "\n",
    "    print(f\"Recording finished. Saving to {filename}...\")\n",
    "\n",
    "    # save the file with soundfile\n",
    "    sf.write(\n",
    "        filename,\n",
    "        recording,\n",
    "        samplerate,\n",
    "        subtype='PCM_16'\n",
    "        )\n",
    "\n",
    "    print(f\"File '{filename}' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record yourself saying the digits 0-9\n",
    "\n",
    "record_audio(\n",
    "    filename='unprocessed/0.wav', # rename the file to 1.wav, 2.wav, 3.wav, etc.\n",
    "    duration=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some variations of your voice through data augmentation!\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def noise(data, noise_amt=0.035):\n",
    "    noise_amp = noise_amt*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, n_steps=2):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take our recorded digits, and augment them to create a larger dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# get all files in the 'unprocessed' directory (only .wav files)\n",
    "files = glob.glob('unprocessed/*.wav')\n",
    "print(files)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    # get the digit from the file name\n",
    "    digit = file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # create the directory if it doesn't exist\n",
    "    os.makedirs(f'processed/{digit}', exist_ok=True)\n",
    "    # load file with sf\n",
    "    audio, sample_rate = sf.read(file)\n",
    "\n",
    "    for i in tqdm(range(1000)):\n",
    "        \n",
    "        processed_audio = noise(audio, np.random.uniform(0.001, 0.01))\n",
    "        processed_audio = stretch(processed_audio, rate=np.random.uniform(0.8, 1.2))\n",
    "        processed_audio = shift(processed_audio)\n",
    "        processed_audio = pitch(processed_audio, sample_rate, n_steps=np.random.randint(-3, 3))\n",
    "\n",
    "        sf.write(f'processed/{digit}/{digit}_{i}.wav', processed_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the fies in speech_digits with glob\n",
    "import glob\n",
    "\n",
    "files = glob.glob('processed/*/*')\n",
    "\n",
    "print(len(files))\n",
    "print(files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display spectogramand audio player for all files in jo_digits/unprocessed\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the directory for digit 0\n",
    "#digit_dir = 'jo_digits/unprocessed'\n",
    "digit_dir = 'processed'\n",
    "\n",
    "\n",
    "# Get all files in the digit directory\n",
    "files = glob.glob('processed/*/*')\n",
    "print(files)\n",
    "\n",
    "# Display spectogram and audio player for each file\n",
    "for file in files[:5]:\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    \n",
    "    # Display the spectogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f'Waveform for {os.path.basename(file)}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the spectogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Spectrogram for {os.path.basename(file)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    display(Audio(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass: preprocess audio files so that they are all the same length\n",
    "\n",
    "import glob\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "files = glob.glob('processed/*/*')\n",
    "print(f\"Total files: {len(files)}\")\n",
    "\n",
    "audio_data = []\n",
    "labels = [] # here is where we create our labels\n",
    "longest_audio_file_length = 0\n",
    "\n",
    "# First pass: load data and find longest audio file\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f)\n",
    "        if len(audio) == 0:\n",
    "            print(f\"Warning: Empty audio file: {f}\")\n",
    "            continue\n",
    "        labels.append(int(f.split('/')[-2]))  # Adjust this based on your file structure\n",
    "        longest_audio_file_length = max(longest_audio_file_length, len(audio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")\n",
    "\n",
    "print(f\"Longest audio size: {longest_audio_file_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass: Pad audio files and resave them\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f)\n",
    "        if len(audio) == 0:\n",
    "            print(f\"Warning: Empty audio file: {f}\")\n",
    "            continue\n",
    "        current_size = len(audio)\n",
    "        pad_size = longest_audio_file_length - current_size\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        padded_audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "        sf.write(f, padded_audio, sample_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third pass: Verify that all files have the same size\n",
    "file_sizes = []\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        audio, _ = librosa.load(f)\n",
    "        file_sizes.append(len(audio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {str(e)}\")\n",
    "\n",
    "if len(set(file_sizes)) == 1:\n",
    "    print(f\"All files have the same size: {file_sizes[0]} samples\")\n",
    "else:\n",
    "    print(\"Warning: Not all files have the same size\")\n",
    "    print(f\"Unique file sizes: {set(file_sizes)}\")\n",
    "    print(f\"Min size: {min(file_sizes)}, Max size: {max(file_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[1000:1005])\n",
    "print(labels[1000:1005])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transforms=transforms.MelSpectrogram()):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        audio_path = self.file_paths[idx]\n",
    "\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # ensure its mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transforms:\n",
    "            spec = self.transforms(waveform)\n",
    "        return spec, self.labels[idx]\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "full_dataset = AudioDataset(files, labels, transforms=transforms.MelSpectrogram())\n",
    "\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "validation_size = int(0.2 * len(full_dataset))\n",
    "test_size = int(0.1 * len(full_dataset))\n",
    "\n",
    "print(train_size)\n",
    "print(validation_size)\n",
    "print(test_size)\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset  = t.utils.data.random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(validation_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see a batch\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(inputs.shape)\n",
    "    print(inputs[0][0].shape)\n",
    "    print(targets)\n",
    "    break\n",
    "\n",
    "\n",
    "mel_freq_bins = inputs[0][0].shape[0]\n",
    "time_steps = inputs[0][0].shape[1]\n",
    "\n",
    "print(\"mel freq bins: \", mel_freq_bins)\n",
    "print(\"time steps: \", time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with a simple Multi-Layer Perceptron (MLP) - Fully-Connected Neural Network\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Flatten(),\n",
    "    t.nn.Linear(mel_freq_bins*time_steps, 512), # 128 mel bins, 366 time steps\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(512, 512),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(512, 10),\n",
    "    t.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# train our model\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(f\"Training for {epochs} epochs\")\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# evaluate our model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with t.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = t.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the model on the test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with conv net\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, mel_freq_bins, time_steps, num_classes=10):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size of the flattened features (using explicit parameters)\n",
    "        self.flat_features = 128 * (mel_freq_bins // 8) * (time_steps // 8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, 128, 366)\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, self.flat_features) # rewrite this line with einops / ARENA\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model (passing mel_freq_bins and time_steps explicitly)\n",
    "\n",
    "conv_model = ConvModel(mel_freq_bins=mel_freq_bins, time_steps=time_steps)\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "\n",
    "conv_model = conv_model.to(device)\n",
    "\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "print(f\"Training for {epochs} epochs\")\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    conv_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = conv_model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "print(\"finished training\")\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "\n",
    "# evaluate our model\n",
    "\n",
    "conv_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# plot the accuracy\n",
    "accuracy_history = []\n",
    "\n",
    "with t.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = conv_model(inputs)\n",
    "        _, predicted = t.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        accuracy_history.append(100 * correct / total)\n",
    "\n",
    "plt.plot(accuracy_history)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy of the conv model on the test set: {100 * correct / total}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with today's datetime\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create config dict with all parameters needed for inference\n",
    "config = {\n",
    "    # Model architecture parameters\n",
    "    \"mel_freq_bins\": mel_freq_bins,\n",
    "    \"time_steps\": time_steps,\n",
    "    \"num_classes\": 10,\n",
    "    \n",
    "    # Audio preprocessing parameters (needed to recreate the same spectrogram shape)\n",
    "    \"sample_rate\": 22050,  # librosa default\n",
    "    \"longest_audio_file_length\": longest_audio_file_length,  # in samples\n",
    "    \n",
    "    # MelSpectrogram params (torchaudio defaults)\n",
    "    \"n_mels\": 128,\n",
    "    \"n_fft\": 400,\n",
    "    \"hop_length\": 512,\n",
    "}\n",
    "\n",
    "# Extract the TRUE test set file paths (files the model never saw during training!)\n",
    "# test_dataset is a Subset, so we get the original indices and map to file paths\n",
    "test_file_paths = [\n",
    "    test_dataset.dataset.file_paths[idx] \n",
    "    for idx in test_dataset.indices\n",
    "]\n",
    "test_file_labels = [\n",
    "    test_dataset.dataset.labels[idx] \n",
    "    for idx in test_dataset.indices\n",
    "]\n",
    "\n",
    "print(f\"Saving {len(test_file_paths)} test set file paths (held out from training)\")\n",
    "\n",
    "# Save config, model weights, AND test set info\n",
    "checkpoint = {\n",
    "    \"config\": config,\n",
    "    \"model_state_dict\": conv_model.state_dict(),\n",
    "    \"test_file_paths\": test_file_paths,\n",
    "    \"test_file_labels\": test_file_labels,\n",
    "}\n",
    "\n",
    "#make dir called model_weights\n",
    "os.makedirs('model_weights', exist_ok=True) \n",
    "saved_model_path = f'model_weights/audrey_model_weights_{timestamp}.pth'\n",
    "\n",
    "t.save(checkpoint, saved_model_path)\n",
    "print(f\"Saved model checkpoint to: {saved_model_path}\")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE ONLY - Run this cell after restart to load model\n",
    "# ============================================================\n",
    "# This cell is self-contained and can be run after clearing \n",
    "# all variables or restarting the notebook kernel.\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "\n",
    "# 1. Define the model architecture (must match training)\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, mel_freq_bins, time_steps, num_classes=10):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flat_features = 128 * (mel_freq_bins // 8) * (time_steps // 8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(-1, self.flat_features)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. Load checkpoint (contains both config and weights)\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "saved_model_path = 'model_weights/audrey_model_weights_2026-02-01_18-00-30.pth' # MAKE SURE TO UPDATE THIS PATH!\n",
    "\n",
    "checkpoint = t.load(saved_model_path, map_location=device)\n",
    "config = checkpoint['config']\n",
    "\n",
    "print(f\"Loaded config: {config}\")\n",
    "\n",
    "# Load the true test set (files the model never saw during training)\n",
    "test_file_paths = checkpoint.get('test_file_paths', [])\n",
    "test_file_labels = checkpoint.get('test_file_labels', [])\n",
    "print(f\"Loaded {len(test_file_paths)} held-out test files\")\n",
    "\n",
    "# 3. Initialize model with saved config and load weights\n",
    "model = ConvModel(\n",
    "    mel_freq_bins=config['mel_freq_bins'],\n",
    "    time_steps=config['time_steps'],\n",
    "    num_classes=config['num_classes']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 4. Define preprocessing function\n",
    "def preprocess_audio(audio_path, config):\n",
    "    \"\"\"Load and preprocess audio to match training data shape.\"\"\"\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample to match training sample rate (librosa default is 22050)\n",
    "    target_sr = config['sample_rate']  # 22050\n",
    "    if sr != target_sr:\n",
    "        resampler = transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Ensure mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Pad or truncate to match training audio length\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = waveform.shape[1]\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        # Pad (center padding like training)\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        waveform = t.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "    elif current_length > target_length:\n",
    "        # Truncate (center crop)\n",
    "        start = (current_length - target_length) // 2\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    \n",
    "    # Use default MelSpectrogram (matches AudioDataset training setup)\n",
    "    mel_transform = transforms.MelSpectrogram()\n",
    "    spec = mel_transform(waveform)\n",
    "    \n",
    "    return spec.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# 5. Define prediction function\n",
    "def predict_digit(audio_path):\n",
    "    \"\"\"Predict the digit from an audio file.\"\"\"\n",
    "    spec = preprocess_audio(audio_path, config).to(device)\n",
    "    \n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "print(f\"Model loaded successfully! Ready for inference.\")\n",
    "print(f\"Use predict_digit('path/to/audio.wav') to make predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Testing: True Held-Out Test Set vs Random Files\n",
    "\n",
    "The cells below demonstrate why having a proper test set matters:\n",
    "\n",
    "1. **True Test Set** - Files the model NEVER saw during training (saved in checkpoint)\n",
    "2. **Random Files** - Random samples from `processed/` (some may have been in training!)\n",
    "\n",
    "If your model is overfitting, you'll see higher accuracy on random files than on the true test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the TRUE HELD-OUT TEST SET (model never saw these during training!)\n",
    "import random\n",
    "\n",
    "if not test_file_paths:\n",
    "    print(\"No test set found in checkpoint. Re-run training with the updated save cell.\")\n",
    "else:\n",
    "    # Test on a sample of the held-out test set\n",
    "    num_tests = min(20, len(test_file_paths))\n",
    "    test_indices = random.sample(range(len(test_file_paths)), num_tests)\n",
    "    correct = 0\n",
    "    \n",
    "    print(f\"Testing on {num_tests} files from the TRUE HELD-OUT TEST SET\")\n",
    "    print(f\"(Total held-out files: {len(test_file_paths)})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"These files were NEVER seen during training!\\n\")\n",
    "    \n",
    "    for i, idx in enumerate(test_indices):\n",
    "        test_file = test_file_paths[idx]\n",
    "        true_label = test_file_labels[idx]\n",
    "        predicted_digit, confidence = predict_digit(test_file)\n",
    "        \n",
    "        is_correct = predicted_digit == true_label\n",
    "        correct += is_correct\n",
    "        \n",
    "        status = \"correct\" if is_correct else \"WRONG\"\n",
    "        print(f\"{i+1}. True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"TRUE TEST SET Accuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a random file from the processed dataset\n",
    "# (Works after kernel restart - just needs the inference cell to be run first)\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# Get all processed audio files\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "print(f\"Found {len(processed_files)} files in processed/\")\n",
    "\n",
    "# Pick a random file\n",
    "test_file = random.choice(processed_files)\n",
    "\n",
    "# Extract true label from folder name (e.g., \"processed/3/3_123.wav\" -> 3)\n",
    "true_label = int(test_file.split('/')[-2])\n",
    "\n",
    "# Make prediction\n",
    "predicted_digit, confidence = predict_digit(test_file)\n",
    "\n",
    "print(f\"\\nTest file: {test_file}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted: {predicted_digit}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n",
    "print(f\"Correct: {'Yes' if predicted_digit == true_label else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on RANDOM files from processed/ (MAY INCLUDE TRAINING DATA!)\n",
    "# Compare this accuracy to the true test set above to see if the model is overfitting\n",
    "import random\n",
    "import glob\n",
    "\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "num_tests = min(20, len(processed_files))\n",
    "correct = 0\n",
    "\n",
    "# Get random files\n",
    "test_files = random.sample(processed_files, num_tests)\n",
    "\n",
    "print(f\"Testing on {num_tests} RANDOM files from processed/\")\n",
    "print(\"=\" * 50)\n",
    "print(\"WARNING: Some of these may have been in the training set!\\n\")\n",
    "\n",
    "for i, test_file in enumerate(test_files):\n",
    "    true_label = int(test_file.split('/')[-2])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{i+1}. True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"RANDOM FILES Accuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")\n",
    "print(\"\\nCompare this to the TRUE TEST SET accuracy above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 10 random files from the processed dataset\n",
    "import glob\n",
    "import random\n",
    "\n",
    "processed_files = glob.glob('processed/*/*')\n",
    "print(f\"Found {len(processed_files)} files in processed/\\n\")\n",
    "\n",
    "num_tests = 10\n",
    "test_files = random.sample(processed_files, num_tests)\n",
    "correct = 0\n",
    "\n",
    "for i, test_file in enumerate(test_files):\n",
    "    true_label = int(test_file.split('/')[-2])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{i+1}. {test_file.split('/')[-1]}: True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{num_tests} = {100*correct/num_tests:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on ALL original recordings from the unprocessed dataset\n",
    "import glob\n",
    "\n",
    "unprocessed_files = sorted(glob.glob('unprocessed/*.wav'))\n",
    "print(f\"Found {len(unprocessed_files)} original recordings in unprocessed/\\n\")\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for test_file in unprocessed_files:\n",
    "    # Extract true label from filename (e.g., \"0.wav\" -> 0)\n",
    "    true_label = int(test_file.split('/')[-1].split('.')[0])\n",
    "    predicted_digit, confidence = predict_digit(test_file)\n",
    "    \n",
    "    is_correct = predicted_digit == true_label\n",
    "    correct += is_correct\n",
    "    \n",
    "    status = \"correct\" if is_correct else \"WRONG\"\n",
    "    print(f\"{test_file}: True={true_label}, Pred={predicted_digit}, Conf={confidence:.1%} [{status}]\")\n",
    "\n",
    "print(f\"\\nAccuracy on original recordings: {correct}/{len(unprocessed_files)} = {100*correct/len(unprocessed_files):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Real-time Digit Recognition (Voice Activated)\n",
    "\n",
    "Run the cell below to do live digit recognition from your microphone. \n",
    "\n",
    "**Requirements:** Run the inference cell (cell 18) first to load the model!\n",
    "\n",
    "**How it works:**\n",
    "1. Listens for audio above the RMS volume threshold (voice activity detection)\n",
    "2. When speech is detected, starts recording\n",
    "3. Stops recording after silence is detected (or max duration reached)\n",
    "4. Preprocesses the audio (resample, pad, mel spectrogram)\n",
    "5. Runs inference and displays the predicted digit\n",
    "6. Returns to listening for the next utterance\n",
    "\n",
    "**Tunable parameters:**\n",
    "- `VOLUME_THRESHOLD`: RMS level to trigger recording (increase if noisy environment)\n",
    "- `SILENCE_DURATION`: How long to wait for silence before stopping\n",
    "- `MIN_RECORDING_DURATION`: Minimum recording length\n",
    "- `MAX_RECORDING_DURATION`: Maximum recording length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time digit recognition with voice activity detection (VAD)\n",
    "# Requires the inference cell (cell 18) to be run first!\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torchaudio import transforms\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Config\n",
    "SAMPLE_RATE = 44100  # Recording sample rate\n",
    "TARGET_SAMPLE_RATE = 22050  # Model expects this (librosa default)\n",
    "BLOCK_SIZE = 1024  # Samples per callback\n",
    "\n",
    "# Voice Activity Detection settings\n",
    "VOLUME_THRESHOLD = 0.02  # RMS threshold to detect speech (adjust if needed)\n",
    "SILENCE_DURATION = 0.5  # Seconds of silence before stopping recording\n",
    "MIN_RECORDING_DURATION = 0.3  # Minimum seconds to record\n",
    "MAX_RECORDING_DURATION = 3.0  # Maximum seconds to record\n",
    "\n",
    "# State variables\n",
    "audio_buffer = []\n",
    "is_recording = False\n",
    "silence_samples = 0\n",
    "silence_samples_threshold = int(SILENCE_DURATION * SAMPLE_RATE / BLOCK_SIZE)\n",
    "min_samples = int(MIN_RECORDING_DURATION * SAMPLE_RATE)\n",
    "max_samples = int(MAX_RECORDING_DURATION * SAMPLE_RATE)\n",
    "\n",
    "def process_and_predict(audio_data):\n",
    "    \"\"\"Process accumulated audio and run prediction.\"\"\"\n",
    "    # Normalize audio (peak normalization to match training data levels)\n",
    "    audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-8)\n",
    "    audio_data = audio_data * 0.9  # Scale to ~90% to avoid clipping\n",
    "    \n",
    "    # Convert to tensor\n",
    "    waveform = t.tensor(audio_data, dtype=t.float32).unsqueeze(0)\n",
    "    \n",
    "    # Resample to match training\n",
    "    resampler = transforms.Resample(orig_freq=SAMPLE_RATE, new_freq=TARGET_SAMPLE_RATE)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    # Pad or truncate to match training audio length\n",
    "    target_length = config['longest_audio_file_length']\n",
    "    current_length = waveform.shape[1]\n",
    "    \n",
    "    if current_length < target_length:\n",
    "        pad_size = target_length - current_length\n",
    "        left_pad = pad_size // 2\n",
    "        right_pad = pad_size - left_pad\n",
    "        waveform = t.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "    elif current_length > target_length:\n",
    "        start = (current_length - target_length) // 2\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    \n",
    "    # Compute mel spectrogram (using defaults to match training)\n",
    "    mel_transform = transforms.MelSpectrogram()\n",
    "    spec = mel_transform(waveform).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with t.no_grad():\n",
    "        output = model(spec)\n",
    "        predicted = t.argmax(output, dim=1).item()\n",
    "        confidence = t.softmax(output, dim=1)[0, predicted].item()\n",
    "    \n",
    "    return predicted, confidence\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Called for each block of audio from the microphone.\"\"\"\n",
    "    global audio_buffer, is_recording, silence_samples\n",
    "    \n",
    "    if status:\n",
    "        print(f\"Status: {status}\")\n",
    "    \n",
    "    # Calculate RMS volume for this block\n",
    "    audio_block = indata[:, 0]\n",
    "    rms = np.sqrt(np.mean(audio_block**2))\n",
    "    \n",
    "    if not is_recording:\n",
    "        # Waiting for speech to start\n",
    "        if rms > VOLUME_THRESHOLD:\n",
    "            is_recording = True\n",
    "            silence_samples = 0\n",
    "            audio_buffer = list(audio_block)  # Start with this block\n",
    "            clear_output(wait=True)\n",
    "            print(\"Recording... (speak your digit)\")\n",
    "    else:\n",
    "        # Currently recording\n",
    "        audio_buffer.extend(audio_block.tolist())\n",
    "        \n",
    "        if rms < VOLUME_THRESHOLD:\n",
    "            silence_samples += 1\n",
    "        else:\n",
    "            silence_samples = 0  # Reset silence counter if sound detected\n",
    "        \n",
    "        # Check if we should stop recording\n",
    "        should_stop = False\n",
    "        \n",
    "        if len(audio_buffer) >= max_samples:\n",
    "            should_stop = True  # Hit max duration\n",
    "        elif silence_samples >= silence_samples_threshold and len(audio_buffer) >= min_samples:\n",
    "            should_stop = True  # Silence detected after minimum recording\n",
    "        \n",
    "        if should_stop:\n",
    "            # Process the recording\n",
    "            audio_data = np.array(audio_buffer)\n",
    "            duration = len(audio_data) / SAMPLE_RATE\n",
    "            \n",
    "            predicted, confidence = process_and_predict(audio_data)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"  Predicted Digit:  {predicted}\")\n",
    "            print(f\"  Confidence:       {confidence:.1%}\")\n",
    "            print(f\"  Audio duration:   {duration:.2f}s\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"\\nListening... (speak a digit to start)\")\n",
    "            \n",
    "            # Reset state\n",
    "            audio_buffer = []\n",
    "            is_recording = False\n",
    "            silence_samples = 0\n",
    "\n",
    "# Start listening\n",
    "print(\"Real-time Digit Recognition (Voice Activated)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Volume threshold: {VOLUME_THRESHOLD} RMS\")\n",
    "print(f\"Silence timeout:  {SILENCE_DURATION}s\")\n",
    "print(f\"Max recording:    {MAX_RECORDING_DURATION}s\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\nListening... (speak a digit to start)\")\n",
    "print(\"\\n>>> Click the STOP button (square) or press 'i' twice to stop <<<\")\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        blocksize=BLOCK_SIZE,\n",
    "        channels=1,\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "    \n",
    "    # Loop with short sleeps - much easier to interrupt\n",
    "    while True:\n",
    "        sd.sleep(100)  # 100ms intervals - responsive to interrupts\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if stream is not None:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "    audio_buffer = []\n",
    "    is_recording = False\n",
    "    silence_samples = 0\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Exercise - Build a version of Audrey using your own recorrdings to create a \"template database\" and make your own sound / speech recognition project. Try doing this with different kinds of utterances that aren't just numbers.\n",
    " # \n",
    " # e.g.\n",
    " # yes / no\n",
    " # colors\n",
    " # shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
