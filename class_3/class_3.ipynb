{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition with Classical ML\n",
    "\n",
    "Humans communicate emotion not just through *what* we say, but through *how* we say it. The same sentence - \"I'm fine\" - can convey happiness, sadness, anger, or sarcasm depending on tone, pitch, and rhythm.\n",
    "\n",
    "**Speech Emotion Recognition (SER)** is the task of automatically detecting emotion from speech audio. In this notebook, we'll build SER systems using classical machine learning - no neural networks required!\n",
    "\n",
    "### What you'll learn:\n",
    "\n",
    "1. **Downloading datasets** from Kaggle (RAVDESS, TESS, SAVEE, CREMA-D)\n",
    "2. **Exploring** audio datasets with pandas\n",
    "3. **Listening** to and **visualizing** emotional speech\n",
    "4. **Data augmentation** - creating variations of audio to improve models\n",
    "5. **Feature extraction** - turning audio into numbers that ML models can understand\n",
    "6. **Classification** with scikit-learn (Random Forest, XGBoost, Logistic Regression, SVM)\n",
    "7. **Evaluating** model performance with confusion matrices and classification reports\n",
    "8. **Combining datasets** for better, more diverse training data\n",
    "9. **Real-time** emotion recognition from your microphone\n",
    "\n",
    "### Why classical ML?\n",
    "\n",
    "Before diving into deep learning, it's important to understand the fundamentals. Classical ML models are:\n",
    "- **Fast** to train (seconds, not hours)\n",
    "- **Interpretable** (you can understand what they're doing)\n",
    "- **Great baselines** to compare against more complex approaches\n",
    "\n",
    "### Background Reading\n",
    "\n",
    "- [The 7 Basic Emotions](https://www.humintell.com/2010/06/the-seven-basic-emotions-do-you-know-them/) - Paul Ekman's foundational research\n",
    "- [On the Praxes and Politics of AI Speech Emotion Recognition](https://dl.acm.org/doi/10.1145/3593013.3594011) - Edward B. Kang (FAccT 2023)\n",
    "- [EU AI Act restrictions on emotion recognition](https://ai-act-law.eu/recital/18/) - Why OpenAI's Advanced Voice Mode isn't available in the EU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Installation\n",
    "\n",
    "Before we begin, we need to install a few Python libraries. Run the cell below to install everything.\n",
    "\n",
    "**What are these libraries?**\n",
    "\n",
    "| Library | What it does |\n",
    "|---------|-------------|\n",
    "| `librosa` | Audio analysis and feature extraction |\n",
    "| `soundfile` | Reading and writing audio files |\n",
    "| `sounddevice` | Recording audio from your microphone |\n",
    "| `pandas` | Working with tabular data (like spreadsheets in Python) |\n",
    "| `matplotlib` / `seaborn` | Creating charts and visualizations |\n",
    "| `scikit-learn` | Machine learning models and tools |\n",
    "| `xgboost` | Gradient boosting classifier (a powerful ML algorithm) |\n",
    "| `kaggle` | Downloading datasets from Kaggle |\n",
    "| `opendatasets` | Alternative way to download Kaggle datasets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries by running:\n",
    "# `uv pip install -r requirements.txt`\n",
    "# If you've already installed these, you can skip this cell\n",
    "# Test that everything is installed correctly\n",
    "# If any of these fail, recreate you environment\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import xgboost\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"librosa version: {librosa.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"xgboost version: {xgboost.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 2: Downloading the RAVDESS Dataset from Kaggle\n",
    "\n",
    "We'll start with the **RAVDESS** (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset.\n",
    "\n",
    "### About RAVDESS\n",
    "- **24 professional actors** (12 female, 12 male)\n",
    "- **8 emotions**: neutral, calm, happy, sad, angry, fearful, disgust, surprised\n",
    "- **1,440 audio files** total\n",
    "- Each actor speaks two sentences with different emotions and intensities\n",
    "\n",
    "### Setting up Kaggle\n",
    "\n",
    "To download datasets from Kaggle, you need an API key:\n",
    "\n",
    "1. Go to [kaggle.com](https://www.kaggle.com) and create an account (if you don't have one)\n",
    "2. Go to your profile → Settings → API → \"Create New Token\"\n",
    "3. This downloads a `kaggle.json` file\n",
    "4. Place it in `~/.kaggle/kaggle.json` (Mac/Linux) or `C:\\Users\\<username>\\.kaggle\\kaggle.json` (Windows)\n",
    "\n",
    "Alternatively, you can use the `opendatasets` library which will prompt you for your credentials.\n",
    "\n",
    "**Attribution**: \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\" by Livingstone & Russo is licensed under CC BY-NA-SC 4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download using the kaggle CLI (requires kaggle.json to be set up)\n",
    "# Uncomment the line below if you have kaggle.json configured\n",
    "\n",
    "# !kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Download using opendatasets (will prompt for your Kaggle username and key)\n",
    "# This is often easier for first-time setup\n",
    "\n",
    "import opendatasets as od\n",
    "\n",
    "od.download(\n",
    "    'https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio',\n",
    "    data_dir='datasets/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you downloaded as a zip file, unzip it here\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = 'datasets/ravdess-emotional-speech-audio.zip'\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall('datasets/ravdess-emotional-speech-audio')\n",
    "    print(\"Unzipped successfully!\")\n",
    "else:\n",
    "    print(\"No zip file found - if you used opendatasets, the files are already extracted.\")\n",
    "    print(\"Check the datasets/ folder to see what's there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we downloaded!\n",
    "# The RAVDESS dataset is organized into folders by actor\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Adjust this path based on how your download was structured\n",
    "# Try both common paths\n",
    "ravdess_paths = [\n",
    "    'datasets/ravdess-emotional-speech-audio/audio_speech_actors_01-24/',\n",
    "    'datasets/ravdess-emotional-speech-audio/',\n",
    "]\n",
    "\n",
    "ravdess_dir = None\n",
    "for path in ravdess_paths:\n",
    "    if os.path.exists(path):\n",
    "        ravdess_dir = path\n",
    "        break\n",
    "\n",
    "if ravdess_dir is None:\n",
    "    print(\"Could not find RAVDESS dataset. Please check the datasets/ folder.\")\n",
    "    print(\"Contents of datasets/:\")\n",
    "    for item in os.listdir('datasets/'):\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    # Count all wav files\n",
    "    all_files = glob.glob(os.path.join(ravdess_dir, '**/*.wav'), recursive=True)\n",
    "    print(f\"Found {len(all_files)} audio files in {ravdess_dir}\")\n",
    "    print(f\"\\nFirst 5 files:\")\n",
    "    for f in all_files[:5]:\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Part 3: Exploring the Dataset with Pandas\n",
    "\n",
    "**Pandas** is a Python library for working with tabular data - think of it like a programmable spreadsheet.\n",
    "\n",
    "We'll create a **DataFrame** (a table) from our audio files. Each row will represent one audio file, with columns for the file path and its metadata.\n",
    "\n",
    "### RAVDESS Filename Convention\n",
    "\n",
    "Each RAVDESS file has a 7-part numerical identifier, e.g. `03-01-06-01-02-01-12.wav`:\n",
    "\n",
    "| Position | Meaning | Values |\n",
    "|----------|---------|--------|\n",
    "| [0] | Modality | 01=full-AV, 02=video-only, 03=audio-only |\n",
    "| [1] | Vocal channel | 01=speech, 02=song |\n",
    "| [2] | **Emotion** | 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised |\n",
    "| [3] | Emotional intensity | 01=normal, 02=strong |\n",
    "| [4] | Statement | 01=\"Kids are talking by the door\", 02=\"Dogs are sitting by the door\" |\n",
    "| [5] | Repetition | 01=1st, 02=2nd |\n",
    "| [6] | Actor | 01-24 (odd=male, even=female) |\n",
    "\n",
    "So `03-01-06-01-02-01-12.wav` means: Audio-only, Speech, Fearful, Normal intensity, \"Dogs\" statement, 1st repetition, Actor 12 (Female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Emotion mapping - maps the code number to a human-readable label\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Build a list of dictionaries - one for each file\n",
    "data_rows = []\n",
    "\n",
    "all_files = glob.glob(os.path.join(ravdess_dir, '**/*.wav'), recursive=True)\n",
    "\n",
    "for file_path in all_files:\n",
    "    # Get the filename without extension and split by '-'\n",
    "    filename = os.path.basename(file_path)  # e.g., '03-01-06-01-02-01-12.wav'\n",
    "    parts = filename.split('.')[0].split('-')  # ['03', '01', '06', '01', '02', '01', '12']\n",
    "    \n",
    "    # Extract info from filename\n",
    "    actor_id = int(parts[6])\n",
    "    \n",
    "    data_rows.append({\n",
    "        'file_path': file_path,\n",
    "        'emotion_code': parts[2],\n",
    "        'emotion': emotion_map[parts[2]],\n",
    "        'intensity': 'normal' if parts[3] == '01' else 'strong',\n",
    "        'statement': 'Kids are talking by the door' if parts[4] == '01' else 'Dogs are sitting by the door',\n",
    "        'repetition': int(parts[5]),\n",
    "        'actor': actor_id,\n",
    "        'gender': 'male' if actor_id % 2 == 1 else 'female',\n",
    "        'dataset': 'RAVDESS'\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "ravdess_df = pd.DataFrame(data_rows)\n",
    "\n",
    "print(f\"Created DataFrame with {len(ravdess_df)} rows and {len(ravdess_df.columns)} columns\")\n",
    "print(f\"\\nColumns: {list(ravdess_df.columns)}\")\n",
    "print(f\"\\nEmotions: {ravdess_df['emotion'].unique()}\")\n",
    "\n",
    "# Show the first few rows\n",
    "ravdess_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of our data\n",
    "# How many samples do we have per emotion? Per gender?\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"Samples per emotion:\")\n",
    "print(\"=\" * 40)\n",
    "print(ravdess_df['emotion'].value_counts())\n",
    "\n",
    "print(f\"\\n{'=' * 40}\")\n",
    "print(\"Samples per gender:\")\n",
    "print(\"=\" * 40)\n",
    "print(ravdess_df['gender'].value_counts())\n",
    "\n",
    "print(f\"\\n{'=' * 40}\")\n",
    "print(\"Samples per intensity:\")\n",
    "print(\"=\" * 40)\n",
    "print(ravdess_df['intensity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Part 4: Listening to the Audio Files\n",
    "\n",
    "Before building any models, we should **listen** to our data! This helps us develop intuition about:\n",
    "- How different emotions sound\n",
    "- How clear the emotional expression is\n",
    "- Whether the dataset quality is good\n",
    "\n",
    "We'll use `IPython.display.Audio` to create playable audio widgets right in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to one random sample from each emotion\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(\"Listening to one sample from each emotion:\\n\")\n",
    "\n",
    "for emotion in sorted(ravdess_df['emotion'].unique()):\n",
    "    # Get one random sample of this emotion\n",
    "    sample = ravdess_df[ravdess_df['emotion'] == emotion].sample(1).iloc[0]\n",
    "    \n",
    "    print(f\"Emotion: {emotion.upper()} | Actor: {sample['actor']} | Gender: {sample['gender']} | Intensity: {sample['intensity']}\")\n",
    "    print(f'  Saying: \"{sample[\"statement\"]}\"')\n",
    "    display(Audio(sample['file_path']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to play a random recording with all its info\n",
    "\n",
    "def play_random_audio(df):\n",
    "    \"\"\"Play a random audio file from the DataFrame and display its info.\"\"\"\n",
    "    sample = df.sample(1).iloc[0]\n",
    "    \n",
    "    print(f\"Emotion:   {sample['emotion']}\")\n",
    "    print(f\"Gender:    {sample['gender']}\")\n",
    "    print(f\"Actor:     {sample['actor']}\")\n",
    "    if 'intensity' in sample:\n",
    "        print(f\"Intensity: {sample['intensity']}\")\n",
    "    if 'statement' in sample:\n",
    "        print(f\"Statement: {sample['statement']}\")\n",
    "    print(f\"Dataset:   {sample['dataset']}\")\n",
    "    print(f\"File:      {sample['file_path']}\")\n",
    "    display(Audio(sample['file_path']))\n",
    "\n",
    "# Try it out! Run this cell multiple times to hear different samples\n",
    "play_random_audio(ravdess_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Part 5: Visualizing the Data\n",
    "\n",
    "Visualization helps us understand patterns in our data before building models. We'll look at:\n",
    "\n",
    "1. **Bar charts** - How many samples per emotion, gender, etc.\n",
    "2. **Waveforms** - The raw audio signal over time\n",
    "3. **Spectrograms** - A visual representation of frequencies over time\n",
    "4. **Mel spectrograms** - Spectrograms on a perceptual scale (how humans hear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: number of samples per emotion\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Emotion distribution\n",
    "emotion_counts = ravdess_df['emotion'].value_counts()\n",
    "sns.barplot(x=emotion_counts.index, y=emotion_counts.values, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Samples per Emotion (RAVDESS)')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gender distribution per emotion\n",
    "gender_emotion = ravdess_df.groupby(['emotion', 'gender']).size().unstack(fill_value=0)\n",
    "gender_emotion.plot(kind='bar', ax=axes[1], color=['#2196F3', '#E91E63'])\n",
    "axes[1].set_title('Samples per Emotion by Gender')\n",
    "axes[1].set_xlabel('Emotion')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(title='Gender')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: 'neutral' has fewer samples because it only has 'normal' intensity (no 'strong' version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Waveforms and Spectrograms\n",
    "\n",
    "**Waveform** (also called a \"wave plot\"): Shows the raw audio signal - amplitude (loudness) over time. Think of it as what the air pressure looks like as sound waves hit a microphone.\n",
    "\n",
    "**Spectrogram**: Shows which **frequencies** are present at each point in time. It's like a heat map where:\n",
    "- X-axis = Time\n",
    "- Y-axis = Frequency (pitch)\n",
    "- Color = How loud that frequency is\n",
    "\n",
    "**Mel Spectrogram**: A spectrogram where the frequency axis is scaled to match how humans perceive pitch. Lower frequencies get more detail (we're more sensitive to differences there).\n",
    "\n",
    "Let's see how different emotions look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize waveforms for different emotions\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "def plot_waveform(file_path, emotion, ax):\n",
    "    \"\"\"Plot the waveform for an audio file.\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    librosa.display.waveshow(y, sr=sr, ax=ax)\n",
    "    ax.set_title(f'{emotion.upper()}')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "\n",
    "# Pick one sample from each emotion\n",
    "emotions = sorted(ravdess_df['emotion'].unique())\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, emotion in enumerate(emotions):\n",
    "    sample = ravdess_df[ravdess_df['emotion'] == emotion].sample(1).iloc[0]\n",
    "    plot_waveform(sample['file_path'], emotion, axes[i])\n",
    "\n",
    "plt.suptitle('Waveforms for Each Emotion', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Can you see visual differences between the emotions?\")\n",
    "print(\"Angry speech tends to have higher amplitude (louder), while sad speech is often quieter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mel spectrograms for different emotions\n",
    "\n",
    "def plot_mel_spectrogram(file_path, emotion, ax):\n",
    "    \"\"\"Plot a mel spectrogram for an audio file.\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "    ax.set_title(f'{emotion.upper()}')\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, emotion in enumerate(emotions):\n",
    "    sample = ravdess_df[ravdess_df['emotion'] == emotion].sample(1).iloc[0]\n",
    "    plot_mel_spectrogram(sample['file_path'], emotion, axes[i])\n",
    "\n",
    "plt.suptitle('Mel Spectrograms for Each Emotion', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Spectrograms show the 'fingerprint' of each emotion.\")\n",
    "print(\"Notice how angry speech often has more energy across all frequencies (brighter colors).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed view: waveform + spectrogram + audio player for a single recording\n",
    "\n",
    "def visualize_audio(file_path, title=\"Audio\"):\n",
    "    \"\"\"Show waveform, mel spectrogram, and audio player for a file.\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Waveform\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0])\n",
    "    axes[0].set_title(f'Waveform - {title}')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[1])\n",
    "    axes[1].set_title(f'Mel Spectrogram - {title}')\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(Audio(file_path))\n",
    "\n",
    "# Visualize a random sample\n",
    "sample = ravdess_df.sample(1).iloc[0]\n",
    "print(f\"Statement: \\\"{sample['statement']}\\\"\")\n",
    "visualize_audio(sample['file_path'], f\"{sample['emotion']} ({sample['gender']}, {sample['intensity']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Part 6: Data Augmentation\n",
    "\n",
    "**Data augmentation** means creating new training examples by slightly modifying existing ones. This is important because:\n",
    "\n",
    "1. **More data = better models** (usually)\n",
    "2. **Variety helps generalization** - the model learns the *concept* of an emotion, not just specific recordings\n",
    "3. **Simulates real-world conditions** - noise, different speaking speeds, etc.\n",
    "\n",
    "### Common Audio Augmentations\n",
    "\n",
    "| Augmentation | What it does | Why it helps |\n",
    "|-------------|-------------|-------------|\n",
    "| **Noise** | Adds random background noise | Simulates noisy environments |\n",
    "| **Time Stretch** | Speeds up or slows down | Simulates different speaking paces |\n",
    "| **Pitch Shift** | Raises or lowers pitch | Simulates voice variation |\n",
    "| **Time Shift** | Shifts audio left/right | Simulates different recording starts |\n",
    "\n",
    "We won't augment the entire dataset right now (that would take a while), but we'll use augmentation during feature extraction to create more training samples on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation functions\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def add_noise(data, noise_factor=0.005):\n",
    "    \"\"\"Add random background noise to audio.\n",
    "    \n",
    "    noise_factor controls how much noise to add (higher = more noise).\n",
    "    Think of it like adding static to a radio signal.\n",
    "    \"\"\"\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    return augmented_data\n",
    "\n",
    "def time_stretch(data, rate=1.0):\n",
    "    \"\"\"Speed up or slow down audio without changing pitch.\n",
    "    \n",
    "    rate > 1.0 = faster\n",
    "    rate < 1.0 = slower\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "def pitch_shift(data, sr, n_steps=0):\n",
    "    \"\"\"Shift pitch up or down by n_steps semitones.\n",
    "    \n",
    "    n_steps > 0 = higher pitch\n",
    "    n_steps < 0 = lower pitch\n",
    "    A semitone is the smallest interval in Western music (one piano key).\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def time_shift(data, shift_max=0.2):\n",
    "    \"\"\"Shift audio left or right in time.\n",
    "    \n",
    "    shift_max = maximum fraction of total length to shift.\n",
    "    \"\"\"\n",
    "    shift = int(len(data) * shift_max * np.random.uniform(-1, 1))\n",
    "    return np.roll(data, shift)\n",
    "\n",
    "print(\"Augmentation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see and hear what each augmentation does!\n",
    "\n",
    "# Pick a random sample\n",
    "sample = ravdess_df.sample(1).iloc[0]\n",
    "y, sr = librosa.load(sample['file_path'], sr=None)\n",
    "\n",
    "print(f\"Original: {sample['emotion']} ({sample['gender']})\")\n",
    "\n",
    "# Create augmented versions\n",
    "augmentations = {\n",
    "    'Original': y,\n",
    "    'Added Noise': add_noise(y, noise_factor=0.01),\n",
    "    'Time Stretched (faster)': time_stretch(y, rate=1.3),\n",
    "    'Pitch Shifted (+3 semitones)': pitch_shift(y, sr, n_steps=3),\n",
    "    'Time Shifted': time_shift(y, shift_max=0.2),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(augmentations), 1, figsize=(14, 3 * len(augmentations)))\n",
    "\n",
    "for i, (name, audio) in enumerate(augmentations.items()):\n",
    "    librosa.display.waveshow(audio, sr=sr, ax=axes[i])\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].set_xlabel('Time (s)')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Play each version\n",
    "for name, audio in augmentations.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    display(Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Part 7: Feature Extraction\n",
    "\n",
    "Machine learning models can't understand raw audio waveforms directly. We need to convert audio into **numerical features** - numbers that describe important characteristics of the sound.\n",
    "\n",
    "Think of it like this: instead of giving someone a painting, you describe it: \"It's mostly blue, has 3 people, is painted in oils, and measures 2x3 feet.\" That description (the features) is what the ML model works with.\n",
    "\n",
    "### Audio Features We'll Extract\n",
    "\n",
    "| Feature | What it measures | Why it matters for emotion |\n",
    "|---------|-----------------|---------------------------|\n",
    "| **MFCCs** (Mel-Frequency Cepstral Coefficients) | The \"shape\" of the sound spectrum | Captures the overall timbre/quality of voice |\n",
    "| **Chroma** | Which musical pitches are present | Related to the melodic pattern of speech |\n",
    "| **Zero Crossing Rate** | How often the signal crosses zero | Higher for noisy/percussive sounds (angry speech) |\n",
    "| **RMS Energy** | Overall loudness | Angry = loud, sad = quiet |\n",
    "| **Mel Spectrogram** (mean) | Average energy at each frequency band | Overall frequency profile of the voice |\n",
    "\n",
    "### What are MFCCs?\n",
    "\n",
    "MFCCs are the most commonly used features in speech processing. They work like this:\n",
    "1. Break audio into small overlapping windows\n",
    "2. Compute the frequency spectrum for each window\n",
    "3. Map to the Mel scale (matches human hearing)\n",
    "4. Apply a mathematical transform to get a compact representation\n",
    "\n",
    "The result is typically 13-40 numbers per time window that compactly describe what the voice sounds like. We'll take the **mean** across all windows to get one set of numbers per audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(data, sr):\n",
    "    \"\"\"Extract audio features from a waveform.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array of audio samples\n",
    "        sr: sample rate\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # 1. MFCCs - 40 coefficients, take mean across time\n",
    "    # These capture the overall \"shape\" of the sound\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=40)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  # Average across time -> 40 values\n",
    "    features.extend(mfccs_mean)\n",
    "    \n",
    "    # 2. Chroma features - 12 pitch classes, take mean\n",
    "    # Related to musical notes present in the speech\n",
    "    chroma = librosa.feature.chroma_stft(y=data, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)  # 12 values\n",
    "    features.extend(chroma_mean)\n",
    "    \n",
    "    # 3. Zero Crossing Rate - how often the signal crosses zero\n",
    "    # Higher for noisy/breathy/aggressive sounds\n",
    "    zcr = librosa.feature.zero_crossing_rate(data)\n",
    "    zcr_mean = np.mean(zcr)  # 1 value\n",
    "    features.append(zcr_mean)\n",
    "    \n",
    "    # 4. RMS Energy - overall loudness\n",
    "    rms = librosa.feature.rms(y=data)\n",
    "    rms_mean = np.mean(rms)  # 1 value\n",
    "    features.append(rms_mean)\n",
    "    \n",
    "    # 5. Spectral Centroid - \"brightness\" of the sound\n",
    "    # Higher centroid = brighter/sharper sound\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)  # 1 value\n",
    "    features.append(spectral_centroid_mean)\n",
    "    \n",
    "    # 6. Spectral Bandwidth - range of frequencies\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sr)\n",
    "    spectral_bandwidth_mean = np.mean(spectral_bandwidth)  # 1 value\n",
    "    features.append(spectral_bandwidth_mean)\n",
    "    \n",
    "    # 7. Spectral Rolloff - frequency below which 85% of energy is concentrated\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=sr)\n",
    "    spectral_rolloff_mean = np.mean(spectral_rolloff)  # 1 value\n",
    "    features.append(spectral_rolloff_mean)\n",
    "    \n",
    "    # Total: 40 + 12 + 1 + 1 + 1 + 1 + 1 = 57 features\n",
    "    return np.array(features)\n",
    "\n",
    "# Test it on one file\n",
    "sample = ravdess_df.sample(1).iloc[0]\n",
    "y, sr = librosa.load(sample['file_path'])\n",
    "features = extract_features(y, sr)\n",
    "\n",
    "print(f\"Extracted {len(features)} features from one audio file\")\n",
    "print(f\"Feature vector shape: {features.shape}\")\n",
    "print(f\"\\nFirst 10 features (MFCCs): {features[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's extract features from ALL files in the RAVDESS dataset\n",
    "# We'll also apply data augmentation to increase our training data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_features_with_augmentation(file_path, sr=22050):\n",
    "    \"\"\"Extract features from original audio AND augmented versions.\n",
    "    \n",
    "    For each audio file, we create:\n",
    "    1. Original features\n",
    "    2. Noisy version features\n",
    "    3. Stretched version features  \n",
    "    4. Pitched version features\n",
    "    5. Shifted version features\n",
    "    \n",
    "    This gives us 5x the training data!\n",
    "    \"\"\"\n",
    "    data, sample_rate = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    # Original\n",
    "    all_features.append(extract_features(data, sample_rate))\n",
    "    \n",
    "    # Augmented versions\n",
    "    all_features.append(extract_features(add_noise(data), sample_rate))\n",
    "    all_features.append(extract_features(time_stretch(data, rate=np.random.uniform(0.8, 1.2)), sample_rate))\n",
    "    all_features.append(extract_features(pitch_shift(data, sample_rate, n_steps=np.random.randint(-3, 4)), sample_rate))\n",
    "    all_features.append(extract_features(time_shift(data), sample_rate))\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# Extract features from all RAVDESS files\n",
    "print(\"Extracting features from RAVDESS dataset (with augmentation)...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "X_ravdess = []  # Features\n",
    "y_ravdess = []  # Labels (emotions)\n",
    "\n",
    "for idx, row in tqdm(ravdess_df.iterrows(), total=len(ravdess_df)):\n",
    "    try:\n",
    "        features_list = extract_features_with_augmentation(row['file_path'])\n",
    "        for features in features_list:\n",
    "            X_ravdess.append(features)\n",
    "            y_ravdess.append(row['emotion'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['file_path']}: {e}\")\n",
    "\n",
    "X_ravdess = np.array(X_ravdess)\n",
    "y_ravdess = np.array(y_ravdess)\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Features shape: {X_ravdess.shape}  ({X_ravdess.shape[0]} samples, {X_ravdess.shape[1]} features each)\")\n",
    "print(f\"Labels shape: {y_ravdess.shape}\")\n",
    "print(f\"\\nOriginal files: {len(ravdess_df)}\")\n",
    "print(f\"With augmentation: {len(X_ravdess)} (5x more!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Part 8: Classification with scikit-learn\n",
    "\n",
    "Now for the exciting part - training ML models to recognize emotions!\n",
    "\n",
    "### What is Classification?\n",
    "\n",
    "**Classification** is a type of machine learning where the model learns to assign **categories** (classes) to inputs. In our case:\n",
    "- **Input**: 57 audio features (numbers)\n",
    "- **Output**: One of 8 emotions\n",
    "\n",
    "### Train/Test Split\n",
    "\n",
    "We split our data into two parts:\n",
    "- **Training set (80%)**: The model learns from these examples\n",
    "- **Test set (20%)**: We evaluate performance on examples the model has *never seen*\n",
    "\n",
    "**Why split?** If we test on the same data we trained on, the model could just memorize the answers. The test set tells us how well the model *generalizes* to new data.\n",
    "\n",
    "### The Models We'll Try\n",
    "\n",
    "| Model | How it works (simplified) | Complexity |\n",
    "|-------|-------------------------|----------|\n",
    "| **KNN** (K-Nearest Neighbors) | Looks at the K most similar training examples | Simplest - no real \"training\", just memorize data |\n",
    "| **Logistic Regression** | Draws lines/boundaries between classes | Simple linear model, fast and interpretable |\n",
    "| **SVM** (Support Vector Machine) | Finds the best separating boundary | More powerful, works well in high dimensions |\n",
    "| **Random Forest** | Builds many decision trees and takes a vote | Ensemble method - robust, handles messy data |\n",
    "| **XGBoost** | Builds trees sequentially, each fixing previous mistakes | Most sophisticated - often the best classical ML model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Encode emotion labels as numbers (ML models need numbers, not strings)\n",
    "# e.g., 'angry' -> 0, 'calm' -> 1, 'disgust' -> 2, etc.\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_ravdess)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} -> {i}\")\n",
    "\n",
    "# Split into training and test sets\n",
    "# test_size=0.2 means 20% of data is held out for testing\n",
    "# random_state=42 ensures reproducible results (same split every time)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ravdess, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Ensures each emotion is equally represented in train and test\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Normalize the features\n",
    "# StandardScaler makes each feature have mean=0 and std=1\n",
    "# This is important because features have very different scales\n",
    "# (e.g., MFCCs might be -500 to 500, while ZCR is 0 to 0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data\n",
    "X_test_scaled = scaler.transform(X_test)         # Apply same scaling to test data\n",
    "\n",
    "print(f\"\\nFeatures normalized. Shape: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train multiple models and compare performance\n",
    "# We'll go from simplest to most sophisticated!\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Define our models - ordered from simplest to most complex\n",
    "models = {\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=5          # Look at 5 nearest training examples\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,         # Maximum iterations for convergence\n",
    "        random_state=42\n",
    "    ),\n",
    "    'SVM (RBF Kernel)': SVC(\n",
    "        kernel='rbf',          # Radial Basis Function kernel\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=300,      # Number of trees in the forest\n",
    "        max_depth=20,          # Maximum depth of each tree\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=300,      # Number of boosting rounds\n",
    "        learning_rate=0.1,     # How much each tree contributes\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'  # Multi-class log loss\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "print(\"Training and evaluating models (simplest -> most complex)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'time': elapsed,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:25s} | Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%) | Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\nBest model: {best_model_name} with {results[best_model_name]['accuracy']*100:.1f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "\n",
    "# Fixed order: simplest to most complex\n",
    "model_names = ['K-Nearest Neighbors', 'Logistic Regression', 'SVM (RBF Kernel)', 'Random Forest', 'XGBoost']\n",
    "accuracies = [results[name]['accuracy'] * 100 for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.barh(model_names, accuracies, color=['#2196F3', '#4CAF50', '#FF9800', '#9C27B0', '#F44336'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Comparison - RAVDESS Dataset')\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Part 9: Model Evaluation\n",
    "\n",
    "Accuracy alone doesn't tell the whole story. We need to understand:\n",
    "- **Which emotions** does the model get right vs. wrong?\n",
    "- **Are certain emotions confused** with each other? (e.g., calm vs. neutral)\n",
    "- **Is the model biased** toward certain classes?\n",
    "\n",
    "### Evaluation Tools\n",
    "\n",
    "| Tool | What it shows |\n",
    "|------|---------------|\n",
    "| **Confusion Matrix** | A grid showing predicted vs. actual labels - reveals which emotions are confused |\n",
    "| **Classification Report** | Precision, recall, and F1-score per emotion |\n",
    "| **Precision** | Of all samples predicted as X, how many were actually X? |\n",
    "| **Recall** | Of all actual X samples, how many did we correctly predict? |\n",
    "| **F1-Score** | Harmonic mean of precision and recall (balanced metric) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the best model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions from the best model\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=label_encoder.classes_\n",
    ")\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}\\n(RAVDESS Only)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read this matrix:\")\n",
    "print(\"- Rows = actual emotion, Columns = predicted emotion\")\n",
    "print(\"- Diagonal = correct predictions (higher is better)\")\n",
    "print(\"- Off-diagonal = mistakes (shows which emotions get confused)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "\n",
    "print(f\"Classification Report - {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    best_predictions, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "print(\"\\nWhat do these metrics mean?\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Precision: When the model says 'angry', how often is it right?\")\n",
    "print(\"Recall:    Of all truly angry samples, how many did we find?\")\n",
    "print(\"F1-score:  Balance between precision and recall\")\n",
    "print(\"Support:   Number of test samples for each emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for ALL models side by side\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Fixed order: simplest to most complex\n",
    "model_names = ['K-Nearest Neighbors', 'Logistic Regression', 'SVM (RBF Kernel)', 'Random Forest', 'XGBoost']\n",
    "\n",
    "for i, name in enumerate(model_names):\n",
    "    if i >= len(model_names):\n",
    "        break\n",
    "    result = results[name]\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=label_encoder.classes_\n",
    "    )\n",
    "    disp.plot(ax=axes[i], cmap='Blues', values_format='d')\n",
    "    axes[i].set_title(f'{name}\\n({result[\"accuracy\"]*100:.1f}%)', fontsize=11)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide the extra subplot\n",
    "if len(results) < 6:\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Models (RAVDESS)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Take a moment to look at the confusion matrices above. You'll likely notice:\n",
    "\n",
    "1. **\"Calm\" and \"Neutral\"** are often confused - they sound similar!\n",
    "2. **\"Happy\" and \"Surprised\"** sometimes get mixed up\n",
    "3. **\"Angry\"** is usually well-recognized - it has distinctive features (loud, fast, sharp)\n",
    "\n",
    "This makes intuitive sense. Even humans sometimes struggle to distinguish calm from neutral speech.\n",
    "\n",
    "### Can we do better?\n",
    "\n",
    "Our dataset only has **1,440 files** from **24 actors** speaking **2 sentences**. That's quite limited!\n",
    "\n",
    "In the next section, we'll add more datasets to create a larger, more diverse training set. But as we'll discover, **more data doesn't automatically mean better accuracy** - it depends on the quality and consistency of the data. This is an important lesson in ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Hearing the Model in Action\n",
    "\n",
    "Numbers are great, but let's actually **listen** to test set samples and see what the model predicts!\n",
    "\n",
    "Below, we pick random samples from the **held-out test set** (data the model never saw during training), play the audio, and show the model's prediction alongside the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to test set samples and see model predictions!\n",
    "# The model has NEVER seen these samples during training.\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import random\n",
    "\n",
    "# Use the best model from our RAVDESS training\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "# Get indices for a random sample of test examples\n",
    "num_samples = 10\n",
    "sample_indices = random.sample(range(len(X_test)), num_samples)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Evaluating on {num_samples} random test samples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    # Get the prediction\n",
    "    features_scaled = X_test_scaled[idx].reshape(1, -1)\n",
    "    prediction = best_model.predict(features_scaled)[0]\n",
    "    predicted_emotion = label_encoder.inverse_transform([prediction])[0]\n",
    "    true_emotion = label_encoder.inverse_transform([y_test[idx]])[0]\n",
    "    \n",
    "    is_correct = predicted_emotion == true_emotion\n",
    "    correct += is_correct\n",
    "    status = 'CORRECT' if is_correct else 'WRONG'\n",
    "    \n",
    "    # Find the original file path for this test sample\n",
    "    # (We need to trace back from the augmented features to the original file)\n",
    "    # Since we used stratified split, we can find matching files from our DataFrame\n",
    "    matching_files = ravdess_df[ravdess_df['emotion'] == true_emotion]\n",
    "    if len(matching_files) > 0:\n",
    "        sample_file = matching_files.sample(1).iloc[0]\n",
    "        print(f\"\\nTrue: {true_emotion:12s} | Predicted: {predicted_emotion:12s} | {status}\")\n",
    "        if 'statement' in sample_file:\n",
    "            print(f\"  Statement: \\\"{sample_file['statement']}\\\"\")\n",
    "        display(Audio(sample_file['file_path']))\n",
    "    else:\n",
    "        print(f\"\\nTrue: {true_emotion:12s} | Predicted: {predicted_emotion:12s} | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Score: {correct}/{num_samples} correct ({100*correct/num_samples:.0f}%)\")\n",
    "print(f\"\\nRun this cell again to hear different samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Building a Larger Dataset\n",
    "\n",
    "One of the most effective ways to improve ML models is to give them **more diverse data**. We'll now download 3 additional speech emotion datasets and combine them with RAVDESS.\n",
    "\n",
    "### Additional Datasets\n",
    "\n",
    "| Dataset | Actors | Emotions | Total Files | Language |\n",
    "|---------|--------|----------|-------------|----------|\n",
    "| **RAVDESS** | 24 | 8 | 1,440 | English (North American) |\n",
    "| **TESS** | 2 | 7 | 2,800 | English (Canadian) |\n",
    "| **SAVEE** | 4 | 7 | 480 | English (British) |\n",
    "| **CREMA-D** | 91 | 6 | 7,442 | English (Various) |\n",
    "| **Combined** | 121 | varies | ~12,000+ | English (Multiple accents) |\n",
    "\n",
    "### Why Combine Datasets?\n",
    "\n",
    "- **More speakers** = model learns emotion patterns that are consistent across different voices\n",
    "- **More accents** = better generalization to new speakers\n",
    "- **More data** = more examples to learn from\n",
    "\n",
    "### Important: Emotion Label Harmonization\n",
    "\n",
    "Different datasets use slightly different emotion categories. We need to map them to a **common set**. We'll use these emotions that are shared across most datasets:\n",
    "\n",
    "**neutral, happy, sad, angry, fearful, disgust, surprised**\n",
    "\n",
    "(We'll drop \"calm\" since it's only in RAVDESS and is hard to distinguish from neutral.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional datasets from Kaggle\n",
    "# Uncomment the method you prefer (kaggle CLI or opendatasets)\n",
    "\n",
    "# --- Option 1: kaggle CLI ---\n",
    "# !kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p datasets/\n",
    "# !kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee -p datasets/\n",
    "# !kaggle datasets download -d ejlok1/cremad -p datasets/\n",
    "\n",
    "# --- Option 2: opendatasets ---\n",
    "import opendatasets as od\n",
    "\n",
    "od.download('https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess', data_dir='datasets/')\n",
    "od.download('https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee', data_dir='datasets/')\n",
    "od.download('https://www.kaggle.com/datasets/ejlok1/cremad', data_dir='datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip any datasets that need it\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_files = {\n",
    "    'datasets/toronto-emotional-speech-set-tess.zip': 'datasets/tess/',\n",
    "    'datasets/surrey-audiovisual-expressed-emotion-savee.zip': 'datasets/savee/',\n",
    "    'datasets/cremad.zip': 'datasets/cremad/',\n",
    "}\n",
    "\n",
    "for zip_path, dest_dir in zip_files.items():\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Unzipping {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(dest_dir)\n",
    "        print(f\"  -> Extracted to {dest_dir}\")\n",
    "    else:\n",
    "        print(f\"No zip found for {zip_path} - may already be extracted\")\n",
    "\n",
    "# Show what we have in datasets/\n",
    "print(\"\\nContents of datasets/ folder:\")\n",
    "for item in sorted(os.listdir('datasets/')):\n",
    "    full_path = os.path.join('datasets/', item)\n",
    "    if os.path.isdir(full_path):\n",
    "        file_count = sum(1 for _ in glob.glob(os.path.join(full_path, '**/*.wav'), recursive=True))\n",
    "        print(f\"  [DIR] {item} ({file_count} wav files)\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Processing TESS (Toronto Emotional Speech Set)\n",
    "\n",
    "**TESS**: Two actresses (aged 26 and 64) say 200 target words in 7 emotions.\n",
    "\n",
    "File structure: The emotion label is in the **folder name** (e.g., `OAF_angry/`, `YAF_happy/`).\n",
    "- `OAF` = Older Adult Female\n",
    "- `YAF` = Young Adult Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process TESS dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find TESS directory (structure may vary by download method)\n",
    "tess_candidates = [\n",
    "    'datasets/tess/',\n",
    "    'datasets/toronto-emotional-speech-set-tess/',\n",
    "    'datasets/tess/TESS Toronto emotional speech set data/',\n",
    "    'datasets/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/',\n",
    "]\n",
    "\n",
    "tess_dir = None\n",
    "for path in tess_candidates:\n",
    "    if os.path.exists(path):\n",
    "        # Check if wav files exist here or in subdirectories\n",
    "        wavs = glob.glob(os.path.join(path, '**/*.wav'), recursive=True)\n",
    "        if len(wavs) > 0:\n",
    "            tess_dir = path\n",
    "            break\n",
    "\n",
    "if tess_dir is None:\n",
    "    print(\"Could not find TESS dataset. Please check the datasets/ folder.\")\n",
    "else:\n",
    "    print(f\"Found TESS at: {tess_dir}\")\n",
    "    \n",
    "    # TESS emotion mapping (folder names contain the emotion)\n",
    "    tess_emotion_map = {\n",
    "        'angry': 'angry',\n",
    "        'disgust': 'disgust',\n",
    "        'fear': 'fearful',\n",
    "        'happy': 'happy',\n",
    "        'neutral': 'neutral',\n",
    "        'ps': 'surprised',  # \"pleasant surprise\"\n",
    "        'sad': 'sad',\n",
    "    }\n",
    "    \n",
    "    tess_rows = []\n",
    "    all_tess_files = glob.glob(os.path.join(tess_dir, '**/*.wav'), recursive=True)\n",
    "    \n",
    "    for file_path in all_tess_files:\n",
    "        # Get emotion from the folder name or filename\n",
    "        # TESS files are like: OAF_angry/OAF_back_angry.wav\n",
    "        filename = os.path.basename(file_path).lower()\n",
    "        parent_dir = os.path.basename(os.path.dirname(file_path)).lower()\n",
    "        \n",
    "        # Try to find the emotion label\n",
    "        emotion_found = None\n",
    "        for key, value in tess_emotion_map.items():\n",
    "            if key in parent_dir or key in filename:\n",
    "                emotion_found = value\n",
    "                break\n",
    "        \n",
    "        if emotion_found:\n",
    "            # Determine speaker from filename\n",
    "            gender = 'female'  # TESS only has female speakers\n",
    "            speaker = 'OAF' if 'oaf' in filename or 'oaf' in parent_dir else 'YAF'\n",
    "            \n",
    "            tess_rows.append({\n",
    "                'file_path': file_path,\n",
    "                'emotion': emotion_found,\n",
    "                'gender': gender,\n",
    "                'actor': speaker,\n",
    "                'dataset': 'TESS'\n",
    "            })\n",
    "    \n",
    "    tess_df = pd.DataFrame(tess_rows)\n",
    "    print(f\"\\nTESS: {len(tess_df)} files\")\n",
    "    print(tess_df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Processing SAVEE (Surrey Audio-Visual Expressed Emotion)\n",
    "\n",
    "**SAVEE**: 4 male English speakers, 7 emotions, 480 total files.\n",
    "\n",
    "File naming: The first two letters indicate emotion:\n",
    "- `a` = angry, `d` = disgust, `f` = fear, `h` = happy, `n` = neutral, `sa` = sad, `su` = surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SAVEE dataset\n",
    "\n",
    "savee_candidates = [\n",
    "    'datasets/savee/',\n",
    "    'datasets/surrey-audiovisual-expressed-emotion-savee/',\n",
    "]\n",
    "\n",
    "savee_dir = None\n",
    "for path in savee_candidates:\n",
    "    if os.path.exists(path):\n",
    "        wavs = glob.glob(os.path.join(path, '**/*.wav'), recursive=True)\n",
    "        if len(wavs) > 0:\n",
    "            savee_dir = path\n",
    "            break\n",
    "\n",
    "if savee_dir is None:\n",
    "    print(\"Could not find SAVEE dataset. Please check the datasets/ folder.\")\n",
    "else:\n",
    "    print(f\"Found SAVEE at: {savee_dir}\")\n",
    "    \n",
    "    # SAVEE emotion codes (prefix of filename)\n",
    "    savee_emotion_map = {\n",
    "        'a': 'angry',\n",
    "        'd': 'disgust',\n",
    "        'f': 'fearful',\n",
    "        'h': 'happy',\n",
    "        'n': 'neutral',\n",
    "        'sa': 'sad',\n",
    "        'su': 'surprised',\n",
    "    }\n",
    "    \n",
    "    savee_rows = []\n",
    "    all_savee_files = glob.glob(os.path.join(savee_dir, '**/*.wav'), recursive=True)\n",
    "    \n",
    "    for file_path in all_savee_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # SAVEE filenames: DC_a01.wav, DC_sa02.wav, etc.\n",
    "        # The speaker is before _, emotion code is after _\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            speaker = parts[0]\n",
    "            emotion_part = parts[1].split('.')[0]  # e.g., 'a01', 'sa02'\n",
    "            \n",
    "            # Extract emotion code\n",
    "            emotion_found = None\n",
    "            # Check longer codes first (sa, su before s)\n",
    "            for code in sorted(savee_emotion_map.keys(), key=len, reverse=True):\n",
    "                if emotion_part.startswith(code):\n",
    "                    emotion_found = savee_emotion_map[code]\n",
    "                    break\n",
    "            \n",
    "            if emotion_found:\n",
    "                savee_rows.append({\n",
    "                    'file_path': file_path,\n",
    "                    'emotion': emotion_found,\n",
    "                    'gender': 'male',  # SAVEE only has male speakers\n",
    "                    'actor': speaker,\n",
    "                    'dataset': 'SAVEE'\n",
    "                })\n",
    "    \n",
    "    savee_df = pd.DataFrame(savee_rows)\n",
    "    print(f\"\\nSAVEE: {len(savee_df)} files\")\n",
    "    print(savee_df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Processing CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)\n",
    "\n",
    "**CREMA-D**: 91 actors (diverse ages, ethnicities), 6 emotions, 7,442 files.\n",
    "\n",
    "This is the largest and most diverse dataset. File naming includes the emotion code:\n",
    "- `ANG` = angry, `DIS` = disgust, `FEA` = fear, `HAP` = happy, `NEU` = neutral, `SAD` = sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CREMA-D dataset\n",
    "\n",
    "cremad_candidates = [\n",
    "    'datasets/cremad/',\n",
    "    'datasets/cremad/AudioWAV/',\n",
    "]\n",
    "\n",
    "cremad_dir = None\n",
    "for path in cremad_candidates:\n",
    "    if os.path.exists(path):\n",
    "        wavs = glob.glob(os.path.join(path, '**/*.wav'), recursive=True)\n",
    "        if len(wavs) > 0:\n",
    "            cremad_dir = path\n",
    "            break\n",
    "\n",
    "if cremad_dir is None:\n",
    "    print(\"Could not find CREMA-D dataset. Please check the datasets/ folder.\")\n",
    "else:\n",
    "    print(f\"Found CREMA-D at: {cremad_dir}\")\n",
    "    \n",
    "    # CREMA-D emotion codes\n",
    "    cremad_emotion_map = {\n",
    "        'ANG': 'angry',\n",
    "        'DIS': 'disgust',\n",
    "        'FEA': 'fearful',\n",
    "        'HAP': 'happy',\n",
    "        'NEU': 'neutral',\n",
    "        'SAD': 'sad',\n",
    "    }\n",
    "    \n",
    "    cremad_rows = []\n",
    "    all_cremad_files = glob.glob(os.path.join(cremad_dir, '**/*.wav'), recursive=True)\n",
    "    \n",
    "    for file_path in all_cremad_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # CREMA-D filenames: 1001_DFA_ANG_XX.wav\n",
    "        # Format: ActorID_Sentence_Emotion_Level.wav\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            actor_id = parts[0]\n",
    "            emotion_code = parts[2]\n",
    "            \n",
    "            if emotion_code in cremad_emotion_map:\n",
    "                cremad_rows.append({\n",
    "                    'file_path': file_path,\n",
    "                    'emotion': cremad_emotion_map[emotion_code],\n",
    "                    'gender': 'unknown',  # CREMA-D doesn't encode gender in filename\n",
    "                    'actor': actor_id,\n",
    "                    'dataset': 'CREMA-D'\n",
    "                })\n",
    "    \n",
    "    cremad_df = pd.DataFrame(cremad_rows)\n",
    "    print(f\"\\nCREMA-D: {len(cremad_df)} files\")\n",
    "    print(cremad_df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets into one big DataFrame!\n",
    "\n",
    "# Start with RAVDESS (drop 'calm' to match other datasets)\n",
    "ravdess_combined = ravdess_df[ravdess_df['emotion'] != 'calm'][['file_path', 'emotion', 'gender', 'actor', 'dataset']].copy()\n",
    "\n",
    "# Collect all available DataFrames\n",
    "all_dfs = [ravdess_combined]\n",
    "\n",
    "if 'tess_df' in dir() and len(tess_df) > 0:\n",
    "    all_dfs.append(tess_df)\n",
    "    \n",
    "if 'savee_df' in dir() and len(savee_df) > 0:\n",
    "    all_dfs.append(savee_df)\n",
    "    \n",
    "if 'cremad_df' in dir() and len(cremad_df) > 0:\n",
    "    all_dfs.append(cremad_df)\n",
    "\n",
    "# Combine!\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(combined_df)} total files\")\n",
    "print(f\"\\nSamples per dataset:\")\n",
    "print(combined_df['dataset'].value_counts())\n",
    "print(f\"\\nSamples per emotion:\")\n",
    "print(combined_df['emotion'].value_counts())\n",
    "print(f\"\\nEmotions: {sorted(combined_df['emotion'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the combined dataset\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Emotion distribution in combined dataset\n",
    "emotion_by_dataset = combined_df.groupby(['emotion', 'dataset']).size().unstack(fill_value=0)\n",
    "emotion_by_dataset.plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                        color=['#2196F3', '#4CAF50', '#FF9800', '#E91E63'])\n",
    "axes[0].set_title('Samples per Emotion (by Dataset)')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(title='Dataset')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_counts = combined_df['dataset'].value_counts()\n",
    "axes[1].pie(dataset_counts.values, labels=dataset_counts.index, autopct='%1.1f%%',\n",
    "           colors=['#2196F3', '#4CAF50', '#FF9800', '#E91E63'])\n",
    "axes[1].set_title('Dataset Composition')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal: {len(combined_df)} audio files from {combined_df['actor'].nunique()} unique actors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to samples from different datasets for the same emotion\n",
    "\n",
    "emotion_to_listen = 'angry'  # Change this to hear other emotions!\n",
    "\n",
    "print(f\"Listening to '{emotion_to_listen}' samples from each dataset:\\n\")\n",
    "\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    subset = combined_df[(combined_df['emotion'] == emotion_to_listen) & (combined_df['dataset'] == dataset)]\n",
    "    if len(subset) > 0:\n",
    "        sample = subset.sample(1).iloc[0]\n",
    "        print(f\"Dataset: {dataset} | Actor: {sample['actor']} | Gender: {sample['gender']}\")\n",
    "        display(Audio(sample['file_path']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# Part 11: Feature Extraction on the Combined Dataset\n",
    "\n",
    "Now let's extract features from our much larger combined dataset. This will take longer, but the extra data should improve our models significantly.\n",
    "\n",
    "**Note**: This cell may take 10-20 minutes depending on your computer. Go grab a coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the combined dataset\n",
    "# Using augmentation to further increase training data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Extracting features from {len(combined_df)} files (with augmentation)...\")\n",
    "print(\"This will take a while - each file generates 5 augmented versions.\")\n",
    "print(f\"Expected total samples: ~{len(combined_df) * 5}\\n\")\n",
    "\n",
    "X_combined = []  # Features\n",
    "y_combined = []  # Labels\n",
    "errors = []\n",
    "\n",
    "for idx, row in tqdm(combined_df.iterrows(), total=len(combined_df)):\n",
    "    try:\n",
    "        features_list = extract_features_with_augmentation(row['file_path'])\n",
    "        for features in features_list:\n",
    "            X_combined.append(features)\n",
    "            y_combined.append(row['emotion'])\n",
    "    except Exception as e:\n",
    "        errors.append((row['file_path'], str(e)))\n",
    "\n",
    "X_combined = np.array(X_combined)\n",
    "y_combined = np.array(y_combined)\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Features shape: {X_combined.shape}\")\n",
    "print(f\"Labels shape: {y_combined.shape}\")\n",
    "if errors:\n",
    "    print(f\"Errors encountered: {len(errors)} files could not be processed\")\n",
    "    for path, err in errors[:5]:\n",
    "        print(f\"  {path}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Part 12: Classification on the Combined Dataset\n",
    "\n",
    "Let's see how our models perform with the larger, more diverse dataset.\n",
    "\n",
    "**Hypothesis**: More data from more speakers should help the model generalize better... but will it?\n",
    "\n",
    "One important thing to keep in mind: RAVDESS is a very **controlled** dataset (professional actors, studio recording conditions, consistent setup). Our combined dataset mixes in data from different recording environments, different labeling conventions, and different speaker pools. This added **diversity** is realistic, but also makes the classification task **harder**.\n",
    "\n",
    "Let's find out what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the combined dataset for training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "label_encoder_combined = LabelEncoder()\n",
    "y_combined_encoded = label_encoder_combined.fit_transform(y_combined)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for i, label in enumerate(label_encoder_combined.classes_):\n",
    "    print(f\"  {label} -> {i}\")\n",
    "\n",
    "# Split the data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_combined, y_combined_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_combined_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_combined = StandardScaler()\n",
    "X_train_c_scaled = scaler_combined.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler_combined.transform(X_test_c)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_c_scaled.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_c_scaled.shape[0]} samples\")\n",
    "print(f\"Number of features: {X_train_c_scaled.shape[1]}\")\n",
    "print(f\"Number of emotions: {len(label_encoder_combined.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models on the combined dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "models_combined = {\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=300, max_depth=20, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='mlogloss'),\n",
    "}\n",
    "\n",
    "results_combined = {}\n",
    "\n",
    "print(\"Training and evaluating models on COMBINED dataset (simplest -> most complex)...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for name, model in models_combined.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train_c_scaled, y_train_c)\n",
    "    y_pred_c = model.predict(X_test_c_scaled)\n",
    "    accuracy = accuracy_score(y_test_c, y_pred_c)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results_combined[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred_c,\n",
    "        'time': elapsed,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:25s} | Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%) | Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "best_combined_name = max(results_combined, key=lambda x: results_combined[x]['accuracy'])\n",
    "print(f\"\\nBest model: {best_combined_name} with {results_combined[best_combined_name]['accuracy']*100:.1f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAVDESS-only vs Combined dataset performance\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Fixed order: simplest to most complex\n",
    "model_names = ['K-Nearest Neighbors', 'Logistic Regression', 'SVM (RBF Kernel)', 'Random Forest', 'XGBoost']\n",
    "\n",
    "# RAVDESS only\n",
    "ravdess_accs = [results[name]['accuracy'] * 100 for name in model_names]\n",
    "bars1 = axes[0].barh(model_names, ravdess_accs, color='#2196F3')\n",
    "axes[0].set_xlabel('Accuracy (%)')\n",
    "axes[0].set_title('RAVDESS Only')\n",
    "axes[0].set_xlim(0, 100)\n",
    "for bar, acc in zip(bars1, ravdess_accs):\n",
    "    axes[0].text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{acc:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# Combined\n",
    "combined_accs = [results_combined[name]['accuracy'] * 100 for name in model_names]\n",
    "bars2 = axes[1].barh(model_names, combined_accs, color='#4CAF50')\n",
    "axes[1].set_xlabel('Accuracy (%)')\n",
    "axes[1].set_title('Combined Dataset (RAVDESS + TESS + SAVEE + CREMA-D)')\n",
    "axes[1].set_xlim(0, 100)\n",
    "for bar, acc in zip(bars2, combined_accs):\n",
    "    axes[1].text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{acc:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Effect of More Data on Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement\n",
    "print(\"\\nImprovement from RAVDESS-only to Combined:\")\n",
    "print(\"-\" * 50)\n",
    "for name in model_names:\n",
    "    diff = results_combined[name]['accuracy'] - results[name]['accuracy']\n",
    "    arrow = '+' if diff > 0 else ''\n",
    "    print(f\"  {name:25s}: {arrow}{diff*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model on combined dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "best_pred_combined = results_combined[best_combined_name]['predictions']\n",
    "\n",
    "# Confusion matrix\n",
    "cm_combined = confusion_matrix(y_test_c, best_pred_combined)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_combined,\n",
    "    display_labels=label_encoder_combined.classes_\n",
    ")\n",
    "disp.plot(ax=ax, cmap='Greens', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_combined_name}\\n(Combined Dataset)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report - {best_combined_name} (Combined Dataset)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    y_test_c,\n",
    "    best_pred_combined,\n",
    "    target_names=label_encoder_combined.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Interpreting the Results: Why More Data Didn't Always Help\n",
    "\n",
    "You might have expected the combined dataset to always outperform RAVDESS-only. But the results tell a more nuanced story! Here's what's going on:\n",
    "\n",
    "#### Why accuracy sometimes *decreased*:\n",
    "\n",
    "| Factor | Impact |\n",
    "|--------|--------|\n",
    "| **Domain shift** | Each dataset was recorded in different studios, with different microphones, room acoustics, and signal quality. Our features (MFCCs, spectral centroid, etc.) capture not just emotion but also these recording characteristics. When you train and test on RAVDESS, the recording conditions match perfectly. The combined dataset mixes 4 different recording pipelines, so the features are partially encoding \"which dataset is this from?\" instead of \"what emotion is this?\" |\n",
    "| **Label interpretation** | \"Happy\" performed by a RAVDESS professional actor in a controlled session sounds quite different from \"happy\" in CREMA-D's crowd-sourced actors. The label is the same string, but the acoustic realization differs. The model is trying to learn one boundary for \"happy\" when there are really multiple overlapping distributions. |\n",
    "| **Dataset dominance** | CREMA-D has ~7,400 files vs. RAVDESS's ~1,440. CREMA-D dominates the combined training data. If CREMA-D's recordings are noisier or its emotional expressions more ambiguous, the model shifts toward that distribution and gets *worse* on cleaner data. |\n",
    "| **Measuring different things** | RAVDESS-only accuracy measures \"can you classify emotions within a controlled lab setting?\" Combined accuracy measures \"can you classify emotions across wildly different recording conditions and acting styles?\" The second task is genuinely much harder - it's not a fair apples-to-apples comparison. |\n",
    "\n",
    "#### The RAVDESS advantage:\n",
    "\n",
    "RAVDESS is a very **controlled** dataset - professional actors, studio conditions, consistent recording setup. When you train AND test on RAVDESS, all the \"non-emotion\" information (microphone quality, room reverb, noise floor) is consistent, so the model can focus entirely on the emotional signal. The combined dataset introduces real-world messiness.\n",
    "\n",
    "#### This is a real phenomenon: Distribution Shift\n",
    "\n",
    "This shows up constantly in applied ML. A medical imaging model trained at one hospital often performs worse at another hospital, even with \"more data.\" A self-driving car trained in sunny California struggles in rainy Seattle. The technical term is **distribution shift** - when the training data and test data come from different underlying distributions.\n",
    "\n",
    "**Key takeaway**: Data quality and consistency matter just as much as data quantity. This is one of the most important lessons in applied ML!\n",
    "\n",
    "---\n",
    "\n",
    "### How Could We Actually Improve With More Data?\n",
    "\n",
    "More data *can* help, but we need to be smarter about how we combine it. Here are concrete strategies:\n",
    "\n",
    "#### 1. Per-Dataset Feature Normalization\n",
    "Instead of extracting features and mixing everything together, normalize features **within each dataset** before combining. This reduces the recording-condition bias so the model focuses on emotion rather than microphone type.\n",
    "\n",
    "```python\n",
    "# Example: normalize per dataset before combining\n",
    "for dataset_name in combined_df['dataset'].unique():\n",
    "    mask = (combined_df['dataset'] == dataset_name)\n",
    "    X[mask] = scaler.fit_transform(X[mask])\n",
    "```\n",
    "\n",
    "#### 2. More Robust Features\n",
    "Our hand-crafted features (MFCCs, spectral centroid) are sensitive to recording conditions. **Pre-trained deep learning embeddings** from models like [wav2vec2](https://huggingface.co/facebook/wav2vec2-base) or [HuBERT](https://huggingface.co/facebook/hubert-base-ls960) are trained on thousands of hours of speech and produce features that better capture *what's being said and how* rather than *what microphone was used*. You could extract these embeddings and feed them into the same sklearn classifiers we used here.\n",
    "\n",
    "#### 3. Domain Adaptation\n",
    "Train the model to explicitly *ignore* dataset-specific characteristics. Techniques include:\n",
    "- **Adversarial training**: Train a second model to predict which dataset a sample came from, and penalize the main model for making that task easy\n",
    "- **Dataset balancing**: Ensure equal representation from each dataset in every training batch\n",
    "\n",
    "#### 4. Consistent Preprocessing\n",
    "Before combining, resample all audio to the same sample rate, apply the same noise reduction, and normalize volume levels. This reduces some of the surface-level differences between datasets.\n",
    "\n",
    "#### 5. Evaluate Per-Dataset\n",
    "Instead of one combined accuracy number, evaluate **per dataset** to understand where the model improves vs. degrades. Maybe the combined model is better on CREMA-D and SAVEE but worse on RAVDESS - that's useful information!\n",
    "\n",
    "#### 6. Use Neural Networks on Spectrograms\n",
    "Instead of hand-crafting 57 features, feed mel spectrograms directly into a **Convolutional Neural Network (CNN)**. CNNs can learn which parts of the spectrogram are relevant for emotion and which are recording artifacts. This is the approach used by state-of-the-art SER systems.\n",
    "\n",
    "#### 7. Fine-tune Pre-trained Speech Models\n",
    "The current best approach: take a model like wav2vec2 that already understands speech, and **fine-tune** it on emotion data. These models have seen so much speech data that they've already learned to separate content from recording conditions.\n",
    "\n",
    "### Putting It in Perspective: Classical ML vs. Modern Approaches\n",
    "\n",
    "It's worth remembering that in this notebook we're using **classical ML techniques** - algorithms like Random Forest, SVM, and XGBoost that work on hand-crafted features. These are great for learning the fundamentals, but they have a ceiling.\n",
    "\n",
    "Modern speech emotion recognition systems use much more powerful approaches:\n",
    "\n",
    "| Approach | What we did | What modern systems do |\n",
    "|----------|------------|----------------------|\n",
    "| **Features** | Hand-crafted (MFCCs, chroma, ZCR) - 57 numbers per audio clip | Learned automatically from raw audio by deep neural networks - thousands of dimensions |\n",
    "| **Models** | Classical ML (Random Forest, SVM, XGBoost) | CNNs on spectrograms (like we saw in the previous class!), Transformers, and large pre-trained models |\n",
    "| **Training data** | ~12,000 clips with augmentation | Hundreds of thousands of hours of speech |\n",
    "| **Typical accuracy** | 50-85% depending on setup | 80-95%+ on standard benchmarks |\n",
    "\n",
    "The key difference is that neural networks can **learn their own features** directly from spectrograms or raw audio, rather than relying on us to decide which 57 numbers best describe emotion. CNNs can pick up on subtle patterns in spectrograms that we'd never think to extract by hand, and Transformers can capture long-range patterns across an entire utterance.\n",
    "\n",
    "**Next week**, we'll bridge this gap by looking at **Whisper** and how to **fine-tune a pre-trained model** for emotion recognition. You'll see how a model that has already learned to understand speech from massive datasets can be adapted to our emotion task with much better results - and surprisingly little code!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Hearing the Combined Model in Action\n",
    "\n",
    "Let's listen to test samples and compare what the RAVDESS-only model and the combined model predict for the same emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to test set samples from the combined dataset and see predictions!\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import random\n",
    "\n",
    "# Use the best model from the combined training\n",
    "best_model_c = results_combined[best_combined_name]['model']\n",
    "\n",
    "# Pick random test samples\n",
    "num_samples = 10\n",
    "sample_indices = random.sample(range(len(X_test_c)), num_samples)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "print(f\"Model: {best_combined_name} (trained on combined dataset)\")\n",
    "print(f\"Evaluating on {num_samples} random test samples...\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    # Get the prediction\n",
    "    features_scaled = X_test_c_scaled[idx].reshape(1, -1)\n",
    "    prediction = best_model_c.predict(features_scaled)[0]\n",
    "    predicted_emotion = label_encoder_combined.inverse_transform([prediction])[0]\n",
    "    true_emotion = label_encoder_combined.inverse_transform([y_test_c[idx]])[0]\n",
    "    \n",
    "    is_correct = predicted_emotion == true_emotion\n",
    "    correct += is_correct\n",
    "    status = 'CORRECT' if is_correct else 'WRONG'\n",
    "    \n",
    "    # Find a matching file to play\n",
    "    matching_files = combined_df[combined_df['emotion'] == true_emotion]\n",
    "    if len(matching_files) > 0:\n",
    "        sample_file = matching_files.sample(1).iloc[0]\n",
    "        print(f\"\\nTrue: {true_emotion:12s} | Predicted: {predicted_emotion:12s} | {status}\")\n",
    "        print(f\"  Dataset: {sample_file['dataset']}\")\n",
    "        if 'statement' in sample_file:\n",
    "            print(f\"  Statement: \\\"{sample_file['statement']}\\\"\")\n",
    "        display(Audio(sample_file['file_path']))\n",
    "    else:\n",
    "        print(f\"\\nTrue: {true_emotion:12s} | Predicted: {predicted_emotion:12s} | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(f\"Score: {correct}/{num_samples} correct ({100*correct/num_samples:.0f}%)\")\n",
    "print(f\"\\nRun this cell again to hear different samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Part 13: Real-time Emotion Recognition (Exercise)\n",
    "\n",
    "Now it's your turn! Let's use the trained model to recognize emotion from your own voice in real-time.\n",
    "\n",
    "### How it works:\n",
    "1. Record a short clip from your microphone\n",
    "2. Extract the same features we used for training\n",
    "3. Feed features into our best model\n",
    "4. Get an emotion prediction!\n",
    "\n",
    "**Important**: The model was trained on acted emotions, which tend to be more exaggerated than natural speech. Try being a bit theatrical when you speak!\n",
    "\n",
    "### Tips for best results:\n",
    "- Speak a full sentence (e.g., \"I can't believe this happened!\")\n",
    "- Exaggerate the emotion slightly\n",
    "- Speak clearly and at a normal volume\n",
    "- Try the same sentence with different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time emotion prediction function\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def predict_emotion(audio_data, sr, model, scaler, label_encoder):\n",
    "    \"\"\"Predict emotion from audio data.\n",
    "    \n",
    "    Args:\n",
    "        audio_data: numpy array of audio samples\n",
    "        sr: sample rate\n",
    "        model: trained sklearn model\n",
    "        scaler: fitted StandardScaler\n",
    "        label_encoder: fitted LabelEncoder\n",
    "    \n",
    "    Returns:\n",
    "        predicted emotion (string), confidence scores (dict)\n",
    "    \"\"\"\n",
    "    # Extract features (same function we used for training!)\n",
    "    features = extract_features(audio_data, sr)\n",
    "    features = features.reshape(1, -1)  # Reshape for single prediction\n",
    "    \n",
    "    # Scale features (using the same scaler from training)\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    predicted_emotion = label_encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Get prediction probabilities if the model supports it\n",
    "    confidence = {}\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(features_scaled)[0]\n",
    "        for i, label in enumerate(label_encoder.classes_):\n",
    "            confidence[label] = proba[i]\n",
    "    \n",
    "    return predicted_emotion, confidence\n",
    "\n",
    "def record_and_predict(duration=3, sr=22050):\n",
    "    \"\"\"Record audio and predict emotion.\"\"\"\n",
    "    print(f\"Recording for {duration} seconds... Speak now!\")\n",
    "    \n",
    "    # Record audio\n",
    "    recording = sd.rec(\n",
    "        int(duration * sr),\n",
    "        samplerate=sr,\n",
    "        channels=1,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    sd.wait()\n",
    "    recording = recording.flatten()\n",
    "    \n",
    "    print(\"Recording finished! Analyzing...\\n\")\n",
    "    \n",
    "    # Choose which model and scaler to use\n",
    "    # Use the combined dataset model if available, otherwise RAVDESS-only\n",
    "    if 'results_combined' in dir() and best_combined_name in results_combined:\n",
    "        model = results_combined[best_combined_name]['model']\n",
    "        sc = scaler_combined\n",
    "        le = label_encoder_combined\n",
    "        dataset_name = 'Combined'\n",
    "    else:\n",
    "        model = results[best_model_name]['model']\n",
    "        sc = scaler\n",
    "        le = label_encoder\n",
    "        dataset_name = 'RAVDESS'\n",
    "    \n",
    "    # Predict emotion\n",
    "    predicted_emotion, confidence = predict_emotion(recording, sr, model, sc, le)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"  Predicted Emotion: {predicted_emotion.upper()}\")\n",
    "    print(f\"  Model: {best_combined_name if dataset_name == 'Combined' else best_model_name}\")\n",
    "    print(f\"  Dataset: {dataset_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Show confidence scores if available\n",
    "    if confidence:\n",
    "        print(\"\\nConfidence scores:\")\n",
    "        for emotion, score in sorted(confidence.items(), key=lambda x: x[1], reverse=True):\n",
    "            bar = '#' * int(score * 40)\n",
    "            print(f\"  {emotion:12s} {score:6.1%} {bar}\")\n",
    "    \n",
    "    # Show waveform\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    librosa.display.waveshow(recording, sr=sr)\n",
    "    plt.title(f'Your Recording - Predicted: {predicted_emotion.upper()}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Play it back\n",
    "    display(Audio(recording, rate=sr))\n",
    "    \n",
    "    return predicted_emotion, confidence\n",
    "\n",
    "print(\"Ready! Use record_and_predict() to test emotion recognition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record yourself and get an emotion prediction!\n",
    "# Run this cell, speak a sentence with emotion, and see what the model thinks.\n",
    "#\n",
    "# Try saying the same sentence with different emotions:\n",
    "# \"I can't believe this is happening right now.\"\n",
    "# - Say it happily, angrily, sadly, fearfully, etc.\n",
    "\n",
    "predicted, confidence = record_and_predict(duration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive loop: Keep testing until you quit\n",
    "# Press Enter to record, type 'q' to quit\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INTERACTIVE EMOTION RECOGNITION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Instructions:\")\n",
    "print(\"  1. Press Enter to start recording (3 seconds)\")\n",
    "print(\"  2. Speak a sentence with emotion\")\n",
    "print(\"  3. See the prediction!\")\n",
    "print(\"  4. Type 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record (or 'q' to quit): \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "        record_and_predict(duration=3)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"\\nThanks for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Compare predictions from RAVDESS-only model vs Combined model\n",
    "# This shows how more data changes the model's behavior\n",
    "\n",
    "def compare_models(duration=3, sr=22050):\n",
    "    \"\"\"Record once and compare predictions from both models.\"\"\"\n",
    "    print(f\"Recording for {duration} seconds... Speak now!\")\n",
    "    \n",
    "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    recording = recording.flatten()\n",
    "    print(\"Recording finished!\\n\")\n",
    "    \n",
    "    # Extract features once\n",
    "    features = extract_features(recording, sr).reshape(1, -1)\n",
    "    \n",
    "    print(\"=\" * 55)\n",
    "    print(f\"{'Model':25s} | {'Predicted Emotion':20s}\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # RAVDESS-only model\n",
    "    if 'results' in dir():\n",
    "        for name in results:\n",
    "            model = results[name]['model']\n",
    "            features_scaled = scaler.transform(features)\n",
    "            pred = model.predict(features_scaled)[0]\n",
    "            emotion = label_encoder.inverse_transform([pred])[0]\n",
    "            print(f\"  RAVDESS {name:20s} | {emotion}\")\n",
    "    \n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Combined model\n",
    "    if 'results_combined' in dir():\n",
    "        for name in results_combined:\n",
    "            model = results_combined[name]['model']\n",
    "            features_scaled = scaler_combined.transform(features)\n",
    "            pred = model.predict(features_scaled)[0]\n",
    "            emotion = label_encoder_combined.inverse_transform([pred])[0]\n",
    "            print(f\"  Combined {name:19s} | {emotion}\")\n",
    "    \n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Play it back\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    librosa.display.waveshow(recording, sr=sr)\n",
    "    plt.title('Your Recording')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    display(Audio(recording, rate=sr))\n",
    "\n",
    "# Uncomment and run to compare:\n",
    "# compare_models(duration=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Now it's your turn to explore! Try these exercises to deepen your understanding.\n",
    "\n",
    "### Exercise 1: Feature Importance\n",
    "Random Forest models can tell you which features were most important for classification. Which audio features matter most for emotion recognition?\n",
    "\n",
    "### Exercise 2: Reduce Emotions\n",
    "Try classifying only 4 emotions (happy, sad, angry, neutral) instead of 7. Does accuracy improve?\n",
    "\n",
    "### Exercise 3: Gender Effects\n",
    "Train separate models for male and female speakers. Does it help?\n",
    "\n",
    "### Exercise 4: Hyperparameter Tuning\n",
    "Try changing model parameters (number of trees, max depth, learning rate, etc.) and see how it affects performance.\n",
    "\n",
    "### Exercise 5: Try It Without Augmentation\n",
    "Re-run feature extraction without augmentation and compare. How much does augmentation help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Feature Importance (starter code)\n",
    "# \n",
    "# Random Forest can tell us which features were most important!\n",
    "# This helps us understand what the model is \"looking at\"\n",
    "\n",
    "# Get the Random Forest model (from combined results if available)\n",
    "if 'results_combined' in dir() and 'Random Forest' in results_combined:\n",
    "    rf_model = results_combined['Random Forest']['model']\n",
    "    le = label_encoder_combined\n",
    "else:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    le = label_encoder\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create feature names\n",
    "feature_names = (\n",
    "    [f'MFCC_{i}' for i in range(40)] +\n",
    "    [f'Chroma_{i}' for i in range(12)] +\n",
    "    ['ZCR', 'RMS', 'Spectral_Centroid', 'Spectral_Bandwidth', 'Spectral_Rolloff']\n",
    ")\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot top 20 most important features\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_n = 20\n",
    "plt.barh(\n",
    "    [feature_names[i] for i in sorted_idx[:top_n]][::-1],\n",
    "    importances[sorted_idx[:top_n]][::-1],\n",
    "    color='#4CAF50'\n",
    ")\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top {top_n} Most Important Features for Emotion Recognition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: MFCCs tend to dominate! They capture the overall vocal quality.\")\n",
    "print(\"But spectral features and ZCR also contribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Reduce to 4 emotions (starter code)\n",
    "# \n",
    "# Try: happy, sad, angry, neutral\n",
    "# Does accuracy improve with fewer classes?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Filter combined_df to only include 4 emotions, then re-extract features\n",
    "# and re-train models. You can reuse the extract_features function.\n",
    "#\n",
    "# four_emotions = ['happy', 'sad', 'angry', 'neutral']\n",
    "# filtered_df = combined_df[combined_df['emotion'].isin(four_emotions)]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Gender-specific models (starter code)\n",
    "#\n",
    "# Train separate models for male and female speakers.\n",
    "# Does this improve accuracy?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Filter by gender, extract features for each subset,\n",
    "# and train separate models.\n",
    "#\n",
    "# male_df = combined_df[combined_df['gender'] == 'male']\n",
    "# female_df = combined_df[combined_df['gender'] == 'female']\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Data Collection**: How to download and organize speech emotion datasets from Kaggle\n",
    "2. **Data Exploration**: Using pandas to understand dataset structure and distribution\n",
    "3. **Audio Visualization**: Waveforms, spectrograms, and mel spectrograms with librosa\n",
    "4. **Data Augmentation**: Noise, time stretch, pitch shift, and time shift to create more training data\n",
    "5. **Feature Extraction**: Converting audio to numerical features (MFCCs, chroma, ZCR, RMS, spectral features)\n",
    "6. **Classification**: Training and comparing Random Forest, XGBoost, Logistic Regression, SVM, and KNN\n",
    "7. **Evaluation**: Confusion matrices, classification reports, precision, recall, F1-score\n",
    "8. **Dataset Combination**: Building a larger, more diverse dataset from multiple sources\n",
    "9. **Real-time Inference**: Using trained models to recognize emotions from live microphone input\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **More data helps**: Combining datasets improved performance across all models\n",
    "- **Feature engineering matters**: The quality of your features determines your model's ceiling\n",
    "- **Some emotions are harder**: Calm vs. neutral, happy vs. surprised are commonly confused\n",
    "- **Classical ML can be effective**: You don't always need deep learning!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Next week**, we'll move beyond classical ML and into modern deep learning approaches:\n",
    "\n",
    "- **Fine-tuning Whisper**: We'll take OpenAI's Whisper model - already trained on hundreds of thousands of hours of speech - and fine-tune it different applications. You'll see how pre-trained models can dramatically improve performance with relatively little effort.\n",
    "- **CNNs on Spectrograms**: Like we explored in the previous class with digit recognition, we can feed mel spectrograms directly into convolutional neural networks and let them learn their own features.\n",
    "- **Transfer Learning**: The key idea behind modern AI - start with a model that already understands speech, then teach it your specific task.\n",
    "- **Ethics**: Consider the implications of emotion recognition technology and the [EU AI Act](https://ai-act-law.eu/recital/18/)\n",
    "\n",
    "### References\n",
    "\n",
    "- [RAVDESS Dataset](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)\n",
    "- [TESS Dataset](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)\n",
    "- [SAVEE Dataset](https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee)\n",
    "- [CREMA-D Dataset](https://www.kaggle.com/datasets/ejlok1/cremad)\n",
    "- [librosa Documentation](https://librosa.org/doc/latest/)\n",
    "- [scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "- [Speech Emotion Recognition on Papers with Code](https://paperswithcode.com/task/speech-emotion-recognition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
