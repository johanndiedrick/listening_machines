{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Environmental Sound Classification with ESC-50\n",
    "\n",
    "In previous classes, we explored audio signal processing, built a digit recognizer, trained emotion classifiers, and used Whisper for speech recognition. All of these focused on **human speech**. But the world is full of non-speech sounds too!\n",
    "\n",
    "**Environmental Sound Classification (ESC)** is the task of automatically identifying sounds in our environment — a dog barking, rain falling, a siren wailing, or a keyboard clicking.\n",
    "\n",
    "### What you'll learn:\n",
    "\n",
    "1. **The ESC-50 dataset** — a benchmark collection of 2,000 environmental sounds across 50 categories\n",
    "2. **Data exploration** — listening to, visualizing, and understanding different types of sounds\n",
    "3. **Audio features** — extracting meaningful numbers from sound (MFCCs, spectral features)\n",
    "4. **Classical ML** — training Random Forest and SVM classifiers (fast, no GPU needed!)\n",
    "5. **Pretrained deep learning** — using an Audio Spectrogram Transformer already trained on ESC-50\n",
    "6. **Evaluation** — confusion matrices, per-class accuracy, and understanding model mistakes\n",
    "7. **Real-time classification** — identifying sounds from your microphone\n",
    "8. **Triggering actions** — using sound classification to control other systems (OSC, Serial, Arduino)\n",
    "\n",
    "### Why Environmental Sound Classification?\n",
    "\n",
    "| Application | Example |\n",
    "|-------------|--------|\n",
    "| **Smart Homes** | Detect a smoke alarm, glass breaking, or doorbell |\n",
    "| **Wildlife Monitoring** | Identify bird species, detect poachers |\n",
    "| **Urban Planning** | Map noise pollution, detect traffic patterns |\n",
    "| **Accessibility** | Alert deaf/hard-of-hearing people to important sounds |\n",
    "| **Creative Coding** | Trigger visuals or music based on environmental sounds |\n",
    "| **Security** | Detect gunshots, screams, or breaking glass |\n",
    "| **Healthcare** | Monitor coughing, snoring, or breathing patterns |\n",
    "\n",
    "### Where the Field Stands\n",
    "\n",
    "Accuracy on ESC-50 has climbed dramatically thanks to **transfer learning** — using models pretrained on large audio/image datasets:\n",
    "\n",
    "| Year | Approach | ESC-50 Accuracy | Key Idea |\n",
    "|------|----------|----------------|----------|\n",
    "| 2015 | Piczak CNN (from scratch) | ~64% | Train a CNN on mel spectrograms |\n",
    "| 2020 | PANNs (Kong et al.) | ~95% | Pretrain CNNs on AudioSet (2M clips), fine-tune |\n",
    "| 2021 | AST (Gong et al.) | ~96% | Apply a Vision Transformer to spectrograms |\n",
    "| 2022 | BEATs (Chen et al.) | ~98% | Self-supervised pretraining with audio tokenizers |\n",
    "| — | **Human listeners** | **~81%** | Reported in the original ESC-50 paper |\n",
    "\n",
    "The big insight: **treating mel spectrograms as images** and using image classification models (CNNs, Vision Transformers) pretrained on millions of examples works remarkably well for audio. In this notebook, you'll experience this progression firsthand!\n",
    "\n",
    "### Background Reading & Key Papers\n",
    "\n",
    "- [ESC-50 Paper](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf) — Piczak (2015), the original dataset paper\n",
    "- [PANNs](https://arxiv.org/abs/1912.10211) — Kong et al. (2020), systematic AudioSet pretraining\n",
    "- [Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) — Gong et al. (2021), Vision Transformer for audio\n",
    "- [BEATs](https://arxiv.org/abs/2212.09058) — Chen et al. (2022), self-supervised SOTA\n",
    "- [HuggingFace Audio Course (Ch. 4)](https://huggingface.co/learn/audio-course/chapter4/introduction) — Free tutorial on pretrained audio models\n",
    "- [ESC-50 GitHub](https://github.com/karolpiczak/ESC-50) — Official dataset repository\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "We need a couple of new libraries for this class.\n",
    "\n",
    "**New libraries:**\n",
    "\n",
    "| Library | What it does |\n",
    "|---------|-------------|\n",
    "| `transformers` | HuggingFace library for loading pretrained AI models |\n",
    "| `datasets` | HuggingFace library for downloading datasets |\n",
    "\n",
    "**Libraries from previous classes (should already be installed):**\n",
    "\n",
    "| Library | What it does |\n",
    "|---------|-------------|\n",
    "| `librosa` | Audio analysis and feature extraction |\n",
    "| `sounddevice` | Recording audio from your microphone |\n",
    "| `soundfile` | Reading and writing audio files |\n",
    "| `scikit-learn` | Machine learning models and evaluation tools |\n",
    "| `matplotlib` / `seaborn` | Charts and visualizations |\n",
    "| `pandas` | Working with tabular data |\n",
    "| `numpy` | Numerical computing |\n",
    "\n",
    "**Install new libraries:**\n",
    "\n",
    "```bash\n",
    "uv pip install transformers datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"Apple Silicon (MPS) detected\")\n",
    "else:\n",
    "    print(\"Using CPU (this is fine for everything in this notebook!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Part 2: The ESC-50 Dataset\n",
    "\n",
    "### What is ESC-50?\n",
    "\n",
    "ESC-50 is a labeled collection of **2,000 environmental audio recordings** designed for benchmarking sound classification methods. It was created by Karol Piczak in 2015 and is one of the most popular benchmarks in audio research.\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Total recordings | 2,000 |\n",
    "| Categories | 50 |\n",
    "| Clips per category | 40 |\n",
    "| Clip duration | 5 seconds |\n",
    "| Sample rate | 44,100 Hz |\n",
    "| Format | WAV (mono) |\n",
    "| Folds | 5 (for cross-validation) |\n",
    "\n",
    "The 50 categories are organized into **5 major groups**:\n",
    "\n",
    "1. **Animals** — dog, cat, rooster, crow, frog...\n",
    "2. **Natural soundscapes** — rain, sea waves, thunderstorm, wind...\n",
    "3. **Human (non-speech)** — crying baby, clapping, laughing, coughing...\n",
    "4. **Domestic/Interior** — clock tick, door knock, keyboard typing, vacuum cleaner...\n",
    "5. **Urban/Exterior** — helicopter, siren, car horn, train, chainsaw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ESC-50 dataset from HuggingFace\n",
    "# First time: downloads ~600MB of audio. After that: loads from cache instantly!\n",
    "\n",
    "print(\"Loading ESC-50 dataset from HuggingFace...\")\n",
    "print(\"(First time: downloads ~600MB. After that: loads from cache)\\n\")\n",
    "\n",
    "dataset = load_dataset(\"ashraq/esc50\")\n",
    "esc50 = dataset['train']\n",
    "\n",
    "print(f\"Dataset loaded! {len(esc50)} audio clips\")\n",
    "print(f\"Columns: {esc50.column_names}\")\n",
    "\n",
    "# Look at the first sample\n",
    "sample = esc50[0]\n",
    "print(f\"\\nFirst sample:\")\n",
    "for key, value in sample.items():\n",
    "    if key == 'audio':\n",
    "        print(f\"  audio: numpy array with {len(value['array'])} samples at {value['sampling_rate']} Hz\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame for easy exploration\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': [esc50[i]['filename'] for i in range(len(esc50))],\n",
    "    'fold': [esc50[i]['fold'] for i in range(len(esc50))],\n",
    "    'target': [esc50[i]['target'] for i in range(len(esc50))],\n",
    "    'category': [esc50[i]['category'] for i in range(len(esc50))],\n",
    "    'esc10': [esc50[i]['esc10'] for i in range(len(esc50))],\n",
    "})\n",
    "\n",
    "print(\"ESC-50 Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of categories: {df['category'].nunique()}\")\n",
    "print(f\"Samples per category: {len(df) // df['category'].nunique()}\")\n",
    "print(f\"Number of folds: {df['fold'].nunique()}\")\n",
    "print(f\"\\nFold distribution:\")\n",
    "print(df['fold'].value_counts().sort_index())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESC-50's 50 classes organized into 5 major categories\n",
    "\n",
    "MAJOR_CATEGORIES = {\n",
    "    'Animals': ['dog', 'rooster', 'pig', 'cow', 'frog', 'cat', 'hen', 'insects', 'sheep', 'crow'],\n",
    "    'Natural': ['rain', 'sea_waves', 'crackling_fire', 'crickets', 'chirping_birds',\n",
    "                'water_drops', 'wind', 'pouring_water', 'toilet_flush', 'thunderstorm'],\n",
    "    'Human': ['crying_baby', 'sneezing', 'clapping', 'breathing', 'coughing',\n",
    "              'footsteps', 'laughing', 'brushing_teeth', 'snoring', 'drinking_sipping'],\n",
    "    'Domestic': ['door_knock', 'mouse_click', 'keyboard_typing', 'door_wood_creaks',\n",
    "                 'can_opening', 'washing_machine', 'vacuum_cleaner', 'clock_alarm',\n",
    "                 'clock_tick', 'glass_breaking'],\n",
    "    'Urban': ['helicopter', 'chainsaw', 'siren', 'car_horn', 'engine',\n",
    "              'train', 'church_bells', 'airplane', 'fireworks', 'hand_saw'],\n",
    "}\n",
    "\n",
    "def get_major_category(category):\n",
    "    \"\"\"Return which major category a sound class belongs to.\"\"\"\n",
    "    for major, classes in MAJOR_CATEGORIES.items():\n",
    "        if category in classes:\n",
    "            return major\n",
    "    return 'Unknown'\n",
    "\n",
    "df['major_category'] = df['category'].apply(get_major_category)\n",
    "\n",
    "# Visualize the dataset\n",
    "COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "category_counts = df['major_category'].value_counts()\n",
    "axes[0].bar(category_counts.index, category_counts.values, color=COLORS)\n",
    "axes[0].set_title('Samples per Major Category')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "fold_counts = df['fold'].value_counts().sort_index()\n",
    "axes[1].bar(fold_counts.index, fold_counts.values, color='steelblue')\n",
    "axes[1].set_title('Samples per Fold (for Cross-Validation)')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_xlabel('Fold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show what's in each category\n",
    "print(\"\\nMajor category breakdown:\")\n",
    "for cat in MAJOR_CATEGORIES:\n",
    "    classes = MAJOR_CATEGORIES[cat]\n",
    "    print(f\"  {cat}: {', '.join(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### What are Folds?\n",
    "\n",
    "ESC-50 comes with **5 pre-defined folds** for cross-validation. Each fold contains 400 clips (8 per category).\n",
    "\n",
    "The folds are designed so that clips from the **same original source recording** are always in the same fold. This prevents **data leakage** — the same issue we discussed in Class 2!\n",
    "\n",
    "For evaluation, we'll use **folds 1–4 for training** and **fold 5 for testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Part 3: Listening to Environmental Sounds\n",
    "\n",
    "The most important step in any ML project: **understand your data before building models**.\n",
    "\n",
    "Let's listen to examples from each major category and visualize what they look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to one example from each major category\n",
    "\n",
    "print(\"One example from each major category:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for major_cat, classes in MAJOR_CATEGORIES.items():\n",
    "    example_class = classes[0]\n",
    "\n",
    "    for i in range(len(esc50)):\n",
    "        if esc50[i]['category'] == example_class:\n",
    "            sample = esc50[i]\n",
    "            break\n",
    "\n",
    "    audio = sample['audio']['array']\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "\n",
    "    print(f\"\\n{major_cat} — {example_class}\")\n",
    "    display(Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare waveforms of 5 different sounds — one from each major category\n",
    "# Notice how different types of sounds have very different shapes!\n",
    "\n",
    "example_sounds = ['dog', 'rain', 'clapping', 'keyboard_typing', 'siren']\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "for idx, sound_name in enumerate(example_sounds):\n",
    "    for i in range(len(esc50)):\n",
    "        if esc50[i]['category'] == sound_name:\n",
    "            sample = esc50[i]\n",
    "            break\n",
    "\n",
    "    audio = sample['audio']['array']\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "    duration = len(audio) / sr\n",
    "    time = np.linspace(0, duration, len(audio))\n",
    "\n",
    "    axes[idx].plot(time, audio, color=COLORS[idx], alpha=0.8, linewidth=0.5)\n",
    "    axes[idx].set_ylabel('Amplitude')\n",
    "    axes[idx].set_title(f'{sound_name} ({get_major_category(sound_name)})')\n",
    "    axes[idx].set_xlim(0, duration)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)')\n",
    "plt.suptitle('Comparing Waveforms of Different Environmental Sounds', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the different patterns:\")\n",
    "print(\"  - Dog bark: short, loud bursts\")\n",
    "print(\"  - Rain: continuous random noise\")\n",
    "print(\"  - Clapping: sharp periodic impacts\")\n",
    "print(\"  - Keyboard: rapid quiet clicks\")\n",
    "print(\"  - Siren: smooth periodic oscillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mel spectrograms of different sounds\n",
    "# Spectrograms show frequency content over time — this is what ML models \"see\"!\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 14))\n",
    "\n",
    "for idx, sound_name in enumerate(example_sounds):\n",
    "    for i in range(len(esc50)):\n",
    "        if esc50[i]['category'] == sound_name:\n",
    "            sample = esc50[i]\n",
    "            break\n",
    "\n",
    "    audio = sample['audio']['array'].astype(np.float32)\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    librosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{sound_name} ({get_major_category(sound_name)})')\n",
    "    axes[idx].label_outer()\n",
    "\n",
    "plt.suptitle('Mel Spectrograms of Different Environmental Sounds', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each sound has a unique 'fingerprint' in its spectrogram:\")\n",
    "print(\"  - Dog bark: bright horizontal bands (harmonics) in short bursts\")\n",
    "print(\"  - Rain: uniform energy spread across all frequencies\")\n",
    "print(\"  - Clapping: vertical lines (broadband impulses)\")\n",
    "print(\"  - Keyboard: faint, quick vertical marks\")\n",
    "print(\"  - Siren: rising and falling frequency sweep\")\n",
    "print(\"\\nThese visual patterns are exactly what neural networks learn to recognize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to multiple examples of the SAME category\n",
    "# Even within one class, there's a lot of variation!\n",
    "\n",
    "explore_category = 'dog'  # Change this to explore other categories!\n",
    "\n",
    "print(f\"Four examples of '{explore_category}':\")\n",
    "print(\"Notice how much variation there is within a single category!\\n\")\n",
    "\n",
    "count = 0\n",
    "for i in range(len(esc50)):\n",
    "    if esc50[i]['category'] == explore_category and count < 4:\n",
    "        sample = esc50[i]\n",
    "        audio = sample['audio']['array']\n",
    "        sr = sample['audio']['sampling_rate']\n",
    "        print(f\"Example {count + 1} ({sample['filename']}):\")\n",
    "        display(Audio(audio, rate=sr))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Part 4: Feature Extraction\n",
    "\n",
    "Before we can train a machine learning model, we need to convert audio into **numbers** that the model can work with. This is called **feature extraction**.\n",
    "\n",
    "### What are Audio Features?\n",
    "\n",
    "Think of features as a compact summary of what makes a sound unique. Instead of feeding the model millions of raw audio samples, we extract a description.\n",
    "\n",
    "| Feature | What it captures | Example |\n",
    "|---------|-----------------|--------|\n",
    "| **MFCCs** | Tonal quality (timbre) | Distinguishes a dog bark from a cat meow |\n",
    "| **Chroma** | Musical pitch content | Tells apart church bells from a siren |\n",
    "| **Spectral Centroid** | \"Brightness\" of the sound | A whistle is bright, thunder is dark |\n",
    "| **Spectral Bandwidth** | How spread out the frequencies are | White noise is wide, a tone is narrow |\n",
    "| **Spectral Rolloff** | Where most of the energy is | High for hissing, low for rumbling |\n",
    "| **Zero Crossing Rate** | How often the signal crosses zero | High for noisy sounds, low for tonal |\n",
    "| **RMS Energy** | Loudness over time | Loud vs. quiet sounds |\n",
    "\n",
    "**MFCCs (Mel-Frequency Cepstral Coefficients)** are the most important features for sound classification. They capture the \"shape\" of the sound's frequency spectrum in a way similar to how humans hear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a single audio sample to see what they look like\n",
    "\n",
    "sample = esc50[0]\n",
    "audio = sample['audio']['array'].astype(np.float32)\n",
    "sr = sample['audio']['sampling_rate']\n",
    "\n",
    "print(f\"Sample: {sample['category']} ({sample['filename']})\")\n",
    "print(f\"Audio shape: {audio.shape}, Sample rate: {sr} Hz\")\n",
    "print(f\"Duration: {len(audio)/sr:.1f} seconds\\n\")\n",
    "\n",
    "# Extract various features\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)\n",
    "rms = librosa.feature.rms(y=audio)\n",
    "\n",
    "print(\"Feature shapes (values for each time frame):\")\n",
    "print(f\"  MFCCs:              {mfccs.shape} (13 coefficients x {mfccs.shape[1]} frames)\")\n",
    "print(f\"  Chroma:             {chroma.shape} (12 pitch classes x {chroma.shape[1]} frames)\")\n",
    "print(f\"  Spectral Centroid:  {spectral_centroid.shape}\")\n",
    "print(f\"  Spectral Bandwidth: {spectral_bandwidth.shape}\")\n",
    "print(f\"  Spectral Rolloff:   {spectral_rolloff.shape}\")\n",
    "print(f\"  Zero Crossing Rate: {zero_crossing_rate.shape}\")\n",
    "print(f\"  RMS Energy:         {rms.shape}\")\n",
    "\n",
    "# Visualize MFCCs\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "img = librosa.display.specshow(mfccs, x_axis='time', sr=sr, ax=axes[0])\n",
    "axes[0].set_title(f'MFCCs for \"{sample[\"category\"]}\"')\n",
    "axes[0].set_ylabel('MFCC Coefficient')\n",
    "plt.colorbar(img, ax=axes[0])\n",
    "\n",
    "S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr, ax=axes[1])\n",
    "times = librosa.times_like(spectral_centroid)\n",
    "axes[1].plot(times, spectral_centroid[0], color='white', linewidth=2, label='Spectral Centroid')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Spectral Centroid (white line) on Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from ALL 2,000 audio clips\n",
    "# We summarize each feature over time (mean + standard deviation) to get\n",
    "# a fixed-length vector per clip. This is the input for our ML classifiers.\n",
    "\n",
    "def extract_features(audio, sr):\n",
    "    \"\"\"Extract audio features and return a fixed-length feature vector.\"\"\"\n",
    "    features = []\n",
    "\n",
    "    # MFCCs (13 coefficients x 2 stats = 26 features)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    features.extend(np.mean(mfccs, axis=1))\n",
    "    features.extend(np.std(mfccs, axis=1))\n",
    "\n",
    "    # Chroma (12 pitch classes x 2 stats = 24 features)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    features.extend(np.mean(chroma, axis=1))\n",
    "    features.extend(np.std(chroma, axis=1))\n",
    "\n",
    "    # Spectral Centroid (2 features)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    features.extend([np.mean(spectral_centroid), np.std(spectral_centroid)])\n",
    "\n",
    "    # Spectral Bandwidth (2 features)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    features.extend([np.mean(spectral_bandwidth), np.std(spectral_bandwidth)])\n",
    "\n",
    "    # Spectral Rolloff (2 features)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    features.extend([np.mean(spectral_rolloff), np.std(spectral_rolloff)])\n",
    "\n",
    "    # Zero Crossing Rate (2 features)\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)\n",
    "    features.extend([np.mean(zero_crossing_rate), np.std(zero_crossing_rate)])\n",
    "\n",
    "    # RMS Energy (2 features)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Extract features from all samples\n",
    "print(f\"Extracting features from all {len(esc50)} samples...\")\n",
    "print(\"This takes 2-5 minutes depending on your machine.\\n\")\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_folds = []\n",
    "all_categories = []\n",
    "\n",
    "for i in tqdm(range(len(esc50))):\n",
    "    sample = esc50[i]\n",
    "    audio = sample['audio']['array'].astype(np.float32)\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "\n",
    "    try:\n",
    "        features = extract_features(audio, sr)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(sample['target'])\n",
    "        all_folds.append(sample['fold'])\n",
    "        all_categories.append(sample['category'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error on sample {i} ({sample['filename']}): {e}\")\n",
    "\n",
    "X = np.array(all_features)\n",
    "y = np.array(all_labels)\n",
    "folds = np.array(all_folds)\n",
    "categories = np.array(all_categories)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape} (samples x features)\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Total features per sample: {X.shape[1]}\")\n",
    "print(\"Done! Ready for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Part 5: Classical ML Classification\n",
    "\n",
    "Now that we have features extracted from every audio clip, we can train machine learning classifiers! These are similar to the models you may have used in Class 3.\n",
    "\n",
    "### Train/Test Split\n",
    "\n",
    "We'll use ESC-50's built-in folds: **folds 1–4 for training** (1,600 samples) and **fold 5 for testing** (400 samples).\n",
    "\n",
    "### Models We'll Train\n",
    "\n",
    "| Model | How it works | Strengths |\n",
    "|-------|-----------|-----------|\n",
    "| **Random Forest** | Ensemble of decision trees that vote on the answer | Fast, hard to overfit, handles many features well |\n",
    "| **SVM** | Finds optimal boundaries between classes | Great for high-dimensional data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using ESC-50's built-in folds\n",
    "\n",
    "train_mask = folds != 5\n",
    "test_mask = folds == 5\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "categories_test = categories[test_mask]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples:     {len(X_test)}\")\n",
    "\n",
    "# Normalize features (important for SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "rf_accuracy = rf.score(X_test_scaled, y_test)\n",
    "print(f\"  Random Forest accuracy: {rf_accuracy:.1%}\")\n",
    "\n",
    "# Train SVM\n",
    "print(\"\\nTraining SVM...\")\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "svm_accuracy = svm.score(X_test_scaled, y_test)\n",
    "print(f\"  SVM accuracy: {svm_accuracy:.1%}\")\n",
    "\n",
    "print(f\"\\nBest classical ML model: {'Random Forest' if rf_accuracy > svm_accuracy else 'SVM'}\")\n",
    "print(f\"(For reference: reported human accuracy on ESC-50 is about 81%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation: confusion matrix and classification report\n",
    "\n",
    "best_model = rf if rf_accuracy >= svm_accuracy else svm\n",
    "best_name = \"Random Forest\" if rf_accuracy >= svm_accuracy else \"SVM\"\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "unique_categories = sorted(set(all_categories))\n",
    "\n",
    "print(f\"Classification Report ({best_name}):\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred, target_names=unique_categories, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=unique_categories,\n",
    "            yticklabels=unique_categories)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix — {best_name} (Accuracy: {accuracy_score(y_test, y_pred):.1%})')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which sounds are easiest and hardest to classify?\n",
    "\n",
    "per_class_acc = {}\n",
    "for i, cat in enumerate(unique_categories):\n",
    "    mask = y_test == i\n",
    "    if mask.sum() > 0:\n",
    "        per_class_acc[cat] = (y_pred[mask] == y_test[mask]).mean()\n",
    "\n",
    "sorted_acc = sorted(per_class_acc.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "cats = [x[0] for x in sorted_acc]\n",
    "accs = [x[1] for x in sorted_acc]\n",
    "bar_colors = ['#2ecc71' if a >= 0.7 else '#f39c12' if a >= 0.4 else '#e74c3c' for a in accs]\n",
    "\n",
    "ax.barh(range(len(cats)), accs, color=bar_colors)\n",
    "ax.set_yticks(range(len(cats)))\n",
    "ax.set_yticklabels(cats, fontsize=8)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title(f'Per-Class Accuracy — {best_name}')\n",
    "ax.axvline(x=accuracy_score(y_test, y_pred), color='black', linestyle='--',\n",
    "           label=f'Overall: {accuracy_score(y_test, y_pred):.1%}')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top confusions\n",
    "print(\"Top 10 Most Confused Sound Pairs:\")\n",
    "print(\"=\" * 55)\n",
    "confusions = []\n",
    "for i in range(len(unique_categories)):\n",
    "    for j in range(len(unique_categories)):\n",
    "        if i != j and cm[i][j] > 0:\n",
    "            confusions.append((unique_categories[i], unique_categories[j], cm[i][j]))\n",
    "\n",
    "confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "for true_cat, pred_cat, count in confusions[:10]:\n",
    "    print(f\"  {true_cat:20s} mistaken for {pred_cat:20s} ({count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "The confusion matrix and per-class accuracy reveal interesting patterns:\n",
    "\n",
    "- **Easy sounds** tend to be very distinctive (e.g., helicopter, clock tick, siren)\n",
    "- **Hard sounds** often share acoustic properties:\n",
    "  - Rain vs. water sounds (both are broadband noise)\n",
    "  - Insects vs. crickets (similar continuous high-frequency sounds)\n",
    "  - Engine vs. train (both are low-frequency rumbling)\n",
    "\n",
    "**Key takeaway:** The features we extracted (MFCCs, spectral features) capture a lot of useful information, but they're just summaries. A neural network looking at the **full spectrogram** can catch patterns that our handcrafted features miss.\n",
    "\n",
    "Let's see how a pretrained deep learning model compares..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: CNN on Spectrograms — The \"Audio as Image\" Paradigm\n",
    "\n",
    "The single most important insight in modern audio classification:\n",
    "\n",
    "> **A mel spectrogram is a 2D image.** Any image classification model can classify it.\n",
    "\n",
    "This means we can take a neural network that was trained on millions of **photographs** (ImageNet) and use it to classify **sounds**! This is called **transfer learning** — the features the network learned for recognizing objects in photos (edges, textures, repeating patterns) turn out to be useful for recognizing patterns in spectrograms too.\n",
    "\n",
    "### The Plan\n",
    "\n",
    "1. Convert each ESC-50 audio clip to a mel spectrogram \"image\"\n",
    "2. Load a **ResNet-18** pretrained on ImageNet (1.2 million photos, 1,000 object classes)\n",
    "3. Replace its final layer to output 50 ESC-50 classes instead of 1,000 ImageNet classes\n",
    "4. **Fine-tune** it on our spectrogram images\n",
    "\n",
    "### What is ResNet?\n",
    "\n",
    "**ResNet** (Residual Network) is one of the most influential neural network architectures in deep learning. It uses \"skip connections\" that let information flow more easily through the network, which solved the problem of training very deep networks.\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Conv layers 1–17 | Extract visual features (edges → textures → patterns) |\n",
    "| Final FC layer | Classify based on extracted features (we replace this!) |\n",
    "| Skip connections | Help gradients flow during training |\n",
    "\n",
    "### Training Time\n",
    "\n",
    "| Device | Approximate Time |\n",
    "|--------|-----------------|\n",
    "| CPU (Intel) | ~20-40 minutes |\n",
    "| Apple Silicon (MPS) | ~5-15 minutes |\n",
    "| GPU (CUDA) | ~2-5 minutes |\n",
    "\n",
    "We'll freeze the early layers (which already know how to extract useful features) and only train the later layers. This speeds up training significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a PyTorch Dataset that converts audio clips to spectrogram \"images\"\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tv_transforms\n",
    "\n",
    "class SpectrogramDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Convert ESC-50 audio clips into mel spectrogram images for a CNN.\"\"\"\n",
    "\n",
    "    def __init__(self, indices, esc50_dataset):\n",
    "        self.indices = indices\n",
    "        self.esc50 = esc50_dataset\n",
    "        self.resize = tv_transforms.Resize((224, 224), antialias=True)\n",
    "        # ImageNet normalization — ResNet was trained with these statistics\n",
    "        self.normalize = tv_transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.esc50[self.indices[idx]]\n",
    "        audio = sample['audio']['array'].astype(np.float32)\n",
    "        sr = sample['audio']['sampling_rate']\n",
    "\n",
    "        # Create mel spectrogram (same transform as our visualizations above!)\n",
    "        S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # Normalize to 0-1 range\n",
    "        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min() + 1e-8)\n",
    "\n",
    "        # ResNet expects 3-channel (RGB) input — repeat the spectrogram 3 times\n",
    "        S_3ch = np.stack([S_db, S_db, S_db], axis=0)\n",
    "        tensor = torch.tensor(S_3ch, dtype=torch.float32)\n",
    "\n",
    "        # Resize to 224x224 (ResNet's expected input size) and normalize\n",
    "        tensor = self.resize(tensor)\n",
    "        tensor = self.normalize(tensor)\n",
    "\n",
    "        return tensor, sample['target']\n",
    "\n",
    "\n",
    "# Split by fold (same split as our classical ML evaluation)\n",
    "train_indices_cnn = [i for i in range(len(esc50)) if esc50[i]['fold'] != 5]\n",
    "test_indices_cnn = [i for i in range(len(esc50)) if esc50[i]['fold'] == 5]\n",
    "\n",
    "train_spec_dataset = SpectrogramDataset(train_indices_cnn, esc50)\n",
    "test_spec_dataset = SpectrogramDataset(test_indices_cnn, esc50)\n",
    "\n",
    "train_spec_loader = torch.utils.data.DataLoader(train_spec_dataset, batch_size=32, shuffle=True)\n",
    "test_spec_loader = torch.utils.data.DataLoader(test_spec_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Peek at a batch\n",
    "batch_x, batch_y = next(iter(train_spec_loader))\n",
    "print(f\"Batch shape: {batch_x.shape}  (batch_size, channels, height, width)\")\n",
    "print(f\"Labels shape: {batch_y.shape}\")\n",
    "print(f\"Training batches per epoch: {len(train_spec_loader)}\")\n",
    "print(f\"Test batches: {len(test_spec_loader)}\")\n",
    "\n",
    "# Visualize what the CNN will \"see\"\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "for i in range(4):\n",
    "    img = batch_x[i][0].numpy()\n",
    "    axes[i].imshow(img, aspect='auto', origin='lower', cmap='viridis')\n",
    "    cat_name = unique_categories[batch_y[i].item()]\n",
    "    axes[i].set_title(cat_name, fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Mel Spectrograms as 224x224 Images (what ResNet sees)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load pretrained ResNet-18 and fine-tune on ESC-50 spectrograms\n",
    "\n",
    "# Use the best available device\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "print(f\"Training on: {device}\")\n",
    "if str(device) == 'cpu':\n",
    "    print(\"(CPU training takes ~20-40 min. Apple Silicon MPS or CUDA would be faster.)\\n\")\n",
    "\n",
    "# Load ResNet-18 pretrained on ImageNet (1.2 million photographs!)\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Replace the final layer: ImageNet has 1000 classes, ESC-50 has 50\n",
    "resnet.fc = torch.nn.Linear(resnet.fc.in_features, 50)\n",
    "\n",
    "# Freeze early layers — they already extract useful visual features.\n",
    "# We only fine-tune layer4 (the deepest conv block) + our new classification head.\n",
    "for name, param in resnet.named_parameters():\n",
    "    if 'layer4' not in name and 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in resnet.parameters())\n",
    "print(f\"Total parameters:     {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,} ({trainable/total:.0%})\")\n",
    "print(f\"Frozen parameters:    {total - trainable:,} ({(total - trainable)/total:.0%})\")\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-3\n",
    ")\n",
    "\n",
    "# Train!\n",
    "EPOCHS = 10\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(f\"\\nTraining for {EPOCHS} epochs...\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    resnet.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(train_spec_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = resnet(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += predicted.eq(batch_y).sum().item()\n",
    "\n",
    "    train_acc = correct / total\n",
    "    avg_loss = total_loss / len(train_spec_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate on test fold\n",
    "    resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_spec_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            output = resnet(batch_x)\n",
    "            _, predicted = output.max(1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += predicted.eq(batch_y).sum().item()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"  Loss: {avg_loss:.4f} | Train: {train_acc:.1%} | Test: {test_acc:.1%}\")\n",
    "\n",
    "resnet_accuracy = max(test_accuracies)\n",
    "print(\"=\" * 65)\n",
    "print(f\"Best test accuracy: {resnet_accuracy:.1%} (epoch {test_accuracies.index(resnet_accuracy)+1})\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(range(1, EPOCHS + 1), train_losses, marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, EPOCHS + 1), test_accuracies, marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Test Accuracy (Fold 5)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClassical ML ({best_name}): {accuracy_score(y_test, y_pred):.1%}\")\n",
    "print(f\"ResNet-18 (fine-tuned):    {resnet_accuracy:.1%}\")\n",
    "print(\"(Pretrained AST will be evaluated next!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### What Did Transfer Learning Actually Transfer?\n",
    "\n",
    "When we used a ResNet pretrained on ImageNet (photographs of dogs, cars, furniture...), it already knew how to detect visual features that turn out to be useful for spectrograms too:\n",
    "\n",
    "| ImageNet Feature | Spectrogram Equivalent |\n",
    "|-----------------|----------------------|\n",
    "| Edges, lines | Harmonic frequency bands |\n",
    "| Textures | Noise patterns (rain, static) |\n",
    "| Repeating patterns | Rhythmic sounds (clock tick, engine) |\n",
    "| Shape boundaries | Onset/offset of sound events |\n",
    "\n",
    "The **early layers** of the network (which we froze) detect these low-level features. The **later layers** (which we fine-tuned) learn to combine them into sound-specific patterns.\n",
    "\n",
    "**This is the core insight of transfer learning:** features learned for one task (image recognition) can be useful for a completely different task (sound classification) when the input representations share structural similarities — both spectrograms and photos are 2D patterns with local structure!\n",
    "\n",
    "Now let's see how a model specifically pretrained on **audio data** compares..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Pretrained Deep Learning Model\n",
    "\n",
    "Our ResNet got a nice boost over classical ML, but it was pretrained on *images* — not audio. What if we use a model pretrained on **millions of audio clips**?\n",
    "\n",
    "Instead of training from scratch, we can download a model that someone has **already trained** on ESC-50. Same idea as using Whisper for speech recognition in Class 4!\n",
    "\n",
    "### The Audio Spectrogram Transformer (AST)\n",
    "\n",
    "The model we'll use is the **Audio Spectrogram Transformer (AST)**, developed by MIT researchers.\n",
    "\n",
    "```\n",
    "Audio Waveform → Mel Spectrogram → Split into Patches → Transformer Encoder → Classification\n",
    "```\n",
    "\n",
    "1. **Mel Spectrogram**: Convert audio to a 2D image of frequency vs. time (just like we did above!)\n",
    "2. **Patch Embedding**: Split the spectrogram into small patches (like puzzle pieces)\n",
    "3. **Transformer Encoder**: Process all patches with self-attention (the same architecture behind ChatGPT and Whisper!)\n",
    "4. **Classification Head**: Output probabilities for each of the 50 ESC-50 categories\n",
    "\n",
    "### Why Use a Pretrained Model?\n",
    "\n",
    "| Approach | Accuracy | Training Time | GPU Needed? |\n",
    "|----------|----------|--------------|-------------|\n",
    "| Classical ML (our features + Random Forest) | ~50-65% | Seconds | No |\n",
    "| ResNet-18 on spectrograms (Part 6) | ~75-90% | 10-30 min | No (CPU ok) |\n",
    "| Train a CNN from scratch | ~70-80% | Hours | Recommended |\n",
    "| **Pretrained AST** | **~85-95%** | **None — already trained!** | **No (CPU is fine)** |\n",
    "\n",
    "The AST was first trained on **AudioSet** (2 million audio clips, 527 categories) and then fine-tuned on ESC-50. It has already learned rich audio representations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained Audio Spectrogram Transformer (AST) fine-tuned on ESC-50\n",
    "# First time: downloads ~350MB. After that: loads from cache.\n",
    "\n",
    "print(\"Loading pretrained AST model fine-tuned on ESC-50...\")\n",
    "print(\"(First time: downloads ~350MB. After that: loads from cache)\\n\")\n",
    "\n",
    "ast_classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"bioamla/ast-esc50\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: Audio Spectrogram Transformer (AST)\")\n",
    "print(f\"Pre-trained on: AudioSet (2M clips) then fine-tuned on ESC-50\")\n",
    "print(f\"Number of classes: 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify individual samples and see the model's predictions!\n",
    "# The model returns the top predicted categories with confidence scores.\n",
    "\n",
    "example_indices = [0, 50, 100, 200, 400, 600, 800, 1000, 1200, 1500]\n",
    "temp_path = \"_temp_ast_sample.wav\"\n",
    "\n",
    "print(\"Classifying examples with the pretrained AST:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in example_indices:\n",
    "    sample = esc50[idx]\n",
    "    audio = sample['audio']['array'].astype(np.float32)\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "    true_label = sample['category']\n",
    "\n",
    "    sf.write(temp_path, audio, sr)\n",
    "    predictions = ast_classifier(temp_path, top_k=3)\n",
    "    top_pred = predictions[0]['label']\n",
    "    top_score = predictions[0]['score']\n",
    "\n",
    "    correct = \"correct\" if top_pred == true_label else \"WRONG\"\n",
    "    print(f\"\\n  True: {true_label:20s} | Pred: {top_pred:20s} ({top_score:.1%}) [{correct}]\")\n",
    "    for p in predictions[1:]:\n",
    "        print(f\"{'':26s}  also: {p['label']:20s} ({p['score']:.1%})\")\n",
    "\n",
    "if os.path.exists(temp_path):\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the pretrained AST on fold 5 (same test set as classical ML)\n",
    "#\n",
    "# Set MAX_EVAL_SAMPLES to a smaller number for a quick test,\n",
    "# or set it to None to evaluate on all 400 test samples (takes ~10-15 min on CPU).\n",
    "\n",
    "MAX_EVAL_SAMPLES = 100  # Set to None for full evaluation\n",
    "\n",
    "test_indices = [i for i in range(len(esc50)) if esc50[i]['fold'] == 5]\n",
    "if MAX_EVAL_SAMPLES is not None:\n",
    "    test_indices = test_indices[:MAX_EVAL_SAMPLES]\n",
    "\n",
    "print(f\"Evaluating pretrained AST on {len(test_indices)} test samples...\")\n",
    "print(\"(This takes ~1-3 seconds per sample on CPU)\\n\")\n",
    "\n",
    "ast_predictions = []\n",
    "ast_true_labels = []\n",
    "temp_path = \"_temp_ast_eval.wav\"\n",
    "\n",
    "for idx in tqdm(test_indices):\n",
    "    sample = esc50[idx]\n",
    "    audio = sample['audio']['array'].astype(np.float32)\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "\n",
    "    sf.write(temp_path, audio, sr)\n",
    "    predictions = ast_classifier(temp_path, top_k=1)\n",
    "    ast_predictions.append(predictions[0]['label'])\n",
    "    ast_true_labels.append(sample['category'])\n",
    "\n",
    "if os.path.exists(temp_path):\n",
    "    os.remove(temp_path)\n",
    "\n",
    "ast_correct = sum(1 for p, t in zip(ast_predictions, ast_true_labels) if p == t)\n",
    "ast_accuracy = ast_correct / len(ast_true_labels)\n",
    "\n",
    "print(f\"\\nPretrained AST Accuracy: {ast_accuracy:.1%}\")\n",
    "print(f\"Classical ML ({best_name}) Accuracy: {accuracy_score(y_test, y_pred):.1%}\")\n",
    "print(f\"\\nThe pretrained model is {(ast_accuracy - accuracy_score(y_test, y_pred))*100:+.1f} percentage points better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to some examples the AST got wrong (if any)\n",
    "\n",
    "misclassified = [(p, t, i) for p, t, i in zip(ast_predictions, ast_true_labels, test_indices) if p != t]\n",
    "\n",
    "if not misclassified:\n",
    "    print(\"The AST classified every sample correctly! Try evaluating more samples.\")\n",
    "else:\n",
    "    print(f\"The AST got {len(misclassified)} samples wrong. Let's listen to some:\\n\")\n",
    "    for pred, true, idx in misclassified[:5]:\n",
    "        sample = esc50[idx]\n",
    "        audio = sample['audio']['array']\n",
    "        sr = sample['audio']['sampling_rate']\n",
    "        print(f\"True: {true:20s} | Predicted: {pred}\")\n",
    "        display(Audio(audio, rate=sr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Comparing All Four Approaches\n",
    "\n",
    "| Approach | Accuracy | Inference Speed | Training Required? | GPU Needed? |\n",
    "|----------|----------|----------------|-------------------|-------------|\n",
    "| Random Forest + librosa features | ~50-65% | Instant | Yes (seconds) | No |\n",
    "| SVM + librosa features | ~50-65% | Instant | Yes (seconds) | No |\n",
    "| ResNet-18 on spectrograms | ~75-90% | ~0.5 sec/clip | Yes (10-30 min) | No (CPU ok) |\n",
    "| Pretrained AST | ~85-95% | ~1-3 sec/clip | No! | No |\n",
    "| Fine-tune AST from scratch | ~95%+ | ~1-3 sec/clip | Yes (hours) | Yes |\n",
    "\n",
    "**Key takeaways:**\n",
    "- Each step up in complexity brings a jump in accuracy — this is the arc of the field!\n",
    "- **Classical ML** is fast, interpretable, and teaches you the fundamentals\n",
    "- **ResNet on spectrograms** shows the power of the \"audio as image\" insight and transfer learning from vision\n",
    "- **Pretrained AST** is the most accurate with zero training — it was pretrained on 2 million audio clips\n",
    "- For real-time applications, classical ML has the fastest inference; the AST has the best accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Part 8: Real-Time Environmental Sound Classification\n",
    "\n",
    "Now for the fun part — using the model to classify sounds from your microphone in real-time!\n",
    "\n",
    "### Sounds You Can Make in the Classroom\n",
    "\n",
    "Try these sounds that are in the ESC-50 dataset:\n",
    "- **Clap your hands** → \"clapping\"\n",
    "- **Type on your keyboard** → \"keyboard_typing\"\n",
    "- **Click your mouse** → \"mouse_click\"\n",
    "- **Knock on your desk** → \"door_knock\"\n",
    "- **Cough** → \"coughing\"\n",
    "- **Snap your fingers** → see what it classifies as!\n",
    "\n",
    "### Two Approaches\n",
    "\n",
    "| Approach | How it works | Best for |\n",
    "|----------|-------------|----------|\n",
    "| **Enter-to-Record** | Press Enter, make a sound, get prediction | Simple and reliable |\n",
    "| **Continuous Listening** | Auto-classifies every 5 seconds | Hands-free, more interactive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENTER-TO-RECORD: Record a sound and classify it!\n",
    "# =============================================================================\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "DURATION = 5  # ESC-50 clips are 5 seconds\n",
    "TEMP_FILE = \"_temp_realtime.wav\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ENTER-TO-RECORD SOUND CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Instructions:\")\n",
    "print(f\"  1. Press Enter when ready to record\")\n",
    "print(f\"  2. Make a sound! (clap, knock, type, cough, etc.)\")\n",
    "print(f\"  3. Recording lasts {DURATION} seconds\")\n",
    "print(f\"  4. See the top 5 predictions!\")\n",
    "print(f\"  5. Type 'q' to quit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nPress Enter to record (or 'q' to quit): \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        print(f\"Recording for {DURATION} seconds... Make a sound!\")\n",
    "        recording = sd.rec(\n",
    "            int(DURATION * SAMPLE_RATE),\n",
    "            samplerate=SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "        print(\"Processing...\\n\")\n",
    "\n",
    "        sf.write(TEMP_FILE, recording, SAMPLE_RATE)\n",
    "\n",
    "        predictions = ast_classifier(TEMP_FILE, top_k=5)\n",
    "\n",
    "        print(\"  Top 5 Predictions:\")\n",
    "        print(\"  \" + \"-\" * 45)\n",
    "        for i, pred in enumerate(predictions):\n",
    "            bar = \"*\" * int(pred['score'] * 30)\n",
    "            print(f\"  {i+1}. {pred['label']:25s} {pred['score']:6.1%} {bar}\")\n",
    "\n",
    "        display(Audio(TEMP_FILE))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        os.remove(TEMP_FILE)\n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONTINUOUS LISTENING: Classify sounds automatically every few seconds\n",
    "# =============================================================================\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "CHUNK_DURATION = 5  # Match ESC-50 clip length\n",
    "TEMP_FILE = \"_temp_continuous.wav\"\n",
    "\n",
    "chunk_samples = int(CHUNK_DURATION * SAMPLE_RATE)\n",
    "audio_buffer = []\n",
    "is_collecting = True\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Collect audio into a buffer.\"\"\"\n",
    "    global audio_buffer\n",
    "    if is_collecting:\n",
    "        audio_buffer.extend(indata[:, 0].tolist())\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONTINUOUS SOUND CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Classifying every {CHUNK_DURATION} seconds...\")\n",
    "print(\"Make different sounds and watch the predictions change!\")\n",
    "print(\"\\nPress the STOP button or Kernel > Interrupt to stop\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        channels=1,\n",
    "        callback=audio_callback,\n",
    "        blocksize=1024,\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    while True:\n",
    "        sd.sleep(int(CHUNK_DURATION * 1000))\n",
    "\n",
    "        if len(audio_buffer) >= chunk_samples:\n",
    "            chunk = np.array(audio_buffer[-chunk_samples:], dtype=np.float32)\n",
    "            audio_buffer = []\n",
    "\n",
    "            sf.write(TEMP_FILE, chunk, SAMPLE_RATE)\n",
    "            predictions = ast_classifier(TEMP_FILE, top_k=5)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"=\" * 50)\n",
    "            print(\"CONTINUOUS SOUND CLASSIFICATION\")\n",
    "            print(\"=\" * 50)\n",
    "            rms = np.sqrt(np.mean(chunk**2))\n",
    "            print(f\"Volume: {'*' * int(rms * 100)}\")\n",
    "            print(f\"\\nTop 5 Predictions:\")\n",
    "            for i, pred in enumerate(predictions):\n",
    "                bar = \"*\" * int(pred['score'] * 30)\n",
    "                print(f\"  {i+1}. {pred['label']:25s} {pred['score']:6.1%} {bar}\")\n",
    "            print(f\"\\nListening... (make a sound!)\")\n",
    "            print(\"Press STOP or Kernel > Interrupt to stop\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    is_collecting = False\n",
    "    if stream is not None:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "    audio_buffer = []\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        os.remove(TEMP_FILE)\n",
    "    print(\"\\nStopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Triggering Actions with Sound Classification\n",
    "\n",
    "Just like in previous classes, you can use the model's predictions to **trigger actions** in other programs!\n",
    "\n",
    "The pattern is always the same:\n",
    "1. **Classify** a sound from the microphone\n",
    "2. **Check** what was detected\n",
    "3. **Send** a message to another program (OSC to p5.js, Serial to Arduino, etc.)\n",
    "\n",
    "### Project Ideas\n",
    "\n",
    "| Sound | Action |\n",
    "|-------|--------|\n",
    "| Clapping | Toggle a light, change scene in p5.js |\n",
    "| Dog barking | Send an alert, play a calming sound |\n",
    "| Keyboard typing | Visualize typing rhythm |\n",
    "| Door knock | Trigger a door animation |\n",
    "| Siren | Flash red warning lights |\n",
    "| Rain | Start a rain visualization |\n",
    "| Church bells | Change background music |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOUND CLASSIFICATION -> ACTION\n",
    "# =============================================================================\n",
    "# Customize the on_sound() function to trigger whatever you want!\n",
    "\n",
    "def on_sound(predictions):\n",
    "    \"\"\"\n",
    "    Called whenever a sound is classified.\n",
    "\n",
    "    predictions: list of dicts with 'label' and 'score' keys\n",
    "    Example: [{'label': 'clapping', 'score': 0.85}, ...]\n",
    "\n",
    "    CUSTOMIZE THIS FUNCTION for your project!\n",
    "    \"\"\"\n",
    "    top_label = predictions[0]['label']\n",
    "    top_score = predictions[0]['score']\n",
    "\n",
    "    if top_score < 0.3:\n",
    "        print(\"  (Low confidence — ignoring)\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Detected: {top_label} ({top_score:.0%})\")\n",
    "\n",
    "    # === ADD YOUR CUSTOM ACTIONS HERE ===\n",
    "\n",
    "    if top_label == 'clapping':\n",
    "        print(\"  -> Clapping! (toggle something)\")\n",
    "        # osc_client.send_message(\"/sound/clapping\", 1)\n",
    "        # arduino.write(b'1')\n",
    "\n",
    "    elif top_label == 'door_knock':\n",
    "        print(\"  -> Knock knock!\")\n",
    "        # osc_client.send_message(\"/sound/knock\", 1)\n",
    "\n",
    "    elif top_label == 'keyboard_typing':\n",
    "        print(\"  -> Typing detected!\")\n",
    "        # osc_client.send_message(\"/sound/typing\", 1)\n",
    "\n",
    "    elif top_label in ['dog', 'cat', 'rooster', 'crow']:\n",
    "        print(f\"  -> Animal sound: {top_label}\")\n",
    "        # osc_client.send_message(\"/sound/animal\", top_label)\n",
    "\n",
    "    elif top_label in ['siren', 'car_horn', 'helicopter']:\n",
    "        print(f\"  -> Urban alert: {top_label}\")\n",
    "        # osc_client.send_message(\"/sound/alert\", top_label)\n",
    "\n",
    "\n",
    "# Demo with enter-to-record\n",
    "SAMPLE_RATE = 44100\n",
    "DURATION = 5\n",
    "TEMP_FILE = \"_temp_action.wav\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SOUND -> ACTION DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Make a sound and watch the action trigger!\")\n",
    "print(\"Type 'q' to quit\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"Press Enter to record: \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        print(f\"Recording {DURATION}s... make a sound!\")\n",
    "        recording = sd.rec(int(DURATION * SAMPLE_RATE),\n",
    "                          samplerate=SAMPLE_RATE, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "\n",
    "        sf.write(TEMP_FILE, recording, SAMPLE_RATE)\n",
    "        predictions = ast_classifier(TEMP_FILE, top_k=5)\n",
    "\n",
    "        on_sound(predictions)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        os.remove(TEMP_FILE)\n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOUND CLASSIFICATION -> OSC (for p5.js or any OSC receiver)\n",
    "# =============================================================================\n",
    "# Install: uv pip install python-osc\n",
    "#\n",
    "# Uncomment the OSC lines when ready to use!\n",
    "\n",
    "# from pythonosc import udp_client\n",
    "# OSC_IP = \"127.0.0.1\"   # localhost\n",
    "# OSC_PORT = 12000        # match this to your p5.js sketch\n",
    "# osc_client = udp_client.SimpleUDPClient(OSC_IP, OSC_PORT)\n",
    "\n",
    "def send_osc_from_sound(predictions):\n",
    "    \"\"\"Send OSC messages based on sound classification.\"\"\"\n",
    "    top_label = predictions[0]['label']\n",
    "    top_score = predictions[0]['score']\n",
    "\n",
    "    # Send the label and confidence\n",
    "    # osc_client.send_message(\"/sound/label\", top_label)\n",
    "    # osc_client.send_message(\"/sound/confidence\", top_score)\n",
    "    print(f\"  [OSC] /sound/label -> '{top_label}'\")\n",
    "    print(f\"  [OSC] /sound/confidence -> {top_score:.2f}\")\n",
    "\n",
    "    # Map major categories to colors for a p5.js visualization\n",
    "    category_colors = {\n",
    "        'Animals': [255, 165, 0],     # Orange\n",
    "        'Natural': [0, 150, 0],       # Green\n",
    "        'Human': [255, 0, 100],       # Pink\n",
    "        'Domestic': [100, 100, 255],  # Blue\n",
    "        'Urban': [255, 0, 0],         # Red\n",
    "    }\n",
    "\n",
    "    major_cat = get_major_category(top_label)\n",
    "    if major_cat in category_colors:\n",
    "        # osc_client.send_message(\"/sound/color\", category_colors[major_cat])\n",
    "        print(f\"  [OSC] /sound/color -> {category_colors[major_cat]} ({major_cat})\")\n",
    "\n",
    "# Demo\n",
    "print(\"OSC Sound Classification Demo:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"(Uncomment OSC lines above to actually send messages)\\n\")\n",
    "\n",
    "demo_preds = [\n",
    "    [{'label': 'dog', 'score': 0.92}],\n",
    "    [{'label': 'rain', 'score': 0.78}],\n",
    "    [{'label': 'clapping', 'score': 0.95}],\n",
    "]\n",
    "for preds in demo_preds:\n",
    "    print(f\"Input: {preds[0]['label']}\")\n",
    "    send_osc_from_sound(preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOUND CLASSIFICATION -> SERIAL (for Arduino)\n",
    "# =============================================================================\n",
    "# Detect specific sounds and send commands to Arduino!\n",
    "#\n",
    "# Uncomment the serial lines when you have Arduino connected.\n",
    "\n",
    "# import serial\n",
    "# SERIAL_PORT = '/dev/cu.usbmodem...'  # Mac: ls /dev/tty* | grep usb\n",
    "# BAUD_RATE = 9600\n",
    "# arduino = serial.Serial(SERIAL_PORT, BAUD_RATE)\n",
    "# import time; time.sleep(2)  # Wait for Arduino reset\n",
    "\n",
    "def send_serial_from_sound(predictions):\n",
    "    \"\"\"Send serial commands based on sound classification.\"\"\"\n",
    "    top_label = predictions[0]['label']\n",
    "    top_score = predictions[0]['score']\n",
    "\n",
    "    if top_score < 0.4:\n",
    "        return\n",
    "\n",
    "    if top_label == 'clapping':\n",
    "        # arduino.write(b'1')\n",
    "        print(f\"  [Serial] -> '1' (LED ON — clapping detected)\")\n",
    "    elif top_label == 'door_knock':\n",
    "        # arduino.write(b'2')\n",
    "        print(f\"  [Serial] -> '2' (BUZZ — knock detected)\")\n",
    "    elif top_label in ['siren', 'car_horn']:\n",
    "        # arduino.write(b'3')\n",
    "        print(f\"  [Serial] -> '3' (FLASH — alert detected)\")\n",
    "    else:\n",
    "        # arduino.write(b'0')\n",
    "        print(f\"  [Serial] -> '0' (OFF — {top_label} not a trigger)\")\n",
    "\n",
    "# Demo\n",
    "print(\"Serial Sound Classification Demo:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"(Uncomment serial lines above when Arduino is connected)\\n\")\n",
    "\n",
    "demo_preds = [\n",
    "    [{'label': 'clapping', 'score': 0.91}],\n",
    "    [{'label': 'door_knock', 'score': 0.75}],\n",
    "    [{'label': 'keyboard_typing', 'score': 0.60}],\n",
    "]\n",
    "for preds in demo_preds:\n",
    "    print(f\"Sound: {preds[0]['label']} ({preds[0]['score']:.0%})\")\n",
    "    send_serial_from_sound(preds)\n",
    "    print()\n",
    "\n",
    "print(\"Combine with the real-time recording code from Part 7!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Fine-Tuning (Stretch Goal — GPU Recommended)\n",
    "\n",
    "Everything above runs on your laptop's CPU. But if you have access to a GPU (Google Colab has free ones!), you can **fine-tune** the AST model on your own data.\n",
    "\n",
    "### Why Fine-Tune?\n",
    "\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| **Custom sounds** | Classify sounds not in ESC-50 (your specific door, your cat) |\n",
    "| **Better accuracy** | Focus the model on a subset of classes you care about |\n",
    "| **New domains** | Underwater sounds, industrial machinery, musical instruments |\n",
    "\n",
    "### Running on Google Colab\n",
    "\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Create a new notebook\n",
    "3. Go to **Runtime → Change runtime type → GPU (T4)**\n",
    "4. Copy the code from the cell below and run it!\n",
    "\n",
    "**Detailed guide**: [Fine-Tune AST with Transformers](https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINE-TUNING AST ON ESC-50 (GPU / GOOGLE COLAB RECOMMENDED)\n",
    "# =============================================================================\n",
    "#\n",
    "# To run this:\n",
    "#   1. Go to colab.research.google.com\n",
    "#   2. Runtime -> Change runtime type -> GPU (T4)\n",
    "#   3. Install: !pip install transformers datasets torchaudio evaluate\n",
    "#   4. Uncomment the code below and run!\n",
    "\n",
    "# --- Uncomment everything below to run on Google Colab ---\n",
    "\n",
    "# from transformers import ASTForAudioClassification, ASTFeatureExtractor\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "# from datasets import load_dataset, Audio\n",
    "# import torch\n",
    "\n",
    "# # Load model and feature extractor\n",
    "# model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "# feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "# model = ASTForAudioClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=50,\n",
    "#     ignore_mismatched_sizes=True,\n",
    "# )\n",
    "\n",
    "# # Load ESC-50\n",
    "# dataset = load_dataset(\"ashraq/esc50\")\n",
    "# dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# # Preprocess audio\n",
    "# def preprocess(batch):\n",
    "#     audio = batch['audio']['array']\n",
    "#     inputs = feature_extractor(\n",
    "#         audio,\n",
    "#         sampling_rate=batch['audio']['sampling_rate'],\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "#     batch['input_values'] = inputs['input_values'].squeeze()\n",
    "#     batch['label'] = batch['target']\n",
    "#     return batch\n",
    "\n",
    "# dataset = dataset.map(preprocess, remove_columns=['audio', 'filename', 'fold',\n",
    "#                                                    'target', 'category', 'esc10',\n",
    "#                                                    'src_file', 'take'])\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./ast-finetuned-esc50\",\n",
    "#     num_train_epochs=5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     learning_rate=5e-5,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=100,\n",
    "#     eval_strategy=\"epoch\",\n",
    "# )\n",
    "\n",
    "# # Train!\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset['train'],\n",
    "#     eval_dataset=dataset['test'],\n",
    "# )\n",
    "# # trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING AST — STRETCH GOAL\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"This code is designed to run on Google Colab with a GPU.\")\n",
    "print()\n",
    "print(\"To try it:\")\n",
    "print(\"  1. Go to colab.research.google.com\")\n",
    "print(\"  2. Runtime -> Change runtime type -> GPU (T4)\")\n",
    "print(\"  3. !pip install transformers datasets torchaudio evaluate\")\n",
    "print(\"  4. Copy this cell's code, uncomment, and run!\")\n",
    "print()\n",
    "print(\"Tutorial:\")\n",
    "print(\"  https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises and Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Explore Misclassifications\n",
    "#\n",
    "# Listen to sounds that the model got WRONG and try to understand why.\n",
    "# Do they sound similar to you? Would a human make the same mistake?\n",
    "#\n",
    "# Hint: Look at the confusion matrix to find the most confused pairs,\n",
    "# then find specific samples of those categories and listen to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Sound Scavenger Hunt\n",
    "#\n",
    "# Go around your space and try to find real examples of sounds in the ESC-50 dataset.\n",
    "# Record them with the enter-to-record cell and see how the model classifies them!\n",
    "#\n",
    "# Try to find at least one sound from each major category:\n",
    "#   Animals:  _______________\n",
    "#   Natural:  _______________\n",
    "#   Human:    _______________\n",
    "#   Domestic: _______________\n",
    "#   Urban:    _______________\n",
    "#\n",
    "# Questions:\n",
    "#   - Did the model classify them correctly?\n",
    "#   - Were some categories easier to find real examples for?\n",
    "#   - Did any real-world sounds confuse the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build a Sound-Triggered Project\n",
    "#\n",
    "# Create a project that uses environmental sound classification\n",
    "# to trigger an action in the real world!\n",
    "#\n",
    "# Ideas:\n",
    "#   - Sound-reactive p5.js visualization (different visuals for different sounds)\n",
    "#   - Arduino alarm that activates when it hears a specific sound\n",
    "#   - \"Sound diary\" that logs what sounds happen in your room over time\n",
    "#   - Sound-based game: make specific sounds to score points\n",
    "#\n",
    "# Use the on_sound() function from Part 8 as your starting point.\n",
    "# Connect it to OSC, Serial, or any other output you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Compare the Models\n",
    "#\n",
    "# Record 10 different sounds from your environment.\n",
    "# Classify each one with BOTH the classical ML model and the pretrained AST.\n",
    "#\n",
    "# Hint: For the classical ML model, extract features and use the trained\n",
    "# Random Forest / SVM. For the AST, use the pipeline.\n",
    "#\n",
    "# Questions:\n",
    "#   - Which model is more accurate on real-world sounds?\n",
    "#   - Which model is faster?\n",
    "#   - Do they make different kinds of mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 (Stretch): ESC-10 Subset\n",
    "#\n",
    "# ESC-50 has a simpler subset called ESC-10 with just 10 categories.\n",
    "# The dataset includes an 'esc10' column that tells you which samples are in this subset.\n",
    "#\n",
    "# 1. Filter the dataset to only ESC-10 samples\n",
    "# 2. Train a new classical ML model on just these 10 classes\n",
    "# 3. How does accuracy change with fewer, more distinct classes?\n",
    "#\n",
    "# Hint: df[df['esc10'] == True] gives you the ESC-10 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "- **Fool the model**: Can you make a sound that the model confidently misclassifies? (Can you make a sound with your mouth that it thinks is a dog bark?)\n",
    "- **Sound mixing**: What happens if two sounds play at the same time? Does the model pick up both?\n",
    "- **Distance test**: How does distance from the microphone affect classification?\n",
    "- **Compare AudioSet model**: Try loading `MIT/ast-finetuned-audioset-10-10-0.4593` (527 categories) and compare its predictions to the ESC-50 model.\n",
    "- **Feature importance**: Which features matter most for Random Forest? Use `rf.feature_importances_` to find out!\n",
    "- **Combine with Whisper**: Build a system that uses Whisper for speech AND the AST for environmental sounds — detect *what type* of sound is happening and handle it differently.\n",
    "- **Data augmentation**: Apply the augmentation techniques from Class 2 (noise, pitch shift, time stretch) to ESC-50 and see if it improves classical ML or ResNet accuracy.\n",
    "- **Try CLAP (zero-shot)**: [CLAP](https://huggingface.co/laion/larger_clap_music_and_speech) (Contrastive Language-Audio Pretraining) lets you classify audio using **text descriptions** instead of fixed labels. You write descriptions like \"a dog barking loudly\" and CLAP matches audio to text — no training needed! (`pip install laion-clap`)\n",
    "- **Visualize attention maps**: The AST model uses attention to decide which parts of the spectrogram matter most. Try extracting and visualizing attention weights to see what the model \"listens to.\"\n",
    "- **Try UrbanSound8K**: [UrbanSound8K](https://urbansounddataset.weebly.com/) is another environmental sound dataset focused on urban sounds (10 classes, 8,732 clips). How do models trained on ESC-50 perform on it?\n",
    "- **Unfreeze more ResNet layers**: Try unfreezing layer3 and layer4 in the ResNet fine-tuning section. Does it improve accuracy? How much slower is training?\n",
    "- **Explore other models on HuggingFace**: Search for audio-classification models at [huggingface.co/models?pipeline_tag=audio-classification](https://huggingface.co/models?pipeline_tag=audio-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook you learned:\n",
    "\n",
    "- **Environmental Sound Classification**: Identifying sounds like dog barks, rain, sirens, and keyboard clicks\n",
    "- **ESC-50 Dataset**: 2,000 clips across 50 categories — the standard benchmark\n",
    "- **Audio Feature Extraction**: MFCCs, chroma, spectral features — compact summaries of sound\n",
    "- **Classical ML**: Random Forest and SVM classifiers using handcrafted features (~50-65%)\n",
    "- **CNN on Spectrograms**: Fine-tuning ResNet-18 from ImageNet — the \"audio as image\" paradigm (~75-90%)\n",
    "- **Pretrained Deep Learning**: Audio Spectrogram Transformer achieving ~85-95% with zero training\n",
    "- **Evaluation**: Confusion matrices, per-class accuracy, understanding model mistakes\n",
    "- **Real-time Classification**: Identifying sounds from your microphone\n",
    "- **Triggering Actions**: Using sound classification to control OSC, Serial, and more\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|-----------------|\n",
    "| **Feature extraction** | Turning raw audio into meaningful numbers |\n",
    "| **Spectrograms as images** | Mel spectrograms can be classified by image models (CNNs, ViTs) |\n",
    "| **Transfer learning** | Features learned for one task (ImageNet photos) help with another (audio spectrograms) |\n",
    "| **Pretrained models** | Downloading and using models trained by others — no GPU needed! |\n",
    "| **Evaluation** | Accuracy alone isn't enough — confusion matrices reveal what the model struggles with |\n",
    "| **Real-time inference** | The record → classify → act pipeline for interactive applications |\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "From Class 1 to now, you've gone from raw audio signals to building systems that can:\n",
    "\n",
    "- **Class 1**: Record, visualize, and analyze audio signals\n",
    "- **Class 2**: Recognize spoken digits with a CNN trained from scratch\n",
    "- **Class 3**: Detect emotions in speech with classical ML\n",
    "- **Class 4**: Transcribe any speech with Whisper\n",
    "- **Class 5**: Classify environmental sounds — from handcrafted features to CNNs to pretrained transformers\n",
    "\n",
    "Each class builds on the last. The fundamentals (FFT, spectrograms, mel scale) power everything from simple pitch detection to state-of-the-art AI models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
